{"docstore/data": {"f6dc0d69-8440-4c68-835f-74254d4b651e": {"__data__": {"id_": "f6dc0d69-8440-4c68-835f-74254d4b651e", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Antonio Paolillo1,2 Pierre Gergondet2 Andrea Cherubini1 Marilena Vendittelli3 Abderrahmane Kheddar1,2 manually controlling its commands (e.g., ignition, gas pedal, and steering) and moving with the whole body to ingress/egress the car. We present a sensor-based reactive framework for realizing the central part of the complete task, consisting of driving the car along unknown roads."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Antonio Paolillo1,2 Pierre Gergondet2 Andrea Cherubini1 Marilena Vendittelli3 Abderrahmane Kheddar1,2", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88bfb23e-6c24-466d-ab81-7778e80ca621": {"__data__": {"id_": "88bfb23e-6c24-466d-ab81-7778e80ca621", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Antonio Paolillo1,2 Pierre Gergondet2 Andrea Cherubini1 Marilena Vendittelli3 Abderrahmane Kheddar1,2 manually controlling its commands (e.g., ignition, gas pedal, and steering) and moving with the whole body to ingress/egress the car. We present a sensor-based reactive framework for realizing the central part of the complete task, consisting of driving the car along unknown roads. The proposed framework provides three driving strategies by which a human supervisor can teleoperate the car or give the robot full or partial control of the car."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "manually controlling its commands (e.g., ignition, gas pedal, and steering) and moving with the whole body to ingress/egress the car.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f64eefdd-b82a-4a10-b1e8-bc9c32c5acd1": {"__data__": {"id_": "f64eefdd-b82a-4a10-b1e8-bc9c32c5acd1", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Antonio Paolillo1,2 Pierre Gergondet2 Andrea Cherubini1 Marilena Vendittelli3 Abderrahmane Kheddar1,2 manually controlling its commands (e.g., ignition, gas pedal, and steering) and moving with the whole body to ingress/egress the car. We present a sensor-based reactive framework for realizing the central part of the complete task, consisting of driving the car along unknown roads. The proposed framework provides three driving strategies by which a human supervisor can teleoperate the car or give the robot full or partial control of the car. A visual servoing scheme uses features of the road image to provide the reference angle for .Simultaneously,aKalman\ufb01ltermerges optical\ufb02owandaccelerometermeasurementstoestimatethecarlinearvelocityandcorrespond- ingly compute the gas pedal command for driving at a desired speed."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We present a sensor-based reactive framework for realizing the central part of the complete task, consisting of driving the car along unknown roads.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2152666-8be9-4f3b-aa52-d3b2163488b7": {"__data__": {"id_": "c2152666-8be9-4f3b-aa52-d3b2163488b7", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "manually controlling its commands (e.g., ignition, gas pedal, and steering) and moving with the whole body to ingress/egress the car. We present a sensor-based reactive framework for realizing the central part of the complete task, consisting of driving the car along unknown roads. The proposed framework provides three driving strategies by which a human supervisor can teleoperate the car or give the robot full or partial control of the car. A visual servoing scheme uses features of the road image to provide the reference angle for .Simultaneously,aKalman\ufb01ltermerges optical\ufb02owandaccelerometermeasurementstoestimatethecarlinearvelocityandcorrespond- ingly compute the gas pedal command for driving at a desired speed. The steering wheel and gas pedal reference are sent to the robot control to achieve the driving task with the humanoid."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed framework provides three driving strategies by which a human supervisor can teleoperate the car or give the robot full or partial control of the car.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9d9dbbb-1016-40b9-9e36-a7d9646054d3": {"__data__": {"id_": "d9d9dbbb-1016-40b9-9e36-a7d9646054d3", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We present a sensor-based reactive framework for realizing the central part of the complete task, consisting of driving the car along unknown roads. The proposed framework provides three driving strategies by which a human supervisor can teleoperate the car or give the robot full or partial control of the car. A visual servoing scheme uses features of the road image to provide the reference angle for .Simultaneously,aKalman\ufb01ltermerges optical\ufb02owandaccelerometermeasurementstoestimatethecarlinearvelocityandcorrespond- ingly compute the gas pedal command for driving at a desired speed. The steering wheel and gas pedal reference are sent to the robot control to achieve the driving task with the humanoid. We present results from a driving experience with a real car and the humanoid robot HRP-2Kai."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A visual servoing scheme uses features of the road image to provide the reference angle for .Simultaneously,aKalman\ufb01ltermerges optical\ufb02owandaccelerometermeasurementstoestimatethecarlinearvelocityandcorrespond- ingly compute the gas pedal command for driving at a desired speed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6c4b0c6-dbc0-41ec-be49-81fa30ffa287": {"__data__": {"id_": "d6c4b0c6-dbc0-41ec-be49-81fa30ffa287", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The proposed framework provides three driving strategies by which a human supervisor can teleoperate the car or give the robot full or partial control of the car. A visual servoing scheme uses features of the road image to provide the reference angle for .Simultaneously,aKalman\ufb01ltermerges optical\ufb02owandaccelerometermeasurementstoestimatethecarlinearvelocityandcorrespond- ingly compute the gas pedal command for driving at a desired speed. The steering wheel and gas pedal reference are sent to the robot control to achieve the driving task with the humanoid. We present results from a driving experience with a real car and the humanoid robot HRP-2Kai. Part of the framework has been used to perform the driving task at the DARPA Robotics Challenge."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The steering wheel and gas pedal reference are sent to the robot control to achieve the driving task with the humanoid.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be294e0d-dbff-4cf5-8a2b-dff8c8097317": {"__data__": {"id_": "be294e0d-dbff-4cf5-8a2b-dff8c8097317", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "A visual servoing scheme uses features of the road image to provide the reference angle for .Simultaneously,aKalman\ufb01ltermerges optical\ufb02owandaccelerometermeasurementstoestimatethecarlinearvelocityandcorrespond- ingly compute the gas pedal command for driving at a desired speed. The steering wheel and gas pedal reference are sent to the robot control to achieve the driving task with the humanoid. We present results from a driving experience with a real car and the humanoid robot HRP-2Kai. Part of the framework has been used to perform the driving task at the DARPA Robotics Challenge. The potential of humanoid robots in the context of disaster has been exhibited recently at the DARPA Robotics Challenge (DRC), .1 The DRC has shown that humanoids should be capable of operating machinery, originally designed for humans."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We present results from a driving experience with a real car and the humanoid robot HRP-2Kai.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f8a061d-e9ce-4da7-a390-971cb1a014a2": {"__data__": {"id_": "8f8a061d-e9ce-4da7-a390-971cb1a014a2", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The steering wheel and gas pedal reference are sent to the robot control to achieve the driving task with the humanoid. We present results from a driving experience with a real car and the humanoid robot HRP-2Kai. Part of the framework has been used to perform the driving task at the DARPA Robotics Challenge. The potential of humanoid robots in the context of disaster has been exhibited recently at the DARPA Robotics Challenge (DRC), .1 The DRC has shown that humanoids should be capable of operating machinery, originally designed for humans. The DRC utility car driving task is a good illustration of the complexity of such tasks."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part of the framework has been used to perform the driving task at the DARPA Robotics Challenge.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d6455ad-2fbb-45da-bfc6-dc8def2f2daf": {"__data__": {"id_": "0d6455ad-2fbb-45da-bfc6-dc8def2f2daf", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We present results from a driving experience with a real car and the humanoid robot HRP-2Kai. Part of the framework has been used to perform the driving task at the DARPA Robotics Challenge. The potential of humanoid robots in the context of disaster has been exhibited recently at the DARPA Robotics Challenge (DRC), .1 The DRC has shown that humanoids should be capable of operating machinery, originally designed for humans. The DRC utility car driving task is a good illustration of the complexity of such tasks. the perception and control algorithms should reproduce the human-driving skills."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The potential of humanoid robots in the context of disaster has been exhibited recently at the DARPA Robotics Challenge (DRC), .1 The DRC has shown that humanoids should be capable of operating machinery, originally designed for humans.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9039f1c0-47f9-4036-a949-36fafb8ccde1": {"__data__": {"id_": "9039f1c0-47f9-4036-a949-36fafb8ccde1", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Part of the framework has been used to perform the driving task at the DARPA Robotics Challenge. The potential of humanoid robots in the context of disaster has been exhibited recently at the DARPA Robotics Challenge (DRC), .1 The DRC has shown that humanoids should be capable of operating machinery, originally designed for humans. The DRC utility car driving task is a good illustration of the complexity of such tasks. the perception and control algorithms should reproduce the human-driving skills. sensors.5\u20137 The success of the DARPA Urban Challenges8,9 and the impressive demonstrations made by Google10 have heightened expec- environments.Consideringthis, , if the car can make its way without a robot?"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The DRC utility car driving task is a good illustration of the complexity of such tasks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a12fd39-c638-4341-b431-b09a9a9a6782": {"__data__": {"id_": "6a12fd39-c638-4341-b431-b09a9a9a6782", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The potential of humanoid robots in the context of disaster has been exhibited recently at the DARPA Robotics Challenge (DRC), .1 The DRC has shown that humanoids should be capable of operating machinery, originally designed for humans. The DRC utility car driving task is a good illustration of the complexity of such tasks. the perception and control algorithms should reproduce the human-driving skills. sensors.5\u20137 The success of the DARPA Urban Challenges8,9 and the impressive demonstrations made by Google10 have heightened expec- environments.Consideringthis, , if the car can make its way without a robot? Although both approaches are not exclusive, this is certainly a legitimate question."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the perception and control algorithms should reproduce the human-driving skills.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c3ac841-db74-4055-b006-fc873742585f": {"__data__": {"id_": "2c3ac841-db74-4055-b006-fc873742585f", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The DRC utility car driving task is a good illustration of the complexity of such tasks. the perception and control algorithms should reproduce the human-driving skills. sensors.5\u20137 The success of the DARPA Urban Challenges8,9 and the impressive demonstrations made by Google10 have heightened expec- environments.Consideringthis, , if the car can make its way without a robot? Although both approaches are not exclusive, this is certainly a legitimate question. controlling its commands (e.g., ignition, steering wheel, pedals), and \ufb01nally egress- ing it."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "sensors.5\u20137 The success of the DARPA Urban Challenges8,9 and the impressive demonstrations made by Google10 have heightened expec- environments.Consideringthis, , if the car can make its way without a robot?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "363a2dd1-14e2-4ee1-ae09-62504c12bccf": {"__data__": {"id_": "363a2dd1-14e2-4ee1-ae09-62504c12bccf", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the perception and control algorithms should reproduce the human-driving skills. sensors.5\u20137 The success of the DARPA Urban Challenges8,9 and the impressive demonstrations made by Google10 have heightened expec- environments.Consideringthis, , if the car can make its way without a robot? Although both approaches are not exclusive, this is certainly a legitimate question. controlling its commands (e.g., ignition, steering wheel, pedals), and \ufb01nally egress- ing it. All these skills can be seen as action templates, to be tailored to each vehicle and robot, and, more importantly, to be properly com- bined and sequenced to achieve driving tasks."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although both approaches are not exclusive, this is certainly a legitimate question.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0db7847d-61c1-4b0b-b6a6-13a65bf670a5": {"__data__": {"id_": "0db7847d-61c1-4b0b-b6a6-13a65bf670a5", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "sensors.5\u20137 The success of the DARPA Urban Challenges8,9 and the impressive demonstrations made by Google10 have heightened expec- environments.Consideringthis, , if the car can make its way without a robot? Although both approaches are not exclusive, this is certainly a legitimate question. controlling its commands (e.g., ignition, steering wheel, pedals), and \ufb01nally egress- ing it. All these skills can be seen as action templates, to be tailored to each vehicle and robot, and, more importantly, to be properly com- bined and sequenced to achieve driving tasks. 11 one of the eight tasks that robot must overtake is driving a utility vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "controlling its commands (e.g., ignition, steering wheel, pedals), and \ufb01nally egress- ing it.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2028aac-fbfe-4356-af18-da154acd9abf": {"__data__": {"id_": "d2028aac-fbfe-4356-af18-da154acd9abf", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Although both approaches are not exclusive, this is certainly a legitimate question. controlling its commands (e.g., ignition, steering wheel, pedals), and \ufb01nally egress- ing it. All these skills can be seen as action templates, to be tailored to each vehicle and robot, and, more importantly, to be properly com- bined and sequenced to achieve driving tasks. 11 one of the eight tasks that robot must overtake is driving a utility vehicle. The reason is that in disaster situations, the interven- tion robot must operate vehicles\u2014usually driven by humans\u2014to trans- port tools, debris, etc."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All these skills can be seen as action templates, to be tailored to each vehicle and robot, and, more importantly, to be properly com- bined and sequenced to achieve driving tasks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75a679f9-1655-4937-b4ed-96bdce2938d9": {"__data__": {"id_": "75a679f9-1655-4937-b4ed-96bdce2938d9", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "controlling its commands (e.g., ignition, steering wheel, pedals), and \ufb01nally egress- ing it. All these skills can be seen as action templates, to be tailored to each vehicle and robot, and, more importantly, to be properly com- bined and sequenced to achieve driving tasks. 11 one of the eight tasks that robot must overtake is driving a utility vehicle. The reason is that in disaster situations, the interven- tion robot must operate vehicles\u2014usually driven by humans\u2014to trans- port tools, debris, etc. Once the vehicle reaches the intervention area, the robot should execute other tasks (e.g., turning a valve, operating a drill)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11 one of the eight tasks that robot must overtake is driving a utility vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1d64d23-288b-49fe-8e37-ce0a976beb10": {"__data__": {"id_": "a1d64d23-288b-49fe-8e37-ce0a976beb10", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "All these skills can be seen as action templates, to be tailored to each vehicle and robot, and, more importantly, to be properly com- bined and sequenced to achieve driving tasks. 11 one of the eight tasks that robot must overtake is driving a utility vehicle. The reason is that in disaster situations, the interven- tion robot must operate vehicles\u2014usually driven by humans\u2014to trans- port tools, debris, etc. Once the vehicle reaches the intervention area, the robot should execute other tasks (e.g., turning a valve, operating a drill). Without a humanoid, these tasks can be hardly achieved by a unique system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reason is that in disaster situations, the interven- tion robot must operate vehicles\u2014usually driven by humans\u2014to trans- port tools, debris, etc.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d4adca8-a6dd-4112-a493-03625cc604ee": {"__data__": {"id_": "1d4adca8-a6dd-4112-a493-03625cc604ee", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "11 one of the eight tasks that robot must overtake is driving a utility vehicle. The reason is that in disaster situations, the interven- tion robot must operate vehicles\u2014usually driven by humans\u2014to trans- port tools, debris, etc. Once the vehicle reaches the intervention area, the robot should execute other tasks (e.g., turning a valve, operating a drill). Without a humanoid, these tasks can be hardly achieved by a unique system. Moreover, the robot should operate cranks or other .12,13 manufacturing industry.14 In fact, current crash-test dummies are pas- siveandnonactuated.Instead,incrashsituations,realhumansperform"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the vehicle reaches the intervention area, the robot should execute other tasks (e.g., turning a valve, operating a drill).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b279fb08-b10f-45bb-b8be-a88b827c0f8d": {"__data__": {"id_": "b279fb08-b10f-45bb-b8be-a88b827c0f8d", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The reason is that in disaster situations, the interven- tion robot must operate vehicles\u2014usually driven by humans\u2014to trans- port tools, debris, etc. Once the vehicle reaches the intervention area, the robot should execute other tasks (e.g., turning a valve, operating a drill). Without a humanoid, these tasks can be hardly achieved by a unique system. Moreover, the robot should operate cranks or other .12,13 manufacturing industry.14 In fact, current crash-test dummies are pas- siveandnonactuated.Instead,incrashsituations,realhumansperform robotic crash-test dum- mies would be more realistic in reproducing typical human behaviors."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Without a humanoid, these tasks can be hardly achieved by a unique system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2593068-93fe-4698-a6a3-82356cdca07f": {"__data__": {"id_": "c2593068-93fe-4698-a6a3-82356cdca07f", "embedding": null, "metadata": {"page_number": 1, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Once the vehicle reaches the intervention area, the robot should execute other tasks (e.g., turning a valve, operating a drill). Without a humanoid, these tasks can be hardly achieved by a unique system. Moreover, the robot should operate cranks or other .12,13 manufacturing industry.14 In fact, current crash-test dummies are pas- siveandnonactuated.Instead,incrashsituations,realhumansperform robotic crash-test dum- mies would be more realistic in reproducing typical human behaviors. this requires the solution of an unprecedented \u201chumanoid-in-the-loop\u201d control problem."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, the robot should operate cranks or other .12,13 manufacturing industry.14 In fact, current crash-test dummies are pas- siveandnonactuated.Instead,incrashsituations,realhumansperform", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfd3bd4c-7ff6-48ee-ad49-7e8ab3814c27": {"__data__": {"id_": "cfd3bd4c-7ff6-48ee-ad49-7e8ab3814c27", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Without a humanoid, these tasks can be hardly achieved by a unique system. Moreover, the robot should operate cranks or other .12,13 manufacturing industry.14 In fact, current crash-test dummies are pas- siveandnonactuated.Instead,incrashsituations,realhumansperform robotic crash-test dum- mies would be more realistic in reproducing typical human behaviors. this requires the solution of an unprecedented \u201chumanoid-in-the-loop\u201d control problem. In our work, we successfully address this and demonstrate the capability of a humanoid robot to drive a real car."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "robotic crash-test dum- mies would be more realistic in reproducing typical human behaviors.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48ea08b0-0a2f-42e9-947e-77786c8de115": {"__data__": {"id_": "48ea08b0-0a2f-42e9-947e-77786c8de115", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Moreover, the robot should operate cranks or other .12,13 manufacturing industry.14 In fact, current crash-test dummies are pas- siveandnonactuated.Instead,incrashsituations,realhumansperform robotic crash-test dum- mies would be more realistic in reproducing typical human behaviors. this requires the solution of an unprecedented \u201chumanoid-in-the-loop\u201d control problem. In our work, we successfully address this and demonstrate the capability of a humanoid robot to drive a real car. This work is based on preliminary results carried out withtheHRP-4robot,drivingasimulatedcar.15 Here,weaddnewfea- - 2Kai driving a real car outdoor on an unknown road."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "this requires the solution of an unprecedented \u201chumanoid-in-the-loop\u201d control problem.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89bb0f48-9076-44e8-b19c-b7102ac8ebe4": {"__data__": {"id_": "89bb0f48-9076-44e8-b19c-b7102ac8ebe4", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "robotic crash-test dum- mies would be more realistic in reproducing typical human behaviors. this requires the solution of an unprecedented \u201chumanoid-in-the-loop\u201d control problem. In our work, we successfully address this and demonstrate the capability of a humanoid robot to drive a real car. This work is based on preliminary results carried out withtheHRP-4robot,drivingasimulatedcar.15 Here,weaddnewfea- - 2Kai driving a real car outdoor on an unknown road. The proposed framework presents the following main features:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our work, we successfully address this and demonstrate the capability of a humanoid robot to drive a real car.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98fd1e3b-539a-4cdb-bf83-e95f8b052821": {"__data__": {"id_": "98fd1e3b-539a-4cdb-bf83-e95f8b052821", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "this requires the solution of an unprecedented \u201chumanoid-in-the-loop\u201d control problem. In our work, we successfully address this and demonstrate the capability of a humanoid robot to drive a real car. This work is based on preliminary results carried out withtheHRP-4robot,drivingasimulatedcar.15 Here,weaddnewfea- - 2Kai driving a real car outdoor on an unknown road. The proposed framework presents the following main features: \u2022 car steering control, to keep the car at a de\ufb01ned center of the road;"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This work is based on preliminary results carried out withtheHRP-4robot,drivingasimulatedcar.15 Here,weaddnewfea- - 2Kai driving a real car outdoor on an unknown road.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "278e2590-1d61-434a-828d-d30c4e7b2455": {"__data__": {"id_": "278e2590-1d61-434a-828d-d30c4e7b2455", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In our work, we successfully address this and demonstrate the capability of a humanoid robot to drive a real car. This work is based on preliminary results carried out withtheHRP-4robot,drivingasimulatedcar.15 Here,weaddnewfea- - 2Kai driving a real car outdoor on an unknown road. The proposed framework presents the following main features: \u2022 car steering control, to keep the car at a de\ufb01ned center of the road; \u2022 car velocity control, to drive the car at a desired speed;"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed framework presents the following main features:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18940f95-2f09-4d1d-9c82-c1a73c428175": {"__data__": {"id_": "18940f95-2f09-4d1d-9c82-c1a73c428175", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This work is based on preliminary results carried out withtheHRP-4robot,drivingasimulatedcar.15 Here,weaddnewfea- - 2Kai driving a real car outdoor on an unknown road. The proposed framework presents the following main features: \u2022 car steering control, to keep the car at a de\ufb01ned center of the road; \u2022 car velocity control, to drive the car at a desired speed; \u2022 admittance control, to ensure safe manipulation of the steering wheel;"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 car steering control, to keep the car at a de\ufb01ned center of the road;", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca9890a8-37b0-4b9b-8873-6841964bbd78": {"__data__": {"id_": "ca9890a8-37b0-4b9b-8873-6841964bbd78", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The proposed framework presents the following main features: \u2022 car steering control, to keep the car at a de\ufb01ned center of the road; \u2022 car velocity control, to drive the car at a desired speed; \u2022 admittance control, to ensure safe manipulation of the steering wheel; \u2022 three different driving strategies, allowing intervention or supervi-"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 car velocity control, to drive the car at a desired speed;", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "856d71d7-c37b-4c2d-a108-62c483a01ab8": {"__data__": {"id_": "856d71d7-c37b-4c2d-a108-62c483a01ab8", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 car steering control, to keep the car at a de\ufb01ned center of the road; \u2022 car velocity control, to drive the car at a desired speed; \u2022 admittance control, to ensure safe manipulation of the steering wheel; \u2022 three different driving strategies, allowing intervention or supervi- the inertial measurement unit (IMU) in the chest, and the force sensors at the wrists."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 admittance control, to ensure safe manipulation of the steering wheel;", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68214f7d-328f-4264-933c-3c92eeed24c9": {"__data__": {"id_": "68214f7d-328f-4264-933c-3c92eeed24c9", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 car velocity control, to drive the car at a desired speed; \u2022 admittance control, to ensure safe manipulation of the steering wheel; \u2022 three different driving strategies, allowing intervention or supervi- the inertial measurement unit (IMU) in the chest, and the force sensors at the wrists. Finally, the approach being purely reactive, it does not need any a priori knowl- edgeoftheenvironment.Asaresult,theframeworkallows\u2014undercer- tainassumptions\u2014 road."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 three different driving strategies, allowing intervention or supervi-", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a85ad1de-ad4f-464e-9282-a83338e38b11": {"__data__": {"id_": "a85ad1de-ad4f-464e-9282-a83338e38b11", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 admittance control, to ensure safe manipulation of the steering wheel; \u2022 three different driving strategies, allowing intervention or supervi- the inertial measurement unit (IMU) in the chest, and the force sensors at the wrists. Finally, the approach being purely reactive, it does not need any a priori knowl- edgeoftheenvironment.Asaresult,theframeworkallows\u2014undercer- tainassumptions\u2014 road. The paper organization re\ufb02ects the schematic description of the approachgiveninSection2, description of the paper sections."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the inertial measurement unit (IMU) in the chest, and the force sensors at the wrists.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc8b7639-2253-4428-8ccc-1151dc73e0d2": {"__data__": {"id_": "dc8b7639-2253-4428-8ccc-1151dc73e0d2", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 three different driving strategies, allowing intervention or supervi- the inertial measurement unit (IMU) in the chest, and the force sensors at the wrists. Finally, the approach being purely reactive, it does not need any a priori knowl- edgeoftheenvironment.Asaresult,theframeworkallows\u2014undercer- tainassumptions\u2014 road. The paper organization re\ufb02ects the schematic description of the approachgiveninSection2, description of the paper sections. The objective of this work is to enable a humanoid robot to velocity."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, the approach being purely reactive, it does not need any a priori knowl- edgeoftheenvironment.Asaresult,theframeworkallows\u2014undercer- tainassumptions\u2014 road.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b77035f-a928-42cb-a234-96d1aa50b62a": {"__data__": {"id_": "1b77035f-a928-42cb-a234-96d1aa50b62a", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the inertial measurement unit (IMU) in the chest, and the force sensors at the wrists. Finally, the approach being purely reactive, it does not need any a priori knowl- edgeoftheenvironment.Asaresult,theframeworkallows\u2014undercer- tainassumptions\u2014 road. The paper organization re\ufb02ects the schematic description of the approachgiveninSection2, description of the paper sections. The objective of this work is to enable a humanoid robot to velocity. More speci\ufb01cally, we focus on the driving task and, there- fore, consider the robot sitting in the car, already in a correct driving posture."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The paper organization re\ufb02ects the schematic description of the approachgiveninSection2, description of the paper sections.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "933fd4b4-eabf-4e89-8501-f4270bfe4173": {"__data__": {"id_": "933fd4b4-eabf-4e89-8501-f4270bfe4173", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Finally, the approach being purely reactive, it does not need any a priori knowl- edgeoftheenvironment.Asaresult,theframeworkallows\u2014undercer- tainassumptions\u2014 road. The paper organization re\ufb02ects the schematic description of the approachgiveninSection2, description of the paper sections. The objective of this work is to enable a humanoid robot to velocity. More speci\ufb01cally, we focus on the driving task and, there- fore, consider the robot sitting in the car, already in a correct driving posture. Most of the existing approaches have achieved this goal by rely- ing on teleoperation.16\u201322 Supervisory steering and gas commands are sent to the robot to drive the car in Karumanchi et al.23; DeDonato andcolleagues24 proposeahybridsolution,withteleoperatedsteering and autonomous speed control."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The objective of this work is to enable a humanoid robot to velocity.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97cd37fe-bdf5-42cb-a657-725a2d8dd514": {"__data__": {"id_": "97cd37fe-bdf5-42cb-a657-725a2d8dd514", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The paper organization re\ufb02ects the schematic description of the approachgiveninSection2, description of the paper sections. The objective of this work is to enable a humanoid robot to velocity. More speci\ufb01cally, we focus on the driving task and, there- fore, consider the robot sitting in the car, already in a correct driving posture. Most of the existing approaches have achieved this goal by rely- ing on teleoperation.16\u201322 Supervisory steering and gas commands are sent to the robot to drive the car in Karumanchi et al.23; DeDonato andcolleagues24 proposeahybridsolution,withteleoperatedsteering and autonomous speed control. The velocity of the car, estimated with stereo cameras, is fed back to a proportional integral (PI) controller, whereas light imaging, detection, and ranging (LIDAR), IMU, and visual .In Kumagaietal.,25 ,using robot kinematics for vehicle path estimation, and point cloud data for"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More speci\ufb01cally, we focus on the driving task and, there- fore, consider the robot sitting in the car, already in a correct driving posture.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "780761c1-b488-4361-94da-2b65f120b510": {"__data__": {"id_": "780761c1-b488-4361-94da-2b65f120b510", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The objective of this work is to enable a humanoid robot to velocity. More speci\ufb01cally, we focus on the driving task and, there- fore, consider the robot sitting in the car, already in a correct driving posture. Most of the existing approaches have achieved this goal by rely- ing on teleoperation.16\u201322 Supervisory steering and gas commands are sent to the robot to drive the car in Karumanchi et al.23; DeDonato andcolleagues24 proposeahybridsolution,withteleoperatedsteering and autonomous speed control. The velocity of the car, estimated with stereo cameras, is fed back to a proportional integral (PI) controller, whereas light imaging, detection, and ranging (LIDAR), IMU, and visual .In Kumagaietal.,25 ,using robot kinematics for vehicle path estimation, and point cloud data for An impedance sys- tem is used to ensure safe manipulation of the steering wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Most of the existing approaches have achieved this goal by rely- ing on teleoperation.16\u201322 Supervisory steering and gas commands are sent to the robot to drive the car in Karumanchi et al.23; DeDonato andcolleagues24 proposeahybridsolution,withteleoperatedsteering and autonomous speed control.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b591cf49-ca56-491a-9c2c-ed0b0ded3a10": {"__data__": {"id_": "b591cf49-ca56-491a-9c2c-ed0b0ded3a10", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "More speci\ufb01cally, we focus on the driving task and, there- fore, consider the robot sitting in the car, already in a correct driving posture. Most of the existing approaches have achieved this goal by rely- ing on teleoperation.16\u201322 Supervisory steering and gas commands are sent to the robot to drive the car in Karumanchi et al.23; DeDonato andcolleagues24 proposeahybridsolution,withteleoperatedsteering and autonomous speed control. The velocity of the car, estimated with stereo cameras, is fed back to a proportional integral (PI) controller, whereas light imaging, detection, and ranging (LIDAR), IMU, and visual .In Kumagaietal.,25 ,using robot kinematics for vehicle path estimation, and point cloud data for An impedance sys- tem is used to ensure safe manipulation of the steering wheel. 26 autonomous robot driving is achieved by following the proper trajectory among obstacles, detected with laser measurements."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The velocity of the car, estimated with stereo cameras, is fed back to a proportional integral (PI) controller, whereas light imaging, detection, and ranging (LIDAR), IMU, and visual .In Kumagaietal.,25 ,using robot kinematics for vehicle path estimation, and point cloud data for", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87015095-e5f0-4a75-ae52-6edddc84dbb1": {"__data__": {"id_": "87015095-e5f0-4a75-ae52-6edddc84dbb1", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Most of the existing approaches have achieved this goal by rely- ing on teleoperation.16\u201322 Supervisory steering and gas commands are sent to the robot to drive the car in Karumanchi et al.23; DeDonato andcolleagues24 proposeahybridsolution,withteleoperatedsteering and autonomous speed control. The velocity of the car, estimated with stereo cameras, is fed back to a proportional integral (PI) controller, whereas light imaging, detection, and ranging (LIDAR), IMU, and visual .In Kumagaietal.,25 ,using robot kinematics for vehicle path estimation, and point cloud data for An impedance sys- tem is used to ensure safe manipulation of the steering wheel. 26 autonomous robot driving is achieved by following the proper trajectory among obstacles, detected with laser measurements. LIDAR scans are used in Rasmussen et al.27 to plan a path for the car, whereas the velocity is estimated with a visual odom- etry module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An impedance sys- tem is used to ensure safe manipulation of the steering wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad4700e8-fff8-411e-9340-87efde9d0d24": {"__data__": {"id_": "ad4700e8-fff8-411e-9340-87efde9d0d24", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The velocity of the car, estimated with stereo cameras, is fed back to a proportional integral (PI) controller, whereas light imaging, detection, and ranging (LIDAR), IMU, and visual .In Kumagaietal.,25 ,using robot kinematics for vehicle path estimation, and point cloud data for An impedance sys- tem is used to ensure safe manipulation of the steering wheel. 26 autonomous robot driving is achieved by following the proper trajectory among obstacles, detected with laser measurements. LIDAR scans are used in Rasmussen et al.27 to plan a path for the car, whereas the velocity is estimated with a visual odom- etry module. The operation of the steering wheel and gas pedal is real- ized with simple controllers."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "26 autonomous robot driving is achieved by following the proper trajectory among obstacles, detected with laser measurements.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "413eac7c-ab10-43e1-8b7f-84bb3ac68183": {"__data__": {"id_": "413eac7c-ab10-43e1-8b7f-84bb3ac68183", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "An impedance sys- tem is used to ensure safe manipulation of the steering wheel. 26 autonomous robot driving is achieved by following the proper trajectory among obstacles, detected with laser measurements. LIDAR scans are used in Rasmussen et al.27 to plan a path for the car, whereas the velocity is estimated with a visual odom- etry module. The operation of the steering wheel and gas pedal is real- ized with simple controllers. we use data from the robot on-board camera and IMU to close the autonomous driver feedback loop."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LIDAR scans are used in Rasmussen et al.27 to plan a path for the car, whereas the velocity is estimated with a visual odom- etry module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea5332d4-9b07-4329-ae4f-55e3c5eada91": {"__data__": {"id_": "ea5332d4-9b07-4329-ae4f-55e3c5eada91", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "26 autonomous robot driving is achieved by following the proper trajectory among obstacles, detected with laser measurements. LIDAR scans are used in Rasmussen et al.27 to plan a path for the car, whereas the velocity is estimated with a visual odom- etry module. The operation of the steering wheel and gas pedal is real- ized with simple controllers. we use data from the robot on-board camera and IMU to close the autonomous driver feedback loop. The force measured on the robot wrists is exploited to operate the car steering wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The operation of the steering wheel and gas pedal is real- ized with simple controllers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12a77b7e-c039-4e5d-a378-f9c4ccf7eab6": {"__data__": {"id_": "12a77b7e-c039-4e5d-a378-f9c4ccf7eab6", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "LIDAR scans are used in Rasmussen et al.27 to plan a path for the car, whereas the velocity is estimated with a visual odom- etry module. The operation of the steering wheel and gas pedal is real- ized with simple controllers. we use data from the robot on-board camera and IMU to close the autonomous driver feedback loop. The force measured on the robot wrists is exploited to operate the car steering wheel. some simplifying assumptions have been introduced to capture the conceptual structure of the prob- lem without losing generality:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we use data from the robot on-board camera and IMU to close the autonomous driver feedback loop.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ffe3ff3-814d-47a7-99ac-0b86ba11cfdb": {"__data__": {"id_": "2ffe3ff3-814d-47a7-99ac-0b86ba11cfdb", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The operation of the steering wheel and gas pedal is real- ized with simple controllers. we use data from the robot on-board camera and IMU to close the autonomous driver feedback loop. The force measured on the robot wrists is exploited to operate the car steering wheel. some simplifying assumptions have been introduced to capture the conceptual structure of the prob- lem without losing generality: The car brake and clutch pedals are not considered, and the driv- ing speed is assumed to be positive and independently controlled through the gas pedal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The force measured on the robot wrists is exploited to operate the car steering wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4910b06-8db6-4528-ae8f-eae4d3efde4e": {"__data__": {"id_": "f4910b06-8db6-4528-ae8f-eae4d3efde4e", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we use data from the robot on-board camera and IMU to close the autonomous driver feedback loop. The force measured on the robot wrists is exploited to operate the car steering wheel. some simplifying assumptions have been introduced to capture the conceptual structure of the prob- lem without losing generality: The car brake and clutch pedals are not considered, and the driv- ing speed is assumed to be positive and independently controlled through the gas pedal. Hence, the steering wheel and the gas ped- als are the only vehicle controls used by the robot for driving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "some simplifying assumptions have been introduced to capture the conceptual structure of the prob- lem without losing generality:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "daedd081-874c-48cb-a20d-2b5829596e2b": {"__data__": {"id_": "daedd081-874c-48cb-a20d-2b5829596e2b", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The force measured on the robot wrists is exploited to operate the car steering wheel. some simplifying assumptions have been introduced to capture the conceptual structure of the prob- lem without losing generality: The car brake and clutch pedals are not considered, and the driv- ing speed is assumed to be positive and independently controlled through the gas pedal. Hence, the steering wheel and the gas ped- als are the only vehicle controls used by the robot for driving. 2. ,withonehand on the steering wheel, the foot on the pedal, and the camera point- ing the road, with focal axis aligned with the car sagittal plane."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The car brake and clutch pedals are not considered, and the driv- ing speed is assumed to be positive and independently controlled through the gas pedal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a53e43cd-c675-4623-bf04-19ccaa9c8747": {"__data__": {"id_": "a53e43cd-c675-4623-bf04-19ccaa9c8747", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "some simplifying assumptions have been introduced to capture the conceptual structure of the prob- lem without losing generality: The car brake and clutch pedals are not considered, and the driv- ing speed is assumed to be positive and independently controlled through the gas pedal. Hence, the steering wheel and the gas ped- als are the only vehicle controls used by the robot for driving. 2. ,withonehand on the steering wheel, the foot on the pedal, and the camera point- ing the road, with focal axis aligned with the car sagittal plane. The hand grasping con\ufb01guration is unchanged during operation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hence, the steering wheel and the gas ped- als are the only vehicle controls used by the robot for driving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08d36cb6-535d-4ec0-8d6d-903d1473798f": {"__data__": {"id_": "08d36cb6-535d-4ec0-8d6d-903d1473798f", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The car brake and clutch pedals are not considered, and the driv- ing speed is assumed to be positive and independently controlled through the gas pedal. Hence, the steering wheel and the gas ped- als are the only vehicle controls used by the robot for driving. 2. ,withonehand on the steering wheel, the foot on the pedal, and the camera point- ing the road, with focal axis aligned with the car sagittal plane. The hand grasping con\ufb01guration is unchanged during operation. The road is assumed to be locally \ufb02at, horizontal, straight, and delimited by parallel borders.\u2217 Although global convergence can be proved only for straight roads, turns with admissible curvature bounds are also feasible, as shown in the Experimental section."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ,withonehand on the steering wheel, the foot on the pedal, and the camera point- ing the road, with focal axis aligned with the car sagittal plane.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "827d6bf7-9fd7-4a23-8e8d-f6232a0fb3d1": {"__data__": {"id_": "827d6bf7-9fd7-4a23-8e8d-f6232a0fb3d1", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Hence, the steering wheel and the gas ped- als are the only vehicle controls used by the robot for driving. 2. ,withonehand on the steering wheel, the foot on the pedal, and the camera point- ing the road, with focal axis aligned with the car sagittal plane. The hand grasping con\ufb01guration is unchanged during operation. The road is assumed to be locally \ufb02at, horizontal, straight, and delimited by parallel borders.\u2217 Although global convergence can be proved only for straight roads, turns with admissible curvature bounds are also feasible, as shown in the Experimental section. Instead,crossings,traf\ufb01clights, , and road signs are not interpreted."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The hand grasping con\ufb01guration is unchanged during operation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "815bc4bb-13ad-4bd9-9e9a-fe7ae468f55f": {"__data__": {"id_": "815bc4bb-13ad-4bd9-9e9a-fe7ae468f55f", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "2. ,withonehand on the steering wheel, the foot on the pedal, and the camera point- ing the road, with focal axis aligned with the car sagittal plane. The hand grasping con\ufb01guration is unchanged during operation. The road is assumed to be locally \ufb02at, horizontal, straight, and delimited by parallel borders.\u2217 Although global convergence can be proved only for straight roads, turns with admissible curvature bounds are also feasible, as shown in the Experimental section. Instead,crossings,traf\ufb01clights, , and road signs are not interpreted. we propose the control architecture in Figure 1."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The road is assumed to be locally \ufb02at, horizontal, straight, and delimited by parallel borders.\u2217 Although global convergence can be proved only for straight roads, turns with admissible curvature bounds are also feasible, as shown in the Experimental section.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1253bc63-093a-4a5d-9acc-53d26f6423db": {"__data__": {"id_": "1253bc63-093a-4a5d-9acc-53d26f6423db", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The hand grasping con\ufb01guration is unchanged during operation. The road is assumed to be locally \ufb02at, horizontal, straight, and delimited by parallel borders.\u2217 Although global convergence can be proved only for straight roads, turns with admissible curvature bounds are also feasible, as shown in the Experimental section. Instead,crossings,traf\ufb01clights, , and road signs are not interpreted. we propose the control architecture in Figure 1. The robot sits in the car, with its camera pointing to the road."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead,crossings,traf\ufb01clights, , and road signs are not interpreted.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e711a01-4ddd-4e61-b7f9-4a97d829107e": {"__data__": {"id_": "0e711a01-4ddd-4e61-b7f9-4a97d829107e", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The road is assumed to be locally \ufb02at, horizontal, straight, and delimited by parallel borders.\u2217 Although global convergence can be proved only for straight roads, turns with admissible curvature bounds are also feasible, as shown in the Experimental section. Instead,crossings,traf\ufb01clights, , and road signs are not interpreted. we propose the control architecture in Figure 1. The robot sits in the car, with its camera pointing to the road. The acquired images and IMU data are used by two branches of the framework running in parallel: car steering and velocity control."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we propose the control architecture in Figure 1.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "825b5cf5-19da-4989-b553-3efa54d20e0f": {"__data__": {"id_": "825b5cf5-19da-4989-b553-3efa54d20e0f", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Instead,crossings,traf\ufb01clights, , and road signs are not interpreted. we propose the control architecture in Figure 1. The robot sits in the car, with its camera pointing to the road. The acquired images and IMU data are used by two branches of the framework running in parallel: car steering and velocity control. whereas an image-processing algorithm detects the road borders (road detection)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot sits in the car, with its camera pointing to the road.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2e86d3e-36ca-4a8d-9ed9-5b45bfb87aec": {"__data__": {"id_": "b2e86d3e-36ca-4a8d-9ed9-5b45bfb87aec", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we propose the control architecture in Figure 1. The robot sits in the car, with its camera pointing to the road. The acquired images and IMU data are used by two branches of the framework running in parallel: car steering and velocity control. whereas an image-processing algorithm detects the road borders (road detection). These borders are used to compute the visual features feeding the steering control block."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The acquired images and IMU data are used by two branches of the framework running in parallel: car steering and velocity control.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98e4587f-91e6-452d-a81d-ed352b14f226": {"__data__": {"id_": "98e4587f-91e6-452d-a81d-ed352b14f226", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The robot sits in the car, with its camera pointing to the road. The acquired images and IMU data are used by two branches of the framework running in parallel: car steering and velocity control. whereas an image-processing algorithm detects the road borders (road detection). These borders are used to compute the visual features feeding the steering control block. Finally, the wheel operation block into a desired trajectory for the robot hand that is operating the steering wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "whereas an image-processing algorithm detects the road borders (road detection).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d252ca11-80c8-43e1-bef5-406fd9f4f772": {"__data__": {"id_": "d252ca11-80c8-43e1-bef5-406fd9f4f772", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The acquired images and IMU data are used by two branches of the framework running in parallel: car steering and velocity control. whereas an image-processing algorithm detects the road borders (road detection). These borders are used to compute the visual features feeding the steering control block. Finally, the wheel operation block into a desired trajectory for the robot hand that is operating the steering wheel. This trajectory can be adjusted by an admittance system, depending on the force exchanged between the robot hand and the steering wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These borders are used to compute the visual features feeding the steering control block.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f771aaf-8c76-494e-8ad1-55da40ae2b25": {"__data__": {"id_": "0f771aaf-8c76-494e-8ad1-55da40ae2b25", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "whereas an image-processing algorithm detects the road borders (road detection). These borders are used to compute the visual features feeding the steering control block. Finally, the wheel operation block into a desired trajectory for the robot hand that is operating the steering wheel. This trajectory can be adjusted by an admittance system, depending on the force exchanged between the robot hand and the steering wheel. \u2217 The assumption on parallel road borders can be relaxed, as proved in Paolillo et al.28 We ,aswillbeshown in Section 5.1."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, the wheel operation block into a desired trajectory for the robot hand that is operating the steering wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20e0a454-1cba-4306-90ed-c0a7b59be9fe": {"__data__": {"id_": "20e0a454-1cba-4306-90ed-c0a7b59be9fe", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "These borders are used to compute the visual features feeding the steering control block. Finally, the wheel operation block into a desired trajectory for the robot hand that is operating the steering wheel. This trajectory can be adjusted by an admittance system, depending on the force exchanged between the robot hand and the steering wheel. \u2217 The assumption on parallel road borders can be relaxed, as proved in Paolillo et al.28 We ,aswillbeshown in Section 5.1. through the gas pedal operation by the robot foot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This trajectory can be adjusted by an admittance system, depending on the force exchanged between the robot hand and the steering wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2713e43e-5259-40e8-8dc1-83e5e7f77a11": {"__data__": {"id_": "2713e43e-5259-40e8-8dc1-83e5e7f77a11", "embedding": null, "metadata": {"page_number": 2, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Finally, the wheel operation block into a desired trajectory for the robot hand that is operating the steering wheel. This trajectory can be adjusted by an admittance system, depending on the force exchanged between the robot hand and the steering wheel. \u2217 The assumption on parallel road borders can be relaxed, as proved in Paolillo et al.28 We ,aswillbeshown in Section 5.1. through the gas pedal operation by the robot foot. A Kalman\ufb01lter(KF) ofthevehicle(carvelocityestimation) control, which provides the gas pedal reference angle for obtaining the desired velocity."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2217 The assumption on parallel road borders can be relaxed, as proved in Paolillo et al.28 We ,aswillbeshown in Section 5.1.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82da8de4-ad60-4617-a276-3881f8c84ef6": {"__data__": {"id_": "82da8de4-ad60-4617-a276-3881f8c84ef6", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This trajectory can be adjusted by an admittance system, depending on the force exchanged between the robot hand and the steering wheel. \u2217 The assumption on parallel road borders can be relaxed, as proved in Paolillo et al.28 We ,aswillbeshown in Section 5.1. through the gas pedal operation by the robot foot. A Kalman\ufb01lter(KF) ofthevehicle(carvelocityestimation) control, which provides the gas pedal reference angle for obtaining the desired velocity. The pedal operation block transforms this signal into a reference for the robot foot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "through the gas pedal operation by the robot foot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "926c6aad-fdf9-4ffe-ac17-0fbf4a30fc46": {"__data__": {"id_": "926c6aad-fdf9-4ffe-ac17-0fbf4a30fc46", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2217 The assumption on parallel road borders can be relaxed, as proved in Paolillo et al.28 We ,aswillbeshown in Section 5.1. through the gas pedal operation by the robot foot. A Kalman\ufb01lter(KF) ofthevehicle(carvelocityestimation) control, which provides the gas pedal reference angle for obtaining the desired velocity. The pedal operation block transforms this signal into a reference for the robot foot. Driving Mode Steering Control Car Velocity Control Autonomous Enabled Enabled Shared-autonomy Enableda Disabled Teleoperated Disabled Disabled"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Kalman\ufb01lter(KF) ofthevehicle(carvelocityestimation) control, which provides the gas pedal reference angle for obtaining the desired velocity.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3daa29bb-a333-475e-9f45-1bbe27124460": {"__data__": {"id_": "3daa29bb-a333-475e-9f45-1bbe27124460", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "through the gas pedal operation by the robot foot. A Kalman\ufb01lter(KF) ofthevehicle(carvelocityestimation) control, which provides the gas pedal reference angle for obtaining the desired velocity. The pedal operation block transforms this signal into a reference for the robot foot. Driving Mode Steering Control Car Velocity Control Autonomous Enabled Enabled Shared-autonomy Enableda Disabled Teleoperated Disabled Disabled allows a humanoid robot to autonomously drive a car along an unknown road at a desired velocity."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The pedal operation block transforms this signal into a reference for the robot foot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e39b995f-29d9-499e-9b71-2e9e69a4d244": {"__data__": {"id_": "e39b995f-29d9-499e-9b71-2e9e69a4d244", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "A Kalman\ufb01lter(KF) ofthevehicle(carvelocityestimation) control, which provides the gas pedal reference angle for obtaining the desired velocity. The pedal operation block transforms this signal into a reference for the robot foot. Driving Mode Steering Control Car Velocity Control Autonomous Enabled Enabled Shared-autonomy Enableda Disabled Teleoperated Disabled Disabled allows a humanoid robot to autonomously drive a car along an unknown road at a desired velocity. We further extend the versatility of our framework by imple- menting three different \u201cdriving modes,\u201d to ease human supervision and possible intervention if needed:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Driving Mode Steering Control Car Velocity Control Autonomous Enabled Enabled Shared-autonomy Enableda Disabled Teleoperated Disabled Disabled", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ce19e1c-4c3f-46ff-b72a-7b1ef13ce89e": {"__data__": {"id_": "3ce19e1c-4c3f-46ff-b72a-7b1ef13ce89e", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The pedal operation block transforms this signal into a reference for the robot foot. Driving Mode Steering Control Car Velocity Control Autonomous Enabled Enabled Shared-autonomy Enableda Disabled Teleoperated Disabled Disabled allows a humanoid robot to autonomously drive a car along an unknown road at a desired velocity. We further extend the versatility of our framework by imple- menting three different \u201cdriving modes,\u201d to ease human supervision and possible intervention if needed: Car steering and velocity control are both enabled, as indicated above, and the robot autonomously drives the car without any human aid."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "allows a humanoid robot to autonomously drive a car along an unknown road at a desired velocity.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c48f863-f5e0-4f41-8734-726e8ccc453f": {"__data__": {"id_": "6c48f863-f5e0-4f41-8734-726e8ccc453f", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Driving Mode Steering Control Car Velocity Control Autonomous Enabled Enabled Shared-autonomy Enableda Disabled Teleoperated Disabled Disabled allows a humanoid robot to autonomously drive a car along an unknown road at a desired velocity. We further extend the versatility of our framework by imple- menting three different \u201cdriving modes,\u201d to ease human supervision and possible intervention if needed: Car steering and velocity control are both enabled, as indicated above, and the robot autonomously drives the car without any human aid. The user remotely operates the robot foot on the pedal and speci\ufb01es the target in the image for the steering opera- tions."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We further extend the versatility of our framework by imple- menting three different \u201cdriving modes,\u201d to ease human supervision and possible intervention if needed:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eba74659-908d-4adf-8e20-b7868a627b75": {"__data__": {"id_": "eba74659-908d-4adf-8e20-b7868a627b75", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "allows a humanoid robot to autonomously drive a car along an unknown road at a desired velocity. We further extend the versatility of our framework by imple- menting three different \u201cdriving modes,\u201d to ease human supervision and possible intervention if needed: Car steering and velocity control are both enabled, as indicated above, and the robot autonomously drives the car without any human aid. The user remotely operates the robot foot on the pedal and speci\ufb01es the target in the image for the steering opera- tions. Both the robot hand and foot are teleoperated for ,respectively.Theref- erence signals are sent to the task-based QP control through a key- boardorjoystick."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Car steering and velocity control are both enabled, as indicated above, and the robot autonomously drives the car without any human aid.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22bada2e-8356-4835-84a0-1cb03918c3a5": {"__data__": {"id_": "22bada2e-8356-4835-84a0-1cb03918c3a5", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We further extend the versatility of our framework by imple- menting three different \u201cdriving modes,\u201d to ease human supervision and possible intervention if needed: Car steering and velocity control are both enabled, as indicated above, and the robot autonomously drives the car without any human aid. The user remotely operates the robot foot on the pedal and speci\ufb01es the target in the image for the steering opera- tions. Both the robot hand and foot are teleoperated for ,respectively.Theref- erence signals are sent to the task-based QP control through a key- boardorjoystick. aspresentedinFigure1.In the following sections, we detail the primitive functionalities required by the autonomous mode, since the shared autonomy and teleopera- tion modes use a subset of such functionalities."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The user remotely operates the robot foot on the pedal and speci\ufb01es the target in the image for the steering opera- tions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d05b455-b713-445e-8417-ecb91539e024": {"__data__": {"id_": "5d05b455-b713-445e-8417-ecb91539e024", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Car steering and velocity control are both enabled, as indicated above, and the robot autonomously drives the car without any human aid. The user remotely operates the robot foot on the pedal and speci\ufb01es the target in the image for the steering opera- tions. Both the robot hand and foot are teleoperated for ,respectively.Theref- erence signals are sent to the task-based QP control through a key- boardorjoystick. aspresentedinFigure1.In the following sections, we detail the primitive functionalities required by the autonomous mode, since the shared autonomy and teleopera- tion modes use a subset of such functionalities. The rest of paper is organized as follows."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both the robot hand and foot are teleoperated for ,respectively.Theref- erence signals are sent to the task-based QP control through a key- boardorjoystick.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e838d63f-8158-4948-a137-218eebcab509": {"__data__": {"id_": "e838d63f-8158-4948-a137-218eebcab509", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The user remotely operates the robot foot on the pedal and speci\ufb01es the target in the image for the steering opera- tions. Both the robot hand and foot are teleoperated for ,respectively.Theref- erence signals are sent to the task-based QP control through a key- boardorjoystick. aspresentedinFigure1.In the following sections, we detail the primitive functionalities required by the autonomous mode, since the shared autonomy and teleopera- tion modes use a subset of such functionalities. The rest of paper is organized as follows. Section 3 describes the model used for the car\u2013robot system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "aspresentedinFigure1.In the following sections, we detail the primitive functionalities required by the autonomous mode, since the shared autonomy and teleopera- tion modes use a subset of such functionalities.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dce6f08-a652-4ffd-94dd-9ffb508bc475": {"__data__": {"id_": "5dce6f08-a652-4ffd-94dd-9ffb508bc475", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Both the robot hand and foot are teleoperated for ,respectively.Theref- erence signals are sent to the task-based QP control through a key- boardorjoystick. aspresentedinFigure1.In the following sections, we detail the primitive functionalities required by the autonomous mode, since the shared autonomy and teleopera- tion modes use a subset of such functionalities. The rest of paper is organized as follows. Section 3 describes the model used for the car\u2013robot system. Then, the main components of the proposed framework are detailed."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rest of paper is organized as follows.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68bfe79c-a812-4407-b799-f7819210ceed": {"__data__": {"id_": "68bfe79c-a812-4407-b799-f7819210ceed", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "aspresentedinFigure1.In the following sections, we detail the primitive functionalities required by the autonomous mode, since the shared autonomy and teleopera- tion modes use a subset of such functionalities. The rest of paper is organized as follows. Section 3 describes the model used for the car\u2013robot system. Then, the main components of the proposed framework are detailed. Section 4 presents the percep- tionpart,thatis, the car velocity."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 3 describes the model used for the car\u2013robot system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8674cb5-c87c-42f5-98da-f70f0eb165c2": {"__data__": {"id_": "c8674cb5-c87c-42f5-98da-f70f0eb165c2", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The rest of paper is organized as follows. Section 3 describes the model used for the car\u2013robot system. Then, the main components of the proposed framework are detailed. Section 4 presents the percep- tionpart,thatis, the car velocity. Section 5 deals with car control, that is, how the feed- back signals are transformed into references for the steering wheel and for the gas pedal, whereas Section 6 focuses on humanoid con- trol, that is, on the computation of the commands for the robot hand and foot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the main components of the proposed framework are detailed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73f091c7-3074-4914-b7da-180871e54ae2": {"__data__": {"id_": "73f091c7-3074-4914-b7da-180871e54ae2", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Section 3 describes the model used for the car\u2013robot system. Then, the main components of the proposed framework are detailed. Section 4 presents the percep- tionpart,thatis, the car velocity. Section 5 deals with car control, that is, how the feed- back signals are transformed into references for the steering wheel and for the gas pedal, whereas Section 6 focuses on humanoid con- trol, that is, on the computation of the commands for the robot hand and foot. The experiments carried out with HRP-2Kai are presented in Section 7."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 4 presents the percep- tionpart,thatis, the car velocity.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "969f0d38-bd83-4bd6-b2c6-a6d5a97fc6f9": {"__data__": {"id_": "969f0d38-bd83-4bd6-b2c6-a6d5a97fc6f9", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Then, the main components of the proposed framework are detailed. Section 4 presents the percep- tionpart,thatis, the car velocity. Section 5 deals with car control, that is, how the feed- back signals are transformed into references for the steering wheel and for the gas pedal, whereas Section 6 focuses on humanoid con- trol, that is, on the computation of the commands for the robot hand and foot. The experiments carried out with HRP-2Kai are presented in Section 7. Finally, Section 8 concludes the paper and outlines future research perspectives."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 5 deals with car control, that is, how the feed- back signals are transformed into references for the steering wheel and for the gas pedal, whereas Section 6 focuses on humanoid con- trol, that is, on the computation of the commands for the robot hand and foot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c53d4a2e-faef-46b8-bc4c-e70fe83d78e5": {"__data__": {"id_": "c53d4a2e-faef-46b8-bc4c-e70fe83d78e5", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Section 4 presents the percep- tionpart,thatis, the car velocity. Section 5 deals with car control, that is, how the feed- back signals are transformed into references for the steering wheel and for the gas pedal, whereas Section 6 focuses on humanoid con- trol, that is, on the computation of the commands for the robot hand and foot. The experiments carried out with HRP-2Kai are presented in Section 7. Finally, Section 8 concludes the paper and outlines future research perspectives. The human user/supervisor can intervene at any moment during the execution of thedrivingtask, .Theselection,"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experiments carried out with HRP-2Kai are presented in Section 7.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fa7b121-5f8f-4540-bd18-2e1fa78680b4": {"__data__": {"id_": "7fa7b121-5f8f-4540-bd18-2e1fa78680b4", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Section 5 deals with car control, that is, how the feed- back signals are transformed into references for the steering wheel and for the gas pedal, whereas Section 6 focuses on humanoid con- trol, that is, on the computation of the commands for the robot hand and foot. The experiments carried out with HRP-2Kai are presented in Section 7. Finally, Section 8 concludes the paper and outlines future research perspectives. The human user/supervisor can intervene at any moment during the execution of thedrivingtask, .Theselection, .29 Torecoverthedynamicsys- tem control input, it is however necessary to know the exact dynamic model, ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, Section 8 concludes the paper and outlines future research perspectives.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59f4d0da-b045-4901-a4b8-d3ae9572702e": {"__data__": {"id_": "59f4d0da-b045-4901-a4b8-d3ae9572702e", "embedding": null, "metadata": {"page_number": 3, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The experiments carried out with HRP-2Kai are presented in Section 7. Finally, Section 8 concludes the paper and outlines future research perspectives. The human user/supervisor can intervene at any moment during the execution of thedrivingtask, .Theselection, .29 Torecoverthedynamicsys- tem control input, it is however necessary to know the exact dynamic model, . are therefore necessary, these do not affect the controller in the con- sidered scenario (low accelerations, \ufb02at and horizontal road)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The human user/supervisor can intervene at any moment during the execution of thedrivingtask, .Theselection,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12c0734a-f277-46ae-a474-e14922e6954e": {"__data__": {"id_": "12c0734a-f277-46ae-a474-e14922e6954e", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Finally, Section 8 concludes the paper and outlines future research perspectives. The human user/supervisor can intervene at any moment during the execution of thedrivingtask, .Theselection, .29 Torecoverthedynamicsys- tem control input, it is however necessary to know the exact dynamic model, . are therefore necessary, these do not affect the controller in the con- sidered scenario (low accelerations, \ufb02at and horizontal road). Online car dynamic parameter identi\ufb01cation could be envisaged and seam- lessly integrated in our framework, whenever the above assumptions are not valid."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ".29 Torecoverthedynamicsys- tem control input, it is however necessary to know the exact dynamic model, .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08afdb8c-4dc8-4cd2-8f06-485de55272e9": {"__data__": {"id_": "08afdb8c-4dc8-4cd2-8f06-485de55272e9", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The human user/supervisor can intervene at any moment during the execution of thedrivingtask, .Theselection, .29 Torecoverthedynamicsys- tem control input, it is however necessary to know the exact dynamic model, . are therefore necessary, these do not affect the controller in the con- sidered scenario (low accelerations, \ufb02at and horizontal road). Online car dynamic parameter identi\ufb01cation could be envisaged and seam- lessly integrated in our framework, whenever the above assumptions are not valid. Note, however, that the proposed kinematic controller would remain valid, since it captures the theoretic challenge of driving in the presence of nonholonomic constraints."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "are therefore necessary, these do not affect the controller in the con- sidered scenario (low accelerations, \ufb02at and horizontal road).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8449f6ee-42e8-48b4-a7e8-fd8635ad1ade": {"__data__": {"id_": "8449f6ee-42e8-48b4-a7e8-fd8635ad1ade", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": ".29 Torecoverthedynamicsys- tem control input, it is however necessary to know the exact dynamic model, . are therefore necessary, these do not affect the controller in the con- sidered scenario (low accelerations, \ufb02at and horizontal road). Online car dynamic parameter identi\ufb01cation could be envisaged and seam- lessly integrated in our framework, whenever the above assumptions are not valid. Note, however, that the proposed kinematic controller would remain valid, since it captures the theoretic challenge of driving in the presence of nonholonomic constraints. the z-axis upward and the x-axis completing the right-handed frame [see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Online car dynamic parameter identi\ufb01cation could be envisaged and seam- lessly integrated in our framework, whenever the above assumptions are not valid.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2bbe6df-a25f-41e6-8f33-a702cc9e71ac": {"__data__": {"id_": "c2bbe6df-a25f-41e6-8f33-a702cc9e71ac", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "are therefore necessary, these do not affect the controller in the con- sidered scenario (low accelerations, \ufb02at and horizontal road). Online car dynamic parameter identi\ufb01cation could be envisaged and seam- lessly integrated in our framework, whenever the above assumptions are not valid. Note, however, that the proposed kinematic controller would remain valid, since it captures the theoretic challenge of driving in the presence of nonholonomic constraints. the z-axis upward and the x-axis completing the right-handed frame [see Fig. The path to be followed is de\ufb01ned as the set of - ders."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note, however, that the proposed kinematic controller would remain valid, since it captures the theoretic challenge of driving in the presence of nonholonomic constraints.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d3454ee-8ec2-431f-8a69-8b516495f1fe": {"__data__": {"id_": "4d3454ee-8ec2-431f-8a69-8b516495f1fe", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Online car dynamic parameter identi\ufb01cation could be envisaged and seam- lessly integrated in our framework, whenever the above assumptions are not valid. Note, however, that the proposed kinematic controller would remain valid, since it captures the theoretic challenge of driving in the presence of nonholonomic constraints. the z-axis upward and the x-axis completing the right-handed frame [see Fig. The path to be followed is de\ufb01ned as the set of - ders. On this path, we consider a tangent Frenet frame p, with origin on the normal projection of W on the path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the z-axis upward and the x-axis completing the right-handed frame [see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1792b58b-7730-4e70-87f7-97ad67dacb87": {"__data__": {"id_": "1792b58b-7730-4e70-87f7-97ad67dacb87", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note, however, that the proposed kinematic controller would remain valid, since it captures the theoretic challenge of driving in the presence of nonholonomic constraints. the z-axis upward and the x-axis completing the right-handed frame [see Fig. The path to be followed is de\ufb01ned as the set of - ders. On this path, we consider a tangent Frenet frame p, with origin on the normal projection of W on the path. Then, the car con\ufb01guration with respect to the path is de\ufb01ned by x, the Cartesian abscissa of W in p, and by \ud835\udf03, the car orientation with respect to the path tangent [(see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The path to be followed is de\ufb01ned as the set of - ders.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2865cfc3-509d-4f99-890c-8deb73445df1": {"__data__": {"id_": "2865cfc3-509d-4f99-890c-8deb73445df1", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the z-axis upward and the x-axis completing the right-handed frame [see Fig. The path to be followed is de\ufb01ned as the set of - ders. On this path, we consider a tangent Frenet frame p, with origin on the normal projection of W on the path. Then, the car con\ufb01guration with respect to the path is de\ufb01ned by x, the Cartesian abscissa of W in p, and by \ud835\udf03, the car orientation with respect to the path tangent [(see Fig. Describing the car motion through the model of a unicycle, with an upper curvature bound cM \u2208 \u211d+, x and \ud835\udf03 evolve according to"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On this path, we consider a tangent Frenet frame p, with origin on the normal projection of W on the path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10ba885a-9575-4557-96ec-865b9e91c8e6": {"__data__": {"id_": "10ba885a-9575-4557-96ec-865b9e91c8e6", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The path to be followed is de\ufb01ned as the set of - ders. On this path, we consider a tangent Frenet frame p, with origin on the normal projection of W on the path. Then, the car con\ufb01guration with respect to the path is de\ufb01ned by x, the Cartesian abscissa of W in p, and by \ud835\udf03, the car orientation with respect to the path tangent [(see Fig. Describing the car motion through the model of a unicycle, with an upper curvature bound cM \u2208 \u211d+, x and \ud835\udf03 evolve according to since the developed controller shows some robustness with respect to model parameters uncertainties as will be shown in Section 5."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the car con\ufb01guration with respect to the path is de\ufb01ned by x, the Cartesian abscissa of W in p, and by \ud835\udf03, the car orientation with respect to the path tangent [(see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1b1e2a8-aaeb-41a6-a9b4-dcdf2fb0fe1f": {"__data__": {"id_": "a1b1e2a8-aaeb-41a6-a9b4-dcdf2fb0fe1f", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "On this path, we consider a tangent Frenet frame p, with origin on the normal projection of W on the path. Then, the car con\ufb01guration with respect to the path is de\ufb01ned by x, the Cartesian abscissa of W in p, and by \ud835\udf03, the car orientation with respect to the path tangent [(see Fig. Describing the car motion through the model of a unicycle, with an upper curvature bound cM \u2208 \u211d+, x and \ud835\udf03 evolve according to since the developed controller shows some robustness with respect to model parameters uncertainties as will be shown in Section 5. where v and \ud835\udf14 represent, respectively, the linear and angular velocity of the unicycle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Describing the car motion through the model of a unicycle, with an upper curvature bound cM \u2208 \u211d+, x and \ud835\udf03 evolve according to", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7701903-29e4-49cb-b229-8bc35e1f88fc": {"__data__": {"id_": "a7701903-29e4-49cb-b229-8bc35e1f88fc", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Then, the car con\ufb01guration with respect to the path is de\ufb01ned by x, the Cartesian abscissa of W in p, and by \ud835\udf03, the car orientation with respect to the path tangent [(see Fig. Describing the car motion through the model of a unicycle, with an upper curvature bound cM \u2208 \u211d+, x and \ud835\udf03 evolve according to since the developed controller shows some robustness with respect to model parameters uncertainties as will be shown in Section 5. where v and \ud835\udf14 represent, respectively, the linear and angular velocity of the unicycle. The front wheel orientation \ud835\udf19 can be approximately related to v and \ud835\udf14 through"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "since the developed controller shows some robustness with respect to model parameters uncertainties as will be shown in Section 5.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2232864b-eae9-404a-83d5-ee7fdba1c3f6": {"__data__": {"id_": "2232864b-eae9-404a-83d5-ee7fdba1c3f6", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Describing the car motion through the model of a unicycle, with an upper curvature bound cM \u2208 \u211d+, x and \ud835\udf03 evolve according to since the developed controller shows some robustness with respect to model parameters uncertainties as will be shown in Section 5. where v and \ud835\udf14 represent, respectively, the linear and angular velocity of the unicycle. The front wheel orientation \ud835\udf19 can be approximately related to v and \ud835\udf14 through with l the constant distance between the rear and front wheel axes.\u2217"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where v and \ud835\udf14 represent, respectively, the linear and angular velocity of the unicycle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d1944e5-d3de-4aba-a440-9382d33e01c8": {"__data__": {"id_": "7d1944e5-d3de-4aba-a440-9382d33e01c8", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "since the developed controller shows some robustness with respect to model parameters uncertainties as will be shown in Section 5. where v and \ud835\udf14 represent, respectively, the linear and angular velocity of the unicycle. The front wheel orientation \ud835\udf19 can be approximately related to v and \ud835\udf14 through with l the constant distance between the rear and front wheel axes.\u2217 The steering wheel is shown in Figure 3, where we indicate, respec- tively, with h and s, the hand and steering wheel reference frames."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The front wheel orientation \ud835\udf19 can be approximately related to v and \ud835\udf14 through", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1ce9816-7b01-4d48-8537-0e8b142e078b": {"__data__": {"id_": "d1ce9816-7b01-4d48-8537-0e8b142e078b", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "where v and \ud835\udf14 represent, respectively, the linear and angular velocity of the unicycle. The front wheel orientation \ud835\udf19 can be approximately related to v and \ud835\udf14 through with l the constant distance between the rear and front wheel axes.\u2217 The steering wheel is shown in Figure 3, where we indicate, respec- tively, with h and s, the hand and steering wheel reference frames. Theoriginof s ,and\ud835\udefc istherotation around its z-axis, that points upward."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with l the constant distance between the rear and front wheel axes.\u2217", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01a8509f-ba7c-40bc-b97d-7b1f7b4b1cb0": {"__data__": {"id_": "01a8509f-ba7c-40bc-b97d-7b1f7b4b1cb0", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The front wheel orientation \ud835\udf19 can be approximately related to v and \ud835\udf14 through with l the constant distance between the rear and front wheel axes.\u2217 The steering wheel is shown in Figure 3, where we indicate, respec- tively, with h and s, the hand and steering wheel reference frames. Theoriginof s ,and\ud835\udefc istherotation around its z-axis, that points upward. Thus, positive values of \ud835\udefc make the car turn left (i.e., lead to negative \ud835\udf14)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The steering wheel is shown in Figure 3, where we indicate, respec- tively, with h and s, the hand and steering wheel reference frames.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12d780a3-35b5-46c5-98e9-ec2e25abc362": {"__data__": {"id_": "12d780a3-35b5-46c5-98e9-ec2e25abc362", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "with l the constant distance between the rear and front wheel axes.\u2217 The steering wheel is shown in Figure 3, where we indicate, respec- tively, with h and s, the hand and steering wheel reference frames. Theoriginof s ,and\ud835\udefc istherotation around its z-axis, that points upward. Thus, positive values of \ud835\udefc make the car turn left (i.e., lead to negative \ud835\udf14). 30 assumingthe front wheels orientation \ud835\udf19 to be proportional to the steering wheel angle \ud835\udefc, controlled by the driver hands, and \ufb01nally assuming small angles \ud835\udf14l\u2215v in (2), leads to"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Theoriginof s ,and\ud835\udefc istherotation around its z-axis, that points upward.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "469504e5-0497-4857-9bfc-88e568d45ba2": {"__data__": {"id_": "469504e5-0497-4857-9bfc-88e568d45ba2", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The steering wheel is shown in Figure 3, where we indicate, respec- tively, with h and s, the hand and steering wheel reference frames. Theoriginof s ,and\ud835\udefc istherotation around its z-axis, that points upward. Thus, positive values of \ud835\udefc make the car turn left (i.e., lead to negative \ud835\udf14). 30 assumingthe front wheels orientation \ud835\udf19 to be proportional to the steering wheel angle \ud835\udefc, controlled by the driver hands, and \ufb01nally assuming small angles \ud835\udf14l\u2215v in (2), leads to with k\ud835\udefc a negative\u2020 scalar, characteristic of the car, accounting also for l."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, positive values of \ud835\udefc make the car turn left (i.e., lead to negative \ud835\udf14).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a086fc32-218f-42ae-88f4-f7a6735b99eb": {"__data__": {"id_": "a086fc32-218f-42ae-88f4-f7a6735b99eb", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Theoriginof s ,and\ud835\udefc istherotation around its z-axis, that points upward. Thus, positive values of \ud835\udefc make the car turn left (i.e., lead to negative \ud835\udf14). 30 assumingthe front wheels orientation \ud835\udf19 to be proportional to the steering wheel angle \ud835\udefc, controlled by the driver hands, and \ufb01nally assuming small angles \ud835\udf14l\u2215v in (2), leads to with k\ud835\udefc a negative\u2020 scalar, characteristic of the car, accounting also for l. \u2217 Bounds on the front wheels orientation characterizing common service cars induce the max- imum curvature constraint in (1)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "30 assumingthe front wheels orientation \ud835\udf19 to be proportional to the steering wheel angle \ud835\udefc, controlled by the driver hands, and \ufb01nally assuming small angles \ud835\udf14l\u2215v in (2), leads to", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2787a4eb-d9be-4993-845d-909ec48bc01b": {"__data__": {"id_": "2787a4eb-d9be-4993-845d-909ec48bc01b", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Thus, positive values of \ud835\udefc make the car turn left (i.e., lead to negative \ud835\udf14). 30 assumingthe front wheels orientation \ud835\udf19 to be proportional to the steering wheel angle \ud835\udefc, controlled by the driver hands, and \ufb01nally assuming small angles \ud835\udf14l\u2215v in (2), leads to with k\ud835\udefc a negative\u2020 scalar, characteristic of the car, accounting also for l. \u2217 Bounds on the front wheels orientation characterizing common service cars induce the max- imum curvature constraint in (1). According to experimental observations, at low velocities, the relationship between the pedal inclination and the car acceleration is linear:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with k\ud835\udefc a negative\u2020 scalar, characteristic of the car, accounting also for l.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "193db374-b10c-479c-b61e-8cf390c96217": {"__data__": {"id_": "193db374-b10c-479c-b61e-8cf390c96217", "embedding": null, "metadata": {"page_number": 4, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "30 assumingthe front wheels orientation \ud835\udf19 to be proportional to the steering wheel angle \ud835\udefc, controlled by the driver hands, and \ufb01nally assuming small angles \ud835\udf14l\u2215v in (2), leads to with k\ud835\udefc a negative\u2020 scalar, characteristic of the car, accounting also for l. \u2217 Bounds on the front wheels orientation characterizing common service cars induce the max- imum curvature constraint in (1). According to experimental observations, at low velocities, the relationship between the pedal inclination and the car acceleration is linear: Assuming small values of \u0394qa and \u0394\ud835\udf01, the point of contact between the foot and the pedal can be considered \ufb01xed on both the foot and the pedal, that is, the length of the segment C2C3 in Figure 4(b) can be considered close to zero.\u2217 Hence, the relationship between \u0394qa and \u0394\ud835\udf01 is easily found to be"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2217 Bounds on the front wheels orientation characterizing common service cars induce the max- imum curvature constraint in (1).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72c75b8e-b856-43ef-b8fa-dbf161e0ab93": {"__data__": {"id_": "72c75b8e-b856-43ef-b8fa-dbf161e0ab93", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "with k\ud835\udefc a negative\u2020 scalar, characteristic of the car, accounting also for l. \u2217 Bounds on the front wheels orientation characterizing common service cars induce the max- imum curvature constraint in (1). According to experimental observations, at low velocities, the relationship between the pedal inclination and the car acceleration is linear: Assuming small values of \u0394qa and \u0394\ud835\udf01, the point of contact between the foot and the pedal can be considered \ufb01xed on both the foot and the pedal, that is, the length of the segment C2C3 in Figure 4(b) can be considered close to zero.\u2217 Hence, the relationship between \u0394qa and \u0394\ud835\udf01 is easily found to be where la (lp) is the distance of the ankle (pedal) rotation axis from the contact point of the foot with the pedal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "According to experimental observations, at low velocities, the relationship between the pedal inclination and the car acceleration is linear:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a98604bb-944a-4d6d-9971-ab49a8a604ca": {"__data__": {"id_": "a98604bb-944a-4d6d-9971-ab49a8a604ca", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2217 Bounds on the front wheels orientation characterizing common service cars induce the max- imum curvature constraint in (1). According to experimental observations, at low velocities, the relationship between the pedal inclination and the car acceleration is linear: Assuming small values of \u0394qa and \u0394\ud835\udf01, the point of contact between the foot and the pedal can be considered \ufb01xed on both the foot and the pedal, that is, the length of the segment C2C3 in Figure 4(b) can be considered close to zero.\u2217 Hence, the relationship between \u0394qa and \u0394\ud835\udf01 is easily found to be where la (lp) is the distance of the ankle (pedal) rotation axis from the contact point of the foot with the pedal. We also indicate with c the robot camera frame (see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assuming small values of \u0394qa and \u0394\ud835\udf01, the point of contact between the foot and the pedal can be considered \ufb01xed on both the foot and the pedal, that is, the length of the segment C2C3 in Figure 4(b) can be considered close to zero.\u2217 Hence, the relationship between \u0394qa and \u0394\ud835\udf01 is easily found to be", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad3149a7-3647-4fd1-a9e6-70f01aa40662": {"__data__": {"id_": "ad3149a7-3647-4fd1-a9e6-70f01aa40662", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "According to experimental observations, at low velocities, the relationship between the pedal inclination and the car acceleration is linear: Assuming small values of \u0394qa and \u0394\ud835\udf01, the point of contact between the foot and the pedal can be considered \ufb01xed on both the foot and the pedal, that is, the length of the segment C2C3 in Figure 4(b) can be considered close to zero.\u2217 Hence, the relationship between \u0394qa and \u0394\ud835\udf01 is easily found to be where la (lp) is the distance of the ankle (pedal) rotation axis from the contact point of the foot with the pedal. We also indicate with c the robot camera frame (see Fig. Its origin is in the optical center of the camera, with the z-axis coinci- dent with the focal axis."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where la (lp) is the distance of the ankle (pedal) rotation axis from the contact point of the foot with the pedal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8629a9fd-2c10-4648-b5d6-c5a26f59aa12": {"__data__": {"id_": "8629a9fd-2c10-4648-b5d6-c5a26f59aa12", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Assuming small values of \u0394qa and \u0394\ud835\udf01, the point of contact between the foot and the pedal can be considered \ufb01xed on both the foot and the pedal, that is, the length of the segment C2C3 in Figure 4(b) can be considered close to zero.\u2217 Hence, the relationship between \u0394qa and \u0394\ud835\udf01 is easily found to be where la (lp) is the distance of the ankle (pedal) rotation axis from the contact point of the foot with the pedal. We also indicate with c the robot camera frame (see Fig. Its origin is in the optical center of the camera, with the z-axis coinci- dent with the focal axis. The y-axis points downwards, and the x-axis completes the right-handed frame."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also indicate with c the robot camera frame (see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d266206a-7ca6-48a4-8082-f8abc33673ee": {"__data__": {"id_": "d266206a-7ca6-48a4-8082-f8abc33673ee", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "where la (lp) is the distance of the ankle (pedal) rotation axis from the contact point of the foot with the pedal. We also indicate with c the robot camera frame (see Fig. Its origin is in the optical center of the camera, with the z-axis coinci- dent with the focal axis. The y-axis points downwards, and the x-axis completes the right-handed frame. c is tilted by an angle \ud835\udefe (taken positive downwards) with respect to the frame w, whereas the vec- torpw c = (xw c ,yw c ,zw c )T expressed in the car reference frame."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Its origin is in the optical center of the camera, with the z-axis coinci- dent with the focal axis.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ff945c6-d5d6-4b01-b4e7-da2e5ea3ea46": {"__data__": {"id_": "1ff945c6-d5d6-4b01-b4e7-da2e5ea3ea46", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We also indicate with c the robot camera frame (see Fig. Its origin is in the optical center of the camera, with the z-axis coinci- dent with the focal axis. The y-axis points downwards, and the x-axis completes the right-handed frame. c is tilted by an angle \ud835\udefe (taken positive downwards) with respect to the frame w, whereas the vec- torpw c = (xw c ,yw c ,zw c )T expressed in the car reference frame. It consists of leading the car on the path and aligning it with the path tangent:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis points downwards, and the x-axis completes the right-handed frame.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b3e3c96-ec00-4dd0-be78-0c879081f4ef": {"__data__": {"id_": "9b3e3c96-ec00-4dd0-be78-0c879081f4ef", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Its origin is in the optical center of the camera, with the z-axis coinci- dent with the focal axis. The y-axis points downwards, and the x-axis completes the right-handed frame. c is tilted by an angle \ud835\udefe (taken positive downwards) with respect to the frame w, whereas the vec- torpw c = (xw c ,yw c ,zw c )T expressed in the car reference frame. It consists of leading the car on the path and aligning it with the path tangent: angle \ud835\udefc. Concurrently, (7) is achieved by the car velocity control real- ized by the robot foot that sets a proper angle \ud835\udf01 for the gas pedal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "c is tilted by an angle \ud835\udefe (taken positive downwards) with respect to the frame w, whereas the vec- torpw c = (xw c ,yw c ,zw c )T expressed in the car reference frame.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1166019f-9ca7-41f7-99db-e032c048c7c9": {"__data__": {"id_": "1166019f-9ca7-41f7-99db-e032c048c7c9", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The y-axis points downwards, and the x-axis completes the right-handed frame. c is tilted by an angle \ud835\udefe (taken positive downwards) with respect to the frame w, whereas the vec- torpw c = (xw c ,yw c ,zw c )T expressed in the car reference frame. It consists of leading the car on the path and aligning it with the path tangent: angle \ud835\udefc. Concurrently, (7) is achieved by the car velocity control real- ized by the robot foot that sets a proper angle \ud835\udf01 for the gas pedal. The computationof\ud835\udefc and\ud835\udf01 ,thatisdetailed in the next section."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It consists of leading the car on the path and aligning it with the path tangent:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ed1b753-f45a-4b1d-84b8-d58dc498856d": {"__data__": {"id_": "6ed1b753-f45a-4b1d-84b8-d58dc498856d", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "c is tilted by an angle \ud835\udefe (taken positive downwards) with respect to the frame w, whereas the vec- torpw c = (xw c ,yw c ,zw c )T expressed in the car reference frame. It consists of leading the car on the path and aligning it with the path tangent: angle \ud835\udefc. Concurrently, (7) is achieved by the car velocity control real- ized by the robot foot that sets a proper angle \ud835\udf01 for the gas pedal. The computationof\ud835\udefc and\ud835\udf01 ,thatisdetailed in the next section. The block diagram of Figure 1 shows our perception\u2013action approach."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "angle \ud835\udefc. Concurrently, (7) is achieved by the car velocity control real- ized by the robot foot that sets a proper angle \ud835\udf01 for the gas pedal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a0cc928-3b50-45cb-b9ca-9098b081d4a2": {"__data__": {"id_": "4a0cc928-3b50-45cb-b9ca-9098b081d4a2", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "It consists of leading the car on the path and aligning it with the path tangent: angle \ud835\udefc. Concurrently, (7) is achieved by the car velocity control real- ized by the robot foot that sets a proper angle \ud835\udf01 for the gas pedal. The computationof\ud835\udefc and\ud835\udf01 ,thatisdetailed in the next section. The block diagram of Figure 1 shows our perception\u2013action approach. At a higher level, the perception block, whose details are described in this section, provides the feedback signals for the car and robot control."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The computationof\ud835\udefc and\ud835\udf01 ,thatisdetailed in the next section.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e353e85-484a-4fef-b386-be41dacb5efc": {"__data__": {"id_": "5e353e85-484a-4fef-b386-be41dacb5efc", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "angle \ud835\udefc. Concurrently, (7) is achieved by the car velocity control real- ized by the robot foot that sets a proper angle \ud835\udf01 for the gas pedal. The computationof\ud835\udefc and\ud835\udf01 ,thatisdetailed in the next section. The block diagram of Figure 1 shows our perception\u2013action approach. At a higher level, the perception block, whose details are described in this section, provides the feedback signals for the car and robot control. Task (6) is achieved by the steering control that uses the kinematic model (1) and is realized by the robot hand according to the steering"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The block diagram of Figure 1 shows our perception\u2013action approach.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eaf33d80-d583-47b1-8249-d4bee3324231": {"__data__": {"id_": "eaf33d80-d583-47b1-8249-d4bee3324231", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The computationof\ud835\udefc and\ud835\udf01 ,thatisdetailed in the next section. The block diagram of Figure 1 shows our perception\u2013action approach. At a higher level, the perception block, whose details are described in this section, provides the feedback signals for the car and robot control. Task (6) is achieved by the steering control that uses the kinematic model (1) and is realized by the robot hand according to the steering (i)thevanishingpoint(V),thatis, , and (ii) the middle point (M), that is, the midpoint of the segment con- ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At a higher level, the perception block, whose details are described in this section, provides the feedback signals for the car and robot control.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c28825f8-0ade-4bbc-b0da-09891da199f0": {"__data__": {"id_": "c28825f8-0ade-4bbc-b0da-09891da199f0", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The block diagram of Figure 1 shows our perception\u2013action approach. At a higher level, the perception block, whose details are described in this section, provides the feedback signals for the car and robot control. Task (6) is achieved by the steering control that uses the kinematic model (1) and is realized by the robot hand according to the steering (i)thevanishingpoint(V),thatis, , and (ii) the middle point (M), that is, the midpoint of the segment con- . However, this length, along with angles \u0394qa and \u0394\ud835\udf01, is almost null."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Task (6) is achieved by the steering control that uses the kinematic model (1) and is realized by the robot hand according to the steering", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e537a22-9251-4f30-a93f-34626165089c": {"__data__": {"id_": "5e537a22-9251-4f30-a93f-34626165089c", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "At a higher level, the perception block, whose details are described in this section, provides the feedback signals for the car and robot control. Task (6) is achieved by the steering control that uses the kinematic model (1) and is realized by the robot hand according to the steering (i)thevanishingpoint(V),thatis, , and (ii) the middle point (M), that is, the midpoint of the segment con- . However, this length, along with angles \u0394qa and \u0394\ud835\udf01, is almost null. the state-of-the-art understanding on road/lane detection, but rather ,wedevelop a simple image processing algorithm for road border extraction."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(i)thevanishingpoint(V),thatis, , and (ii) the middle point (M), that is, the midpoint of the segment con- .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72ecdd6c-737d-4cd2-a5bb-29374433dd54": {"__data__": {"id_": "72ecdd6c-737d-4cd2-a5bb-29374433dd54", "embedding": null, "metadata": {"page_number": 5, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Task (6) is achieved by the steering control that uses the kinematic model (1) and is realized by the robot hand according to the steering (i)thevanishingpoint(V),thatis, , and (ii) the middle point (M), that is, the midpoint of the segment con- . However, this length, along with angles \u0394qa and \u0394\ud835\udf01, is almost null. the state-of-the-art understanding on road/lane detection, but rather ,wedevelop a simple image processing algorithm for road border extraction. More complex algorithms can be used to improve the detection and track- ing of the road,31\u201334 or even to detect road markings.35 However, our method has the advantage of being based solely on vision, avoiding the complexity induced by integration of other sensors.36,37 Note that to \ufb01nd in open-code source or binary."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, this length, along with angles \u0394qa and \u0394\ud835\udf01, is almost null.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a38e4d6-b2cc-438b-bde4-f73c85117410": {"__data__": {"id_": "2a38e4d6-b2cc-438b-bde4-f73c85117410", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "(i)thevanishingpoint(V),thatis, , and (ii) the middle point (M), that is, the midpoint of the segment con- . However, this length, along with angles \u0394qa and \u0394\ud835\udf01, is almost null. the state-of-the-art understanding on road/lane detection, but rather ,wedevelop a simple image processing algorithm for road border extraction. More complex algorithms can be used to improve the detection and track- ing of the road,31\u201334 or even to detect road markings.35 However, our method has the advantage of being based solely on vision, avoiding the complexity induced by integration of other sensors.36,37 Note that to \ufb01nd in open-code source or binary. \u2022 To remove \u201csalt and pepper noise,\u201d the dilation and erosion oper- ators are applied to the binary images."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the state-of-the-art understanding on road/lane detection, but rather ,wedevelop a simple image processing algorithm for road border extraction.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f7d4a8b-c353-437e-8455-27adfe9182b0": {"__data__": {"id_": "9f7d4a8b-c353-437e-8455-27adfe9182b0", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "However, this length, along with angles \u0394qa and \u0394\ud835\udf01, is almost null. the state-of-the-art understanding on road/lane detection, but rather ,wedevelop a simple image processing algorithm for road border extraction. More complex algorithms can be used to improve the detection and track- ing of the road,31\u201334 or even to detect road markings.35 However, our method has the advantage of being based solely on vision, avoiding the complexity induced by integration of other sensors.36,37 Note that to \ufb01nd in open-code source or binary. \u2022 To remove \u201csalt and pepper noise,\u201d the dilation and erosion oper- ators are applied to the binary images. Then, the two images are merged by using the OR logic operator to obtain a mask of the road [Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More complex algorithms can be used to improve the detection and track- ing of the road,31\u201334 or even to detect road markings.35 However, our method has the advantage of being based solely on vision, avoiding the complexity induced by integration of other sensors.36,37 Note that to \ufb01nd in open-code source or binary.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "967f6fe5-3f71-4e52-a4c3-2ff78acd5b39": {"__data__": {"id_": "967f6fe5-3f71-4e52-a4c3-2ff78acd5b39", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the state-of-the-art understanding on road/lane detection, but rather ,wedevelop a simple image processing algorithm for road border extraction. More complex algorithms can be used to improve the detection and track- ing of the road,31\u201334 or even to detect road markings.35 However, our method has the advantage of being based solely on vision, avoiding the complexity induced by integration of other sensors.36,37 Note that to \ufb01nd in open-code source or binary. \u2022 To remove \u201csalt and pepper noise,\u201d the dilation and erosion oper- ators are applied to the binary images. Then, the two images are merged by using the OR logic operator to obtain a mask of the road [Fig. \u2022 The convex hull is computed with areas greater than a given thresh- old on the mask found in the previous step; then, a Gaussian \ufb01lter is applied for smoothing."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 To remove \u201csalt and pepper noise,\u201d the dilation and erosion oper- ators are applied to the binary images.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b6c9b64-df71-4778-8b08-aa8a647f4751": {"__data__": {"id_": "8b6c9b64-df71-4778-8b08-aa8a647f4751", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "More complex algorithms can be used to improve the detection and track- ing of the road,31\u201334 or even to detect road markings.35 However, our method has the advantage of being based solely on vision, avoiding the complexity induced by integration of other sensors.36,37 Note that to \ufb01nd in open-code source or binary. \u2022 To remove \u201csalt and pepper noise,\u201d the dilation and erosion oper- ators are applied to the binary images. Then, the two images are merged by using the OR logic operator to obtain a mask of the road [Fig. \u2022 The convex hull is computed with areas greater than a given thresh- old on the mask found in the previous step; then, a Gaussian \ufb01lter is applied for smoothing. Part of the road borders extraction procedure follows standard techniques used in the \ufb01eld of computer vision38 and is based on the OpenCV library39 that provides ready-to-use methods for our vision- based algorithm."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the two images are merged by using the OR logic operator to obtain a mask of the road [Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0096cdb0-e4f5-4b34-b322-cd36c7a3969c": {"__data__": {"id_": "0096cdb0-e4f5-4b34-b322-cd36c7a3969c", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 To remove \u201csalt and pepper noise,\u201d the dilation and erosion oper- ators are applied to the binary images. Then, the two images are merged by using the OR logic operator to obtain a mask of the road [Fig. \u2022 The convex hull is computed with areas greater than a given thresh- old on the mask found in the previous step; then, a Gaussian \ufb01lter is applied for smoothing. Part of the road borders extraction procedure follows standard techniques used in the \ufb01eld of computer vision38 and is based on the OpenCV library39 that provides ready-to-use methods for our vision- based algorithm. More in detail, the steps used for the detection of theroadbordersonthe , with reference to Figure 6."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The convex hull is computed with areas greater than a given thresh- old on the mask found in the previous step; then, a Gaussian \ufb01lter is applied for smoothing.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4fa28c3-a558-4e0c-ba30-c38c41bb876c": {"__data__": {"id_": "e4fa28c3-a558-4e0c-ba30-c38c41bb876c", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Then, the two images are merged by using the OR logic operator to obtain a mask of the road [Fig. \u2022 The convex hull is computed with areas greater than a given thresh- old on the mask found in the previous step; then, a Gaussian \ufb01lter is applied for smoothing. Part of the road borders extraction procedure follows standard techniques used in the \ufb01eld of computer vision38 and is based on the OpenCV library39 that provides ready-to-use methods for our vision- based algorithm. More in detail, the steps used for the detection of theroadbordersonthe , with reference to Figure 6. \u2022 Fromtheimage,aregionofinterest(ROI),shownwithwhiteborders in Figure 6(a), is manually selected at the initialization and kept con- .Then,ateachcycleoftheimage processing, we compute the average and standard deviation of hue and saturation channels of the HSV (hue, saturation and value) color space on two central rectangular areas in the ROI."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part of the road borders extraction procedure follows standard techniques used in the \ufb01eld of computer vision38 and is based on the OpenCV library39 that provides ready-to-use methods for our vision- based algorithm.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50cd0038-399c-47fe-b3d6-d43d1eb45635": {"__data__": {"id_": "50cd0038-399c-47fe-b3d6-d43d1eb45635", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 The convex hull is computed with areas greater than a given thresh- old on the mask found in the previous step; then, a Gaussian \ufb01lter is applied for smoothing. Part of the road borders extraction procedure follows standard techniques used in the \ufb01eld of computer vision38 and is based on the OpenCV library39 that provides ready-to-use methods for our vision- based algorithm. More in detail, the steps used for the detection of theroadbordersonthe , with reference to Figure 6. \u2022 Fromtheimage,aregionofinterest(ROI),shownwithwhiteborders in Figure 6(a), is manually selected at the initialization and kept con- .Then,ateachcycleoftheimage processing, we compute the average and standard deviation of hue and saturation channels of the HSV (hue, saturation and value) color space on two central rectangular areas in the ROI. These values are considered for the thresholding operations described in the next step."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More in detail, the steps used for the detection of theroadbordersonthe , with reference to Figure 6.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "772616ed-69a7-421a-87d8-ce757d9fb282": {"__data__": {"id_": "772616ed-69a7-421a-87d8-ce757d9fb282", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Part of the road borders extraction procedure follows standard techniques used in the \ufb01eld of computer vision38 and is based on the OpenCV library39 that provides ready-to-use methods for our vision- based algorithm. More in detail, the steps used for the detection of theroadbordersonthe , with reference to Figure 6. \u2022 Fromtheimage,aregionofinterest(ROI),shownwithwhiteborders in Figure 6(a), is manually selected at the initialization and kept con- .Then,ateachcycleoftheimage processing, we compute the average and standard deviation of hue and saturation channels of the HSV (hue, saturation and value) color space on two central rectangular areas in the ROI. These values are considered for the thresholding operations described in the next step. 6(g)], is applied to detect the line segments on the image."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Fromtheimage,aregionofinterest(ROI),shownwithwhiteborders in Figure 6(a), is manually selected at the initialization and kept con- .Then,ateachcycleoftheimage processing, we compute the average and standard deviation of hue and saturation channels of the HSV (hue, saturation and value) color space on two central rectangular areas in the ROI.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c969d3b-5305-4adc-b263-149ccedc57bd": {"__data__": {"id_": "1c969d3b-5305-4adc-b263-149ccedc57bd", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "More in detail, the steps used for the detection of theroadbordersonthe , with reference to Figure 6. \u2022 Fromtheimage,aregionofinterest(ROI),shownwithwhiteborders in Figure 6(a), is manually selected at the initialization and kept con- .Then,ateachcycleoftheimage processing, we compute the average and standard deviation of hue and saturation channels of the HSV (hue, saturation and value) color space on two central rectangular areas in the ROI. These values are considered for the thresholding operations described in the next step. 6(g)], is applied to detect the line segments on the image. \u2022 Similar segments are merged,\u2217 as depicted in Figure 6(h)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These values are considered for the thresholding operations described in the next step.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6a417a7-f5e4-4449-b01d-368a37c13bc0": {"__data__": {"id_": "b6a417a7-f5e4-4449-b01d-368a37c13bc0", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Fromtheimage,aregionofinterest(ROI),shownwithwhiteborders in Figure 6(a), is manually selected at the initialization and kept con- .Then,ateachcycleoftheimage processing, we compute the average and standard deviation of hue and saturation channels of the HSV (hue, saturation and value) color space on two central rectangular areas in the ROI. These values are considered for the thresholding operations described in the next step. 6(g)], is applied to detect the line segments on the image. \u2022 Similar segments are merged,\u2217 as depicted in Figure 6(h). with arti\ufb01cial ones, de\ufb01ned of\ufb02ine as oblique lines that, according to the geometry of the road and to the con\ufb01gu- ration of the camera, most likely correspond to the road borders."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6(g)], is applied to detect the line segments on the image.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ed98fa3-a9ba-4ac6-82b4-c680ab94eda4": {"__data__": {"id_": "8ed98fa3-a9ba-4ac6-82b4-c680ab94eda4", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "These values are considered for the thresholding operations described in the next step. 6(g)], is applied to detect the line segments on the image. \u2022 Similar segments are merged,\u2217 as depicted in Figure 6(h). with arti\ufb01cial ones, de\ufb01ned of\ufb02ine as oblique lines that, according to the geometry of the road and to the con\ufb01gu- ration of the camera, most likely correspond to the road borders. This allows the computation of the vanishing and middle point even when one (or both) real road borders are not correctly detected."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Similar segments are merged,\u2217 as depicted in Figure 6(h).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2b2007f-156c-44c4-bdfa-3a1e8b2f1863": {"__data__": {"id_": "d2b2007f-156c-44c4-bdfa-3a1e8b2f1863", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "6(g)], is applied to detect the line segments on the image. \u2022 Similar segments are merged,\u2217 as depicted in Figure 6(h). with arti\ufb01cial ones, de\ufb01ned of\ufb02ine as oblique lines that, according to the geometry of the road and to the con\ufb01gu- ration of the camera, most likely correspond to the road borders. This allows the computation of the vanishing and middle point even when one (or both) real road borders are not correctly detected. \u2022 Twobinaryimages[Figs.6(b)and6(c)] pixels in the ROI, whose hue and saturation value are in the ranges (average \u00b1 standard deviation) de\ufb01ned in the previous step."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with arti\ufb01cial ones, de\ufb01ned of\ufb02ine as oblique lines that, according to the geometry of the road and to the con\ufb01gu- ration of the camera, most likely correspond to the road borders.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e08a0dd1-0358-424e-a718-aba261e28efa": {"__data__": {"id_": "e08a0dd1-0358-424e-a718-aba261e28efa", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Similar segments are merged,\u2217 as depicted in Figure 6(h). with arti\ufb01cial ones, de\ufb01ned of\ufb02ine as oblique lines that, according to the geometry of the road and to the con\ufb01gu- ration of the camera, most likely correspond to the road borders. This allows the computation of the vanishing and middle point even when one (or both) real road borders are not correctly detected. \u2022 Twobinaryimages[Figs.6(b)and6(c)] pixels in the ROI, whose hue and saturation value are in the ranges (average \u00b1 standard deviation) de\ufb01ned in the previous step. with state composed of the slope and intercept of the two borders.\u2217 In the prediction step, the KF models the position of lines on the image plane as constant (a reasonable design choice, under Assumption 3, of locally \ufb02at and straight road), whereas the measurement step uses the road borders as detected in the current image."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This allows the computation of the vanishing and middle point even when one (or both) real road borders are not correctly detected.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6936279-6bb3-41c1-8440-2afccd0719ee": {"__data__": {"id_": "e6936279-6bb3-41c1-8440-2afccd0719ee", "embedding": null, "metadata": {"page_number": 6, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "with arti\ufb01cial ones, de\ufb01ned of\ufb02ine as oblique lines that, according to the geometry of the road and to the con\ufb01gu- ration of the camera, most likely correspond to the road borders. This allows the computation of the vanishing and middle point even when one (or both) real road borders are not correctly detected. \u2022 Twobinaryimages[Figs.6(b)and6(c)] pixels in the ROI, whose hue and saturation value are in the ranges (average \u00b1 standard deviation) de\ufb01ned in the previous step. with state composed of the slope and intercept of the two borders.\u2217 In the prediction step, the KF models the position of lines on the image plane as constant (a reasonable design choice, under Assumption 3, of locally \ufb02at and straight road), whereas the measurement step uses the road borders as detected in the current image. The measurement is based on both the optical \ufb02ow (vOF), and the output of the IMU accelerometers (aIMU)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Twobinaryimages[Figs.6(b)and6(c)] pixels in the ROI, whose hue and saturation value are in the ranges (average \u00b1 standard deviation) de\ufb01ned in the previous step.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8dbebd5b-af6b-4b07-9e0c-eaf66d730507": {"__data__": {"id_": "8dbebd5b-af6b-4b07-9e0c-eaf66d730507", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This allows the computation of the vanishing and middle point even when one (or both) real road borders are not correctly detected. \u2022 Twobinaryimages[Figs.6(b)and6(c)] pixels in the ROI, whose hue and saturation value are in the ranges (average \u00b1 standard deviation) de\ufb01ned in the previous step. with state composed of the slope and intercept of the two borders.\u2217 In the prediction step, the KF models the position of lines on the image plane as constant (a reasonable design choice, under Assumption 3, of locally \ufb02at and straight road), whereas the measurement step uses the road borders as detected in the current image. The measurement is based on both the optical \ufb02ow (vOF), and the output of the IMU accelerometers (aIMU). Then, the estimation of the car velocity v will correspond to the \ufb01rst element of state vector \ud835\udf43."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with state composed of the slope and intercept of the two borders.\u2217 In the prediction step, the KF models the position of lines on the image plane as constant (a reasonable design choice, under Assumption 3, of locally \ufb02at and straight road), whereas the measurement step uses the road borders as detected in the current image.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d917202-dda6-4179-9bf2-764a4ac5181f": {"__data__": {"id_": "3d917202-dda6-4179-9bf2-764a4ac5181f", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Twobinaryimages[Figs.6(b)and6(c)] pixels in the ROI, whose hue and saturation value are in the ranges (average \u00b1 standard deviation) de\ufb01ned in the previous step. with state composed of the slope and intercept of the two borders.\u2217 In the prediction step, the KF models the position of lines on the image plane as constant (a reasonable design choice, under Assumption 3, of locally \ufb02at and straight road), whereas the measurement step uses the road borders as detected in the current image. The measurement is based on both the optical \ufb02ow (vOF), and the output of the IMU accelerometers (aIMU). Then, the estimation of the car velocity v will correspond to the \ufb01rst element of state vector \ud835\udf43. The process to obtain vOF and aIMU is detailed below."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The measurement is based on both the optical \ufb02ow (vOF), and the output of the IMU accelerometers (aIMU).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "493a31d2-f938-48b5-9d39-6bfef4e7f300": {"__data__": {"id_": "493a31d2-f938-48b5-9d39-6bfef4e7f300", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "with state composed of the slope and intercept of the two borders.\u2217 In the prediction step, the KF models the position of lines on the image plane as constant (a reasonable design choice, under Assumption 3, of locally \ufb02at and straight road), whereas the measurement step uses the road borders as detected in the current image. The measurement is based on both the optical \ufb02ow (vOF), and the output of the IMU accelerometers (aIMU). Then, the estimation of the car velocity v will correspond to the \ufb01rst element of state vector \ud835\udf43. The process to obtain vOF and aIMU is detailed below. From the obtained road borders (shown in red in Figure 6(a)), the vanishing and middle point are derived, with simple geometrical com- putations."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the estimation of the car velocity v will correspond to the \ufb01rst element of state vector \ud835\udf43.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2e8911f-a156-4f6c-8a0e-c9ba93a1ecf3": {"__data__": {"id_": "a2e8911f-a156-4f6c-8a0e-c9ba93a1ecf3", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The measurement is based on both the optical \ufb02ow (vOF), and the output of the IMU accelerometers (aIMU). Then, the estimation of the car velocity v will correspond to the \ufb01rst element of state vector \ud835\udf43. The process to obtain vOF and aIMU is detailed below. From the obtained road borders (shown in red in Figure 6(a)), the vanishing and middle point are derived, with simple geometrical com- putations. Their values are then smoothed with a low-pass frequency \ufb01lter and \ufb01nally fed to the steering control, which will be described in Section 5.1."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The process to obtain vOF and aIMU is detailed below.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b157d691-f164-4442-bc9c-0805c1d0653a": {"__data__": {"id_": "b157d691-f164-4442-bc9c-0805c1d0653a", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Then, the estimation of the car velocity v will correspond to the \ufb01rst element of state vector \ud835\udf43. The process to obtain vOF and aIMU is detailed below. From the obtained road borders (shown in red in Figure 6(a)), the vanishing and middle point are derived, with simple geometrical com- putations. Their values are then smoothed with a low-pass frequency \ufb01lter and \ufb01nally fed to the steering control, which will be described in Section 5.1. 4.2.1 Measure of the car speed with optical \ufb02ow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the obtained road borders (shown in red in Figure 6(a)), the vanishing and middle point are derived, with simple geometrical com- putations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fde62790-a231-4123-a4ec-a7681cf332ad": {"__data__": {"id_": "fde62790-a231-4123-a4ec-a7681cf332ad", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The process to obtain vOF and aIMU is detailed below. From the obtained road borders (shown in red in Figure 6(a)), the vanishing and middle point are derived, with simple geometrical com- putations. Their values are then smoothed with a low-pass frequency \ufb01lter and \ufb01nally fed to the steering control, which will be described in Section 5.1. 4.2.1 Measure of the car speed with optical \ufb02ow Optical \ufb02ow can be used to reconstruct the motion of the camera, and from that, assuming that the transformation from the robot camera frame to the car frame is known, it is straightforward to derive the vehicle velocity."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Their values are then smoothed with a low-pass frequency \ufb01lter and \ufb01nally fed to the steering control, which will be described in Section 5.1.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26fae75b-b907-437b-af2e-6dc9e82ab20c": {"__data__": {"id_": "26fae75b-b907-437b-af2e-6dc9e82ab20c", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "From the obtained road borders (shown in red in Figure 6(a)), the vanishing and middle point are derived, with simple geometrical com- putations. Their values are then smoothed with a low-pass frequency \ufb01lter and \ufb01nally fed to the steering control, which will be described in Section 5.1. 4.2.1 Measure of the car speed with optical \ufb02ow Optical \ufb02ow can be used to reconstruct the motion of the camera, and from that, assuming that the transformation from the robot camera frame to the car frame is known, it is straightforward to derive the vehicle velocity. we use the robot cam- eratomeasuretheoptical\ufb02ow, visual features, due to the relative motion between camera and scene."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.2.1 Measure of the car speed with optical \ufb02ow", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f8abb0f-28bf-4adb-a846-0985087b36e1": {"__data__": {"id_": "3f8abb0f-28bf-4adb-a846-0985087b36e1", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Their values are then smoothed with a low-pass frequency \ufb01lter and \ufb01nally fed to the steering control, which will be described in Section 5.1. 4.2.1 Measure of the car speed with optical \ufb02ow Optical \ufb02ow can be used to reconstruct the motion of the camera, and from that, assuming that the transformation from the robot camera frame to the car frame is known, it is straightforward to derive the vehicle velocity. we use the robot cam- eratomeasuretheoptical\ufb02ow, visual features, due to the relative motion between camera and scene. the six-dimensional velocity vector v c of the frame c can be related to the velocity of the point tracked in the image \u0307x through the following relation: p"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Optical \ufb02ow can be used to reconstruct the motion of the camera, and from that, assuming that the transformation from the robot camera frame to the car frame is known, it is straightforward to derive the vehicle velocity.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "885fa230-71eb-48cb-a42e-5eb05a7d8a00": {"__data__": {"id_": "885fa230-71eb-48cb-a42e-5eb05a7d8a00", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "4.2.1 Measure of the car speed with optical \ufb02ow Optical \ufb02ow can be used to reconstruct the motion of the camera, and from that, assuming that the transformation from the robot camera frame to the car frame is known, it is straightforward to derive the vehicle velocity. we use the robot cam- eratomeasuretheoptical\ufb02ow, visual features, due to the relative motion between camera and scene. the six-dimensional velocity vector v c of the frame c can be related to the velocity of the point tracked in the image \u0307x through the following relation: p The literature in the \ufb01eld of autonomous car control provides numerous methods for estimating the car speed by means of optical \ufb02ow.40,41 To improve the velocity estimate, the optical \ufb02ow can be fused with inertial measurements, as done in the case of aerial robots, in Grabe et al.42 Inspired by that approach, we design a KF, fusing the acceleration measured by the robot IMU and the velocity measured with optical \ufb02ow."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we use the robot cam- eratomeasuretheoptical\ufb02ow, visual features, due to the relative motion between camera and scene.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f18998dd-5e73-4091-b5aa-48fdac623325": {"__data__": {"id_": "f18998dd-5e73-4091-b5aa-48fdac623325", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Optical \ufb02ow can be used to reconstruct the motion of the camera, and from that, assuming that the transformation from the robot camera frame to the car frame is known, it is straightforward to derive the vehicle velocity. we use the robot cam- eratomeasuretheoptical\ufb02ow, visual features, due to the relative motion between camera and scene. the six-dimensional velocity vector v c of the frame c can be related to the velocity of the point tracked in the image \u0307x through the following relation: p The literature in the \ufb01eld of autonomous car control provides numerous methods for estimating the car speed by means of optical \ufb02ow.40,41 To improve the velocity estimate, the optical \ufb02ow can be fused with inertial measurements, as done in the case of aerial robots, in Grabe et al.42 Inspired by that approach, we design a KF, fusing the acceleration measured by the robot IMU and the velocity measured with optical \ufb02ow. Considering the linear velocity and acceleration along the forward car axis yw as state \ud835\udf43 = (v a)T of the KF, we use a simple discrete-time stochastic model to describe the car motion:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the six-dimensional velocity vector v c of the frame c can be related to the velocity of the point tracked in the image \u0307x through the following relation: p", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fdd81865-5400-40d8-aaa2-80b69b937cab": {"__data__": {"id_": "fdd81865-5400-40d8-aaa2-80b69b937cab", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we use the robot cam- eratomeasuretheoptical\ufb02ow, visual features, due to the relative motion between camera and scene. the six-dimensional velocity vector v c of the frame c can be related to the velocity of the point tracked in the image \u0307x through the following relation: p The literature in the \ufb01eld of autonomous car control provides numerous methods for estimating the car speed by means of optical \ufb02ow.40,41 To improve the velocity estimate, the optical \ufb02ow can be fused with inertial measurements, as done in the case of aerial robots, in Grabe et al.42 Inspired by that approach, we design a KF, fusing the acceleration measured by the robot IMU and the velocity measured with optical \ufb02ow. Considering the linear velocity and acceleration along the forward car axis yw as state \ud835\udf43 = (v a)T of the KF, we use a simple discrete-time stochastic model to describe the car motion: (xp,yp) are the image coordinates (in pixels) of the point on the ground, expressed as (xg,yg,zg) in the camera frame (see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The literature in the \ufb01eld of autonomous car control provides numerous methods for estimating the car speed by means of optical \ufb02ow.40,41 To improve the velocity estimate, the optical \ufb02ow can be fused with inertial measurements, as done in the case of aerial robots, in Grabe et al.42 Inspired by that approach, we design a KF, fusing the acceleration measured by the robot IMU and the velocity measured with optical \ufb02ow.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90d2702b-2082-49ca-96da-07eb81dd8a2a": {"__data__": {"id_": "90d2702b-2082-49ca-96da-07eb81dd8a2a", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the six-dimensional velocity vector v c of the frame c can be related to the velocity of the point tracked in the image \u0307x through the following relation: p The literature in the \ufb01eld of autonomous car control provides numerous methods for estimating the car speed by means of optical \ufb02ow.40,41 To improve the velocity estimate, the optical \ufb02ow can be fused with inertial measurements, as done in the case of aerial robots, in Grabe et al.42 Inspired by that approach, we design a KF, fusing the acceleration measured by the robot IMU and the velocity measured with optical \ufb02ow. Considering the linear velocity and acceleration along the forward car axis yw as state \ud835\udf43 = (v a)T of the KF, we use a simple discrete-time stochastic model to describe the car motion: (xp,yp) are the image coordinates (in pixels) of the point on the ground, expressed as (xg,yg,zg) in the camera frame (see Fig. Fur- thermore, it is Sx,y = f\ud835\udefc x,y, where f is the camera focal length and \ud835\udefc x\u2215\ud835\udefc y the pixel aspect ratio."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Considering the linear velocity and acceleration along the forward car axis yw as state \ud835\udf43 = (v a)T of the KF, we use a simple discrete-time stochastic model to describe the car motion:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9a73a75-7026-4f8b-a381-a73fea148ff7": {"__data__": {"id_": "d9a73a75-7026-4f8b-a381-a73fea148ff7", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The literature in the \ufb01eld of autonomous car control provides numerous methods for estimating the car speed by means of optical \ufb02ow.40,41 To improve the velocity estimate, the optical \ufb02ow can be fused with inertial measurements, as done in the case of aerial robots, in Grabe et al.42 Inspired by that approach, we design a KF, fusing the acceleration measured by the robot IMU and the velocity measured with optical \ufb02ow. Considering the linear velocity and acceleration along the forward car axis yw as state \ud835\udf43 = (v a)T of the KF, we use a simple discrete-time stochastic model to describe the car motion: (xp,yp) are the image coordinates (in pixels) of the point on the ground, expressed as (xg,yg,zg) in the camera frame (see Fig. Fur- thermore, it is Sx,y = f\ud835\udefc x,y, where f is the camera focal length and \ud835\udefc x\u2215\ud835\udefc y the pixel aspect ratio. In the computation of L, we consider that the image principal point coincides with the image center."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(xp,yp) are the image coordinates (in pixels) of the point on the ground, expressed as (xg,yg,zg) in the camera frame (see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "629c9f22-9103-4376-8e72-2d009041f51b": {"__data__": {"id_": "629c9f22-9103-4376-8e72-2d009041f51b", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Considering the linear velocity and acceleration along the forward car axis yw as state \ud835\udf43 = (v a)T of the KF, we use a simple discrete-time stochastic model to describe the car motion: (xp,yp) are the image coordinates (in pixels) of the point on the ground, expressed as (xg,yg,zg) in the camera frame (see Fig. Fur- thermore, it is Sx,y = f\ud835\udefc x,y, where f is the camera focal length and \ud835\udefc x\u2215\ud835\udefc y the pixel aspect ratio. In the computation of L, we consider that the image principal point coincides with the image center. As shown in Figure 7(b), the point depth zg can be reconstructed through the image point ordinate yp and the camera con\ufb01guration (tilt angle \ud835\udefe and height zw c ):"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fur- thermore, it is Sx,y = f\ud835\udefc x,y, where f is the camera focal length and \ud835\udefc x\u2215\ud835\udefc y the pixel aspect ratio.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00622f2c-aaf3-4b21-a290-d44cdf507307": {"__data__": {"id_": "00622f2c-aaf3-4b21-a290-d44cdf507307", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "(xp,yp) are the image coordinates (in pixels) of the point on the ground, expressed as (xg,yg,zg) in the camera frame (see Fig. Fur- thermore, it is Sx,y = f\ud835\udefc x,y, where f is the camera focal length and \ud835\udefc x\u2215\ud835\udefc y the pixel aspect ratio. In the computation of L, we consider that the image principal point coincides with the image center. As shown in Figure 7(b), the point depth zg can be reconstructed through the image point ordinate yp and the camera con\ufb01guration (tilt angle \ud835\udefe and height zw c ): with \u0394T the sampling time and n k the zero-mean white Gaussian noise."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the computation of L, we consider that the image principal point coincides with the image center.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e783a99-9d00-4522-901e-daaa708c8536": {"__data__": {"id_": "1e783a99-9d00-4522-901e-daaa708c8536", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Fur- thermore, it is Sx,y = f\ud835\udefc x,y, where f is the camera focal length and \ud835\udefc x\u2215\ud835\udefc y the pixel aspect ratio. In the computation of L, we consider that the image principal point coincides with the image center. As shown in Figure 7(b), the point depth zg can be reconstructed through the image point ordinate yp and the camera con\ufb01guration (tilt angle \ud835\udefe and height zw c ): with \u0394T the sampling time and n k the zero-mean white Gaussian noise. The corresponding output of the KF is modeled as"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As shown in Figure 7(b), the point depth zg can be reconstructed through the image point ordinate yp and the camera con\ufb01guration (tilt angle \ud835\udefe and height zw c ):", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02c0a05b-8e21-4a3f-a17f-ffad3d02be9c": {"__data__": {"id_": "02c0a05b-8e21-4a3f-a17f-ffad3d02be9c", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In the computation of L, we consider that the image principal point coincides with the image center. As shown in Figure 7(b), the point depth zg can be reconstructed through the image point ordinate yp and the camera con\ufb01guration (tilt angle \ud835\udefe and height zw c ): with \u0394T the sampling time and n k the zero-mean white Gaussian noise. The corresponding output of the KF is modeled as \u2217 Although three parameters are suf\ufb01cient if the borders are parallel, a four-dimensional state vector will cover all cases, while guaranteeing robustness to image processing noise."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with \u0394T the sampling time and n k the zero-mean white Gaussian noise.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d988e54-6aa7-4631-83bd-26b8cdaa97e3": {"__data__": {"id_": "5d988e54-6aa7-4631-83bd-26b8cdaa97e3", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "As shown in Figure 7(b), the point depth zg can be reconstructed through the image point ordinate yp and the camera con\ufb01guration (tilt angle \ud835\udefe and height zw c ): with \u0394T the sampling time and n k the zero-mean white Gaussian noise. The corresponding output of the KF is modeled as \u2217 Although three parameters are suf\ufb01cient if the borders are parallel, a four-dimensional state vector will cover all cases, while guaranteeing robustness to image processing noise. p = ( \u0307x p,1 \u22ef \u0307x p,n)T, instead of L and \u0307x p. Then, v c is obtained by solv- ing a least-squares problem\u2217:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The corresponding output of the KF is modeled as", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb5c2262-b8eb-47a5-b383-fc754589d405": {"__data__": {"id_": "cb5c2262-b8eb-47a5-b383-fc754589d405", "embedding": null, "metadata": {"page_number": 7, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "with \u0394T the sampling time and n k the zero-mean white Gaussian noise. The corresponding output of the KF is modeled as \u2217 Although three parameters are suf\ufb01cient if the borders are parallel, a four-dimensional state vector will cover all cases, while guaranteeing robustness to image processing noise. p = ( \u0307x p,1 \u22ef \u0307x p,n)T, instead of L and \u0307x p. Then, v c is obtained by solv- ing a least-squares problem\u2217: p in (13) is based on the computation of the optical \ufb02ow."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2217 Although three parameters are suf\ufb01cient if the borders are parallel, a four-dimensional state vector will cover all cases, while guaranteeing robustness to image processing noise.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab2530f6-7064-411e-b125-49fd53572aa3": {"__data__": {"id_": "ab2530f6-7064-411e-b125-49fd53572aa3", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The corresponding output of the KF is modeled as \u2217 Although three parameters are suf\ufb01cient if the borders are parallel, a four-dimensional state vector will cover all cases, while guaranteeing robustness to image processing noise. p = ( \u0307x p,1 \u22ef \u0307x p,n)T, instead of L and \u0307x p. Then, v c is obtained by solv- ing a least-squares problem\u2217: p in (13) is based on the computation of the optical \ufb02ow. However, during the navigation of the car, the vibration of the engine, poor textured views, and other unmodeled effects add noise to the measurement process.40 Furthermore, other factors, such ,shadows,andrepetitivetextures,canjeop- ardizefeaturetracking.Therefore,rawoptical\ufb02ow,asprovidedbyoff- the-shelf algorithms, for example, from the OpenCV library,39 gives noisy data that are insuf\ufb01cient for accurate velocity estimation; so \ufb01l- tering and outlier rejection techniques must be added."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "p = ( \u0307x p,1 \u22ef \u0307x p,n)T, instead of L and \u0307x p. Then, v c is obtained by solv- ing a least-squares problem\u2217:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b395b144-5be5-4998-99bb-bc08615eea67": {"__data__": {"id_": "b395b144-5be5-4998-99bb-bc08615eea67", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2217 Although three parameters are suf\ufb01cient if the borders are parallel, a four-dimensional state vector will cover all cases, while guaranteeing robustness to image processing noise. p = ( \u0307x p,1 \u22ef \u0307x p,n)T, instead of L and \u0307x p. Then, v c is obtained by solv- ing a least-squares problem\u2217: p in (13) is based on the computation of the optical \ufb02ow. However, during the navigation of the car, the vibration of the engine, poor textured views, and other unmodeled effects add noise to the measurement process.40 Furthermore, other factors, such ,shadows,andrepetitivetextures,canjeop- ardizefeaturetracking.Therefore,rawoptical\ufb02ow,asprovidedbyoff- the-shelf algorithms, for example, from the OpenCV library,39 gives noisy data that are insuf\ufb01cient for accurate velocity estimation; so \ufb01l- tering and outlier rejection techniques must be added. weuseadenseoptical \ufb02ow algorithm that differs from sparse algorithms, in that it computes the apparent motion of all the pixels of the image plane."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "p in (13) is based on the computation of the optical \ufb02ow.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ad570a6-5ce1-4351-aca5-e05dd72399e6": {"__data__": {"id_": "5ad570a6-5ce1-4351-aca5-e05dd72399e6", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "p = ( \u0307x p,1 \u22ef \u0307x p,n)T, instead of L and \u0307x p. Then, v c is obtained by solv- ing a least-squares problem\u2217: p in (13) is based on the computation of the optical \ufb02ow. However, during the navigation of the car, the vibration of the engine, poor textured views, and other unmodeled effects add noise to the measurement process.40 Furthermore, other factors, such ,shadows,andrepetitivetextures,canjeop- ardizefeaturetracking.Therefore,rawoptical\ufb02ow,asprovidedbyoff- the-shelf algorithms, for example, from the OpenCV library,39 gives noisy data that are insuf\ufb01cient for accurate velocity estimation; so \ufb01l- tering and outlier rejection techniques must be added. weuseadenseoptical \ufb02ow algorithm that differs from sparse algorithms, in that it computes the apparent motion of all the pixels of the image plane. Then, we \ufb01lter thedenseoptical\ufb02ow,\ufb01rstaccordingtogeometricrationales,andthen with an outlier rejection method.41 The whole procedure is described below, step by step:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, during the navigation of the car, the vibration of the engine, poor textured views, and other unmodeled effects add noise to the measurement process.40 Furthermore, other factors, such ,shadows,andrepetitivetextures,canjeop- ardizefeaturetracking.Therefore,rawoptical\ufb02ow,asprovidedbyoff- the-shelf algorithms, for example, from the OpenCV library,39 gives noisy data that are insuf\ufb01cient for accurate velocity estimation; so \ufb01l- tering and outlier rejection techniques must be added.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95e9ad27-16ad-492c-9903-31a080e397b1": {"__data__": {"id_": "95e9ad27-16ad-492c-9903-31a080e397b1", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "p in (13) is based on the computation of the optical \ufb02ow. However, during the navigation of the car, the vibration of the engine, poor textured views, and other unmodeled effects add noise to the measurement process.40 Furthermore, other factors, such ,shadows,andrepetitivetextures,canjeop- ardizefeaturetracking.Therefore,rawoptical\ufb02ow,asprovidedbyoff- the-shelf algorithms, for example, from the OpenCV library,39 gives noisy data that are insuf\ufb01cient for accurate velocity estimation; so \ufb01l- tering and outlier rejection techniques must be added. weuseadenseoptical \ufb02ow algorithm that differs from sparse algorithms, in that it computes the apparent motion of all the pixels of the image plane. Then, we \ufb01lter thedenseoptical\ufb02ow,\ufb01rstaccordingtogeometricrationales,andthen with an outlier rejection method.41 The whole procedure is described below, step by step: \u2022 Take two consecutive images from the robot on-board camera."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "weuseadenseoptical \ufb02ow algorithm that differs from sparse algorithms, in that it computes the apparent motion of all the pixels of the image plane.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ded8ebe-fd88-4b6d-a761-60762ef7f877": {"__data__": {"id_": "6ded8ebe-fd88-4b6d-a761-60762ef7f877", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "However, during the navigation of the car, the vibration of the engine, poor textured views, and other unmodeled effects add noise to the measurement process.40 Furthermore, other factors, such ,shadows,andrepetitivetextures,canjeop- ardizefeaturetracking.Therefore,rawoptical\ufb02ow,asprovidedbyoff- the-shelf algorithms, for example, from the OpenCV library,39 gives noisy data that are insuf\ufb01cient for accurate velocity estimation; so \ufb01l- tering and outlier rejection techniques must be added. weuseadenseoptical \ufb02ow algorithm that differs from sparse algorithms, in that it computes the apparent motion of all the pixels of the image plane. Then, we \ufb01lter thedenseoptical\ufb02ow,\ufb01rstaccordingtogeometricrationales,andthen with an outlier rejection method.41 The whole procedure is described below, step by step: \u2022 Take two consecutive images from the robot on-board camera. \u2022 Consider only the pixels in a ROI that includes the area of the image plane corresponding to the road."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, we \ufb01lter thedenseoptical\ufb02ow,\ufb01rstaccordingtogeometricrationales,andthen with an outlier rejection method.41 The whole procedure is described below, step by step:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f99361b-12bd-4b30-9dd2-268aaacde7a5": {"__data__": {"id_": "2f99361b-12bd-4b30-9dd2-268aaacde7a5", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "weuseadenseoptical \ufb02ow algorithm that differs from sparse algorithms, in that it computes the apparent motion of all the pixels of the image plane. Then, we \ufb01lter thedenseoptical\ufb02ow,\ufb01rstaccordingtogeometricrationales,andthen with an outlier rejection method.41 The whole procedure is described below, step by step: \u2022 Take two consecutive images from the robot on-board camera. \u2022 Consider only the pixels in a ROI that includes the area of the image plane corresponding to the road. This ROI is kept constant along all the experiment and, thus, identical for the two consecutive frames."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Take two consecutive images from the robot on-board camera.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc71e666-8c96-4fc5-b80b-87c8457d4d14": {"__data__": {"id_": "bc71e666-8c96-4fc5-b80b-87c8457d4d14", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Then, we \ufb01lter thedenseoptical\ufb02ow,\ufb01rstaccordingtogeometricrationales,andthen with an outlier rejection method.41 The whole procedure is described below, step by step: \u2022 Take two consecutive images from the robot on-board camera. \u2022 Consider only the pixels in a ROI that includes the area of the image plane corresponding to the road. This ROI is kept constant along all the experiment and, thus, identical for the two consecutive frames. \u2022 Covert the frames to gray scale, apply a Gaussian \ufb01lter, and equalize with respect to the histogram."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Consider only the pixels in a ROI that includes the area of the image plane corresponding to the road.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a35f4cf-1a26-47bd-be85-14f77c3827c1": {"__data__": {"id_": "0a35f4cf-1a26-47bd-be85-14f77c3827c1", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Take two consecutive images from the robot on-board camera. \u2022 Consider only the pixels in a ROI that includes the area of the image plane corresponding to the road. This ROI is kept constant along all the experiment and, thus, identical for the two consecutive frames. \u2022 Covert the frames to gray scale, apply a Gaussian \ufb01lter, and equalize with respect to the histogram. \u2022 Compute the dense optical \ufb02ow, using the Farneb\u00e4ck algorithm44 implemented in OpenCV."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This ROI is kept constant along all the experiment and, thus, identical for the two consecutive frames.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49d27d87-60de-4ce0-ac91-1726d6d1e8d5": {"__data__": {"id_": "49d27d87-60de-4ce0-ac91-1726d6d1e8d5", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Consider only the pixels in a ROI that includes the area of the image plane corresponding to the road. This ROI is kept constant along all the experiment and, thus, identical for the two consecutive frames. \u2022 Covert the frames to gray scale, apply a Gaussian \ufb01lter, and equalize with respect to the histogram. \u2022 Compute the dense optical \ufb02ow, using the Farneb\u00e4ck algorithm44 implemented in OpenCV. \u2022 Since the car is supposed to move forward, in the dense optical \ufb02ow vector, consider only those elements pointing downwards on the image plane, and discard those not having a signi\ufb01cant cen- trifugal motion from the principal point."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Covert the frames to gray scale, apply a Gaussian \ufb01lter, and equalize with respect to the histogram.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59a04999-801b-45e2-a472-b6c764916e4b": {"__data__": {"id_": "59a04999-801b-45e2-a472-b6c764916e4b", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This ROI is kept constant along all the experiment and, thus, identical for the two consecutive frames. \u2022 Covert the frames to gray scale, apply a Gaussian \ufb01lter, and equalize with respect to the histogram. \u2022 Compute the dense optical \ufb02ow, using the Farneb\u00e4ck algorithm44 implemented in OpenCV. \u2022 Since the car is supposed to move forward, in the dense optical \ufb02ow vector, consider only those elements pointing downwards on the image plane, and discard those not having a signi\ufb01cant cen- trifugal motion from the principal point. Furthermore, consider only contributions with length between an upper and a lower thresh- old and whose origin is on an image edge (detected applying Canny operator)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Compute the dense optical \ufb02ow, using the Farneb\u00e4ck algorithm44 implemented in OpenCV.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c436dc4-31af-4141-82d1-b8478e195a70": {"__data__": {"id_": "5c436dc4-31af-4141-82d1-b8478e195a70", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Covert the frames to gray scale, apply a Gaussian \ufb01lter, and equalize with respect to the histogram. \u2022 Compute the dense optical \ufb02ow, using the Farneb\u00e4ck algorithm44 implemented in OpenCV. \u2022 Since the car is supposed to move forward, in the dense optical \ufb02ow vector, consider only those elements pointing downwards on the image plane, and discard those not having a signi\ufb01cant cen- trifugal motion from the principal point. Furthermore, consider only contributions with length between an upper and a lower thresh- old and whose origin is on an image edge (detected applying Canny operator). \u2022 Reject the outliers, that is, the contributions ( \u0307xp,i, \u0307yp,i), i \u2208 {1,\u2026,n}, suchthat \u0307xp,i \u2209 [\u0304\u0307xp \u00b1 \ud835\udf0e x]and \u0307yp,i \u2209 [\u0304\u0307yp \u00b1 \ud835\udf0e y],where \u0304\u0307xp (\u0304\u0307yp )and\ud835\udf0e x (\ud835\udf0e y) are the average and standard deviation of the optical \ufb02ow horizon- tal (vertical) contributions."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Since the car is supposed to move forward, in the dense optical \ufb02ow vector, consider only those elements pointing downwards on the image plane, and discard those not having a signi\ufb01cant cen- trifugal motion from the principal point.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b179f188-71de-4267-aab2-3cb7a8b5907c": {"__data__": {"id_": "b179f188-71de-4267-aab2-3cb7a8b5907c", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Compute the dense optical \ufb02ow, using the Farneb\u00e4ck algorithm44 implemented in OpenCV. \u2022 Since the car is supposed to move forward, in the dense optical \ufb02ow vector, consider only those elements pointing downwards on the image plane, and discard those not having a signi\ufb01cant cen- trifugal motion from the principal point. Furthermore, consider only contributions with length between an upper and a lower thresh- old and whose origin is on an image edge (detected applying Canny operator). \u2022 Reject the outliers, that is, the contributions ( \u0307xp,i, \u0307yp,i), i \u2208 {1,\u2026,n}, suchthat \u0307xp,i \u2209 [\u0304\u0307xp \u00b1 \ud835\udf0e x]and \u0307yp,i \u2209 [\u0304\u0307yp \u00b1 \ud835\udf0e y],where \u0304\u0307xp (\u0304\u0307yp )and\ud835\udf0e x (\ud835\udf0e y) are the average and standard deviation of the optical \ufb02ow horizon- tal (vertical) contributions. This operation is made separately for the contributions of the right and left side of the image, where the mod- ule and the direction of the optical \ufb02ow vectors can be quite differ- ent (e.g., on turns)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, consider only contributions with length between an upper and a lower thresh- old and whose origin is on an image edge (detected applying Canny operator).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b2eeb60-8af2-4b57-883c-9a05ab81ec96": {"__data__": {"id_": "8b2eeb60-8af2-4b57-883c-9a05ab81ec96", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Since the car is supposed to move forward, in the dense optical \ufb02ow vector, consider only those elements pointing downwards on the image plane, and discard those not having a signi\ufb01cant cen- trifugal motion from the principal point. Furthermore, consider only contributions with length between an upper and a lower thresh- old and whose origin is on an image edge (detected applying Canny operator). \u2022 Reject the outliers, that is, the contributions ( \u0307xp,i, \u0307yp,i), i \u2208 {1,\u2026,n}, suchthat \u0307xp,i \u2209 [\u0304\u0307xp \u00b1 \ud835\udf0e x]and \u0307yp,i \u2209 [\u0304\u0307yp \u00b1 \ud835\udf0e y],where \u0304\u0307xp (\u0304\u0307yp )and\ud835\udf0e x (\ud835\udf0e y) are the average and standard deviation of the optical \ufb02ow horizon- tal (vertical) contributions. This operation is made separately for the contributions of the right and left side of the image, where the mod- ule and the direction of the optical \ufb02ow vectors can be quite differ- ent (e.g., on turns). p, is fed to (13), to obtain v c, that is then low-pass \ufb01ltered."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Reject the outliers, that is, the contributions ( \u0307xp,i, \u0307yp,i), i \u2208 {1,\u2026,n}, suchthat \u0307xp,i \u2209 [\u0304\u0307xp \u00b1 \ud835\udf0e x]and \u0307yp,i \u2209 [\u0304\u0307yp \u00b1 \ud835\udf0e y],where \u0304\u0307xp (\u0304\u0307yp )and\ud835\udf0e x (\ud835\udf0e y) are the average and standard deviation of the optical \ufb02ow horizon- tal (vertical) contributions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eaa94b16-3c77-4bb5-9484-2b642782c6e7": {"__data__": {"id_": "eaa94b16-3c77-4bb5-9484-2b642782c6e7", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Furthermore, consider only contributions with length between an upper and a lower thresh- old and whose origin is on an image edge (detected applying Canny operator). \u2022 Reject the outliers, that is, the contributions ( \u0307xp,i, \u0307yp,i), i \u2208 {1,\u2026,n}, suchthat \u0307xp,i \u2209 [\u0304\u0307xp \u00b1 \ud835\udf0e x]and \u0307yp,i \u2209 [\u0304\u0307yp \u00b1 \ud835\udf0e y],where \u0304\u0307xp (\u0304\u0307yp )and\ud835\udf0e x (\ud835\udf0e y) are the average and standard deviation of the optical \ufb02ow horizon- tal (vertical) contributions. This operation is made separately for the contributions of the right and left side of the image, where the mod- ule and the direction of the optical \ufb02ow vectors can be quite differ- ent (e.g., on turns). p, is fed to (13), to obtain v c, that is then low-pass \ufb01ltered. To transform the velocity v c in frame c, obtained from (13), into velocity v w in the car frame w, we apply:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This operation is made separately for the contributions of the right and left side of the image, where the mod- ule and the direction of the optical \ufb02ow vectors can be quite differ- ent (e.g., on turns).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1be9bdd2-687b-4aaa-b26c-32d043deaa80": {"__data__": {"id_": "1be9bdd2-687b-4aaa-b26c-32d043deaa80", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2022 Reject the outliers, that is, the contributions ( \u0307xp,i, \u0307yp,i), i \u2208 {1,\u2026,n}, suchthat \u0307xp,i \u2209 [\u0304\u0307xp \u00b1 \ud835\udf0e x]and \u0307yp,i \u2209 [\u0304\u0307yp \u00b1 \ud835\udf0e y],where \u0304\u0307xp (\u0304\u0307yp )and\ud835\udf0e x (\ud835\udf0e y) are the average and standard deviation of the optical \ufb02ow horizon- tal (vertical) contributions. This operation is made separately for the contributions of the right and left side of the image, where the mod- ule and the direction of the optical \ufb02ow vectors can be quite differ- ent (e.g., on turns). p, is fed to (13), to obtain v c, that is then low-pass \ufb01ltered. To transform the velocity v c in frame c, obtained from (13), into velocity v w in the car frame w, we apply: \u2217 To solve the least-square problem, n \u2265 3 points are necessary."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "p, is fed to (13), to obtain v c, that is then low-pass \ufb01ltered.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d94bcb8a-b9ad-4b78-aaf9-8ddea38a5609": {"__data__": {"id_": "d94bcb8a-b9ad-4b78-aaf9-8ddea38a5609", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This operation is made separately for the contributions of the right and left side of the image, where the mod- ule and the direction of the optical \ufb02ow vectors can be quite differ- ent (e.g., on turns). p, is fed to (13), to obtain v c, that is then low-pass \ufb01ltered. To transform the velocity v c in frame c, obtained from (13), into velocity v w in the car frame w, we apply: \u2217 To solve the least-square problem, n \u2265 3 points are necessary. In our implementation, we used the openCV solve function, and to\ufb01lter the noise due to few contributions, we set n \u2265 25."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To transform the velocity v c in frame c, obtained from (13), into velocity v w in the car frame w, we apply:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d47b84d3-53a0-4cdc-ae9d-d523a71e5458": {"__data__": {"id_": "d47b84d3-53a0-4cdc-ae9d-d523a71e5458", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "p, is fed to (13), to obtain v c, that is then low-pass \ufb01ltered. To transform the velocity v c in frame c, obtained from (13), into velocity v w in the car frame w, we apply: \u2217 To solve the least-square problem, n \u2265 3 points are necessary. In our implementation, we used the openCV solve function, and to\ufb01lter the noise due to few contributions, we set n \u2265 25. Rw c is the rotation matrix from car to camera frame and Sw c is the skew symmetric matrix associated with the position pw c of the origin of c in w."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2217 To solve the least-square problem, n \u2265 3 points are necessary.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1354447-033b-48b0-ae07-451df4fbcc9f": {"__data__": {"id_": "e1354447-033b-48b0-ae07-451df4fbcc9f", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "To transform the velocity v c in frame c, obtained from (13), into velocity v w in the car frame w, we apply: \u2217 To solve the least-square problem, n \u2265 3 points are necessary. In our implementation, we used the openCV solve function, and to\ufb01lter the noise due to few contributions, we set n \u2265 25. Rw c is the rotation matrix from car to camera frame and Sw c is the skew symmetric matrix associated with the position pw c of the origin of c in w. the speed of the car is set as the y-component of v w: vOF = vw,y."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our implementation, we used the openCV solve function, and to\ufb01lter the noise due to few contributions, we set n \u2265 25.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05b9c90f-5522-41b6-bd43-26aab5ab4552": {"__data__": {"id_": "05b9c90f-5522-41b6-bd43-26aab5ab4552", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u2217 To solve the least-square problem, n \u2265 3 points are necessary. In our implementation, we used the openCV solve function, and to\ufb01lter the noise due to few contributions, we set n \u2265 25. Rw c is the rotation matrix from car to camera frame and Sw c is the skew symmetric matrix associated with the position pw c of the origin of c in w. the speed of the car is set as the y-component of v w: vOF = vw,y. This will constitute the \ufb01rst component of the KF measurement vector."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rw c is the rotation matrix from car to camera frame and Sw c is the skew symmetric matrix associated with the position pw c of the origin of c in w.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d48815b8-f544-49f4-8390-6d3f399100b8": {"__data__": {"id_": "d48815b8-f544-49f4-8390-6d3f399100b8", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In our implementation, we used the openCV solve function, and to\ufb01lter the noise due to few contributions, we set n \u2265 25. Rw c is the rotation matrix from car to camera frame and Sw c is the skew symmetric matrix associated with the position pw c of the origin of c in w. the speed of the car is set as the y-component of v w: vOF = vw,y. This will constitute the \ufb01rst component of the KF measurement vector. 4.2.2 Measure of the car acceleration with robot accelerometers"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the speed of the car is set as the y-component of v w: vOF = vw,y.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d993373-e1f1-4b35-816c-e921782aba31": {"__data__": {"id_": "6d993373-e1f1-4b35-816c-e921782aba31", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Rw c is the rotation matrix from car to camera frame and Sw c is the skew symmetric matrix associated with the position pw c of the origin of c in w. the speed of the car is set as the y-component of v w: vOF = vw,y. This will constitute the \ufb01rst component of the KF measurement vector. 4.2.2 Measure of the car acceleration with robot accelerometers we \ufb01rst compensate the gravitycomponent, experiment.\u2020 This gives a b, the three-dimensional (3D) robot accelera- tion, expressed in the robot frame b."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This will constitute the \ufb01rst component of the KF measurement vector.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d118efe-4cee-4fa8-a2ad-44d925eebca0": {"__data__": {"id_": "1d118efe-4cee-4fa8-a2ad-44d925eebca0", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the speed of the car is set as the y-component of v w: vOF = vw,y. This will constitute the \ufb01rst component of the KF measurement vector. 4.2.2 Measure of the car acceleration with robot accelerometers we \ufb01rst compensate the gravitycomponent, experiment.\u2020 This gives a b, the three-dimensional (3D) robot accelera- tion, expressed in the robot frame b. Then, we transform a b in the car frame w, to obtain"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.2.2 Measure of the car acceleration with robot accelerometers", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7a4ea5e-4b70-47b0-91f8-8ebef28b2150": {"__data__": {"id_": "a7a4ea5e-4b70-47b0-91f8-8ebef28b2150", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This will constitute the \ufb01rst component of the KF measurement vector. 4.2.2 Measure of the car acceleration with robot accelerometers we \ufb01rst compensate the gravitycomponent, experiment.\u2020 This gives a b, the three-dimensional (3D) robot accelera- tion, expressed in the robot frame b. Then, we transform a b in the car frame w, to obtain aIMU is obtained by selecting the y-component of a w. This will constitute the second component of the KF measure- ment vector."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we \ufb01rst compensate the gravitycomponent, experiment.\u2020 This gives a b, the three-dimensional (3D) robot accelera- tion, expressed in the robot frame b.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3f7ac13-8c91-487f-9a50-d6eb01e48090": {"__data__": {"id_": "b3f7ac13-8c91-487f-9a50-d6eb01e48090", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "4.2.2 Measure of the car acceleration with robot accelerometers we \ufb01rst compensate the gravitycomponent, experiment.\u2020 This gives a b, the three-dimensional (3D) robot accelera- tion, expressed in the robot frame b. Then, we transform a b in the car frame w, to obtain aIMU is obtained by selecting the y-component of a w. This will constitute the second component of the KF measure- ment vector. The objective of car control is (i) to drive the rear wheel axis center W along the curvilinear path that is equally distant from the left and right road borders [see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, we transform a b in the car frame w, to obtain", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e83fef94-f233-4970-b3fe-477d05e0dbe7": {"__data__": {"id_": "e83fef94-f233-4970-b3fe-477d05e0dbe7", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we \ufb01rst compensate the gravitycomponent, experiment.\u2020 This gives a b, the three-dimensional (3D) robot accelera- tion, expressed in the robot frame b. Then, we transform a b in the car frame w, to obtain aIMU is obtained by selecting the y-component of a w. This will constitute the second component of the KF measure- ment vector. The objective of car control is (i) to drive the rear wheel axis center W along the curvilinear path that is equally distant from the left and right road borders [see Fig. 2(b)], while aligning the car with the tan- gent to this path, and (ii) to track desired vehicle velocity v\u2217."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "aIMU is obtained by selecting the y-component of a w. This will constitute the second component of the KF measure- ment vector.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d15c3e00-b33b-4140-a7df-b9f3fb53ce65": {"__data__": {"id_": "d15c3e00-b33b-4140-a7df-b9f3fb53ce65", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Then, we transform a b in the car frame w, to obtain aIMU is obtained by selecting the y-component of a w. This will constitute the second component of the KF measure- ment vector. The objective of car control is (i) to drive the rear wheel axis center W along the curvilinear path that is equally distant from the left and right road borders [see Fig. 2(b)], while aligning the car with the tan- gent to this path, and (ii) to track desired vehicle velocity v\u2217. Basically, carcontrol consists of achieving tasks (6)and (7),with the steering and car velocity controllers described in the following subsections."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The objective of car control is (i) to drive the rear wheel axis center W along the curvilinear path that is equally distant from the left and right road borders [see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "632c1702-d43f-4e4c-972e-6c313e1c9835": {"__data__": {"id_": "632c1702-d43f-4e4c-972e-6c313e1c9835", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "aIMU is obtained by selecting the y-component of a w. This will constitute the second component of the KF measure- ment vector. The objective of car control is (i) to drive the rear wheel axis center W along the curvilinear path that is equally distant from the left and right road borders [see Fig. 2(b)], while aligning the car with the tan- gent to this path, and (ii) to track desired vehicle velocity v\u2217. Basically, carcontrol consists of achieving tasks (6)and (7),with the steering and car velocity controllers described in the following subsections. the vision-based steering controller generates the car angularvelocityinput\ud835\udf14toregulatebothxand\ud835\udf03 tozero.Thisreference ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2(b)], while aligning the car with the tan- gent to this path, and (ii) to track desired vehicle velocity v\u2217.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4c1f171-2eb7-4406-afec-edc3c91c811d": {"__data__": {"id_": "c4c1f171-2eb7-4406-afec-edc3c91c811d", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The objective of car control is (i) to drive the rear wheel axis center W along the curvilinear path that is equally distant from the left and right road borders [see Fig. 2(b)], while aligning the car with the tan- gent to this path, and (ii) to track desired vehicle velocity v\u2217. Basically, carcontrol consists of achieving tasks (6)and (7),with the steering and car velocity controllers described in the following subsections. the vision-based steering controller generates the car angularvelocityinput\ud835\udf14toregulatebothxand\ud835\udf03 tozero.Thisreference . The controller is based on the algorithm introduced by Toibero et al.45 for unicycle corridor following and recently extended to the navigation of humanoids in environments with corridors connected throughcurvesandT-junctions.28 , the same algorithm can be applied here."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Basically, carcontrol consists of achieving tasks (6)and (7),with the steering and car velocity controllers described in the following subsections.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54d5c17d-1d46-4add-b595-915eba71feb0": {"__data__": {"id_": "54d5c17d-1d46-4add-b595-915eba71feb0", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "2(b)], while aligning the car with the tan- gent to this path, and (ii) to track desired vehicle velocity v\u2217. Basically, carcontrol consists of achieving tasks (6)and (7),with the steering and car velocity controllers described in the following subsections. the vision-based steering controller generates the car angularvelocityinput\ud835\udf14toregulatebothxand\ud835\udf03 tozero.Thisreference . The controller is based on the algorithm introduced by Toibero et al.45 for unicycle corridor following and recently extended to the navigation of humanoids in environments with corridors connected throughcurvesandT-junctions.28 , the same algorithm can be applied here. For the sake of completeness, in the following, we brie\ufb02y recall the derivation of the features model (that can be found, e.g., also in Vassallo et al.46) and the control law"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the vision-based steering controller generates the car angularvelocityinput\ud835\udf14toregulatebothxand\ud835\udf03 tozero.Thisreference .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3a708fc-4da6-4e97-ab1e-8a8358a60cf8": {"__data__": {"id_": "a3a708fc-4da6-4e97-ab1e-8a8358a60cf8", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Basically, carcontrol consists of achieving tasks (6)and (7),with the steering and car velocity controllers described in the following subsections. the vision-based steering controller generates the car angularvelocityinput\ud835\udf14toregulatebothxand\ud835\udf03 tozero.Thisreference . The controller is based on the algorithm introduced by Toibero et al.45 for unicycle corridor following and recently extended to the navigation of humanoids in environments with corridors connected throughcurvesandT-junctions.28 , the same algorithm can be applied here. For the sake of completeness, in the following, we brie\ufb02y recall the derivation of the features model (that can be found, e.g., also in Vassallo et al.46) and the control law originally presented by Toibero et al.45 In doing so, we illustrate the adaptations needed to deal with the speci\ufb01city of our problem."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The controller is based on the algorithm introduced by Toibero et al.45 for unicycle corridor following and recently extended to the navigation of humanoids in environments with corridors connected throughcurvesandT-junctions.28 , the same algorithm can be applied here.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac0feaec-8d7a-492b-bcb6-300c42e81199": {"__data__": {"id_": "ac0feaec-8d7a-492b-bcb6-300c42e81199", "embedding": null, "metadata": {"page_number": 8, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the vision-based steering controller generates the car angularvelocityinput\ud835\udf14toregulatebothxand\ud835\udf03 tozero.Thisreference . The controller is based on the algorithm introduced by Toibero et al.45 for unicycle corridor following and recently extended to the navigation of humanoids in environments with corridors connected throughcurvesandT-junctions.28 , the same algorithm can be applied here. For the sake of completeness, in the following, we brie\ufb02y recall the derivation of the features model (that can be found, e.g., also in Vassallo et al.46) and the control law originally presented by Toibero et al.45 In doing so, we illustrate the adaptations needed to deal with the speci\ufb01city of our problem. 47 Tc w is the transformation from the car frame w to c, and Tw p is from the path frame p to w."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the sake of completeness, in the following, we brie\ufb02y recall the derivation of the features model (that can be found, e.g., also in Vassallo et al.46) and the control law", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5fedd20-9c78-4792-a5e2-3d7936a673ac": {"__data__": {"id_": "c5fedd20-9c78-4792-a5e2-3d7936a673ac", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The controller is based on the algorithm introduced by Toibero et al.45 for unicycle corridor following and recently extended to the navigation of humanoids in environments with corridors connected throughcurvesandT-junctions.28 , the same algorithm can be applied here. For the sake of completeness, in the following, we brie\ufb02y recall the derivation of the features model (that can be found, e.g., also in Vassallo et al.46) and the control law originally presented by Toibero et al.45 In doing so, we illustrate the adaptations needed to deal with the speci\ufb01city of our problem. 47 Tc w is the transformation from the car frame w to c, and Tw p is from the path frame p to w. As intuitive from Figure 2, the projection matrix depends on both the car coordinates and the camera intrinsic and extrinsic parame- ters."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "originally presented by Toibero et al.45 In doing so, we illustrate the adaptations needed to deal with the speci\ufb01city of our problem.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca0b644c-5d0c-4da0-9967-59ab175eb51b": {"__data__": {"id_": "ca0b644c-5d0c-4da0-9967-59ab175eb51b", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "For the sake of completeness, in the following, we brie\ufb02y recall the derivation of the features model (that can be found, e.g., also in Vassallo et al.46) and the control law originally presented by Toibero et al.45 In doing so, we illustrate the adaptations needed to deal with the speci\ufb01city of our problem. 47 Tc w is the transformation from the car frame w to c, and Tw p is from the path frame p to w. As intuitive from Figure 2, the projection matrix depends on both the car coordinates and the camera intrinsic and extrinsic parame- ters. Here, we assume that the camera principal point coincides with the image center, and we neglect image distortion."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "47 Tc w is the transformation from the car frame w to c, and Tw p is from the path frame p to w.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7936823e-03e5-4173-9553-0efe86bab6f1": {"__data__": {"id_": "7936823e-03e5-4173-9553-0efe86bab6f1", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "originally presented by Toibero et al.45 In doing so, we illustrate the adaptations needed to deal with the speci\ufb01city of our problem. 47 Tc w is the transformation from the car frame w to c, and Tw p is from the path frame p to w. As intuitive from Figure 2, the projection matrix depends on both the car coordinates and the camera intrinsic and extrinsic parame- ters. Here, we assume that the camera principal point coincides with the image center, and we neglect image distortion. Furthermore, P has -coordinatesofthefeatures,sincethey do not affect the control task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As intuitive from Figure 2, the projection matrix depends on both the car coordinates and the camera intrinsic and extrinsic parame- ters.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6014149-c447-4863-9db1-b0b4746510c0": {"__data__": {"id_": "a6014149-c447-4863-9db1-b0b4746510c0", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "47 Tc w is the transformation from the car frame w to c, and Tw p is from the path frame p to w. As intuitive from Figure 2, the projection matrix depends on both the car coordinates and the camera intrinsic and extrinsic parame- ters. Here, we assume that the camera principal point coincides with the image center, and we neglect image distortion. Furthermore, P has -coordinatesofthefeatures,sincethey do not affect the control task. Under these assumptions, using P, the abscissas of the vanishing and middle point, respectively, denoted by xv and xm, can be expressed as45,46:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, we assume that the camera principal point coincides with the image center, and we neglect image distortion.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "382dd6d4-31e9-4692-80fa-e168455bd91e": {"__data__": {"id_": "382dd6d4-31e9-4692-80fa-e168455bd91e", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "As intuitive from Figure 2, the projection matrix depends on both the car coordinates and the camera intrinsic and extrinsic parame- ters. Here, we assume that the camera principal point coincides with the image center, and we neglect image distortion. Furthermore, P has -coordinatesofthefeatures,sincethey do not affect the control task. Under these assumptions, using P, the abscissas of the vanishing and middle point, respectively, denoted by xv and xm, can be expressed as45,46: We denote cos(\u2217) and sin(\u2217) with c\u2217 and s\u2217, respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, P has -coordinatesofthefeatures,sincethey do not affect the control task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec2d215e-8b40-4387-838e-f85ea1c2c34c": {"__data__": {"id_": "ec2d215e-8b40-4387-838e-f85ea1c2c34c", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Here, we assume that the camera principal point coincides with the image center, and we neglect image distortion. Furthermore, P has -coordinatesofthefeatures,sincethey do not affect the control task. Under these assumptions, using P, the abscissas of the vanishing and middle point, respectively, denoted by xv and xm, can be expressed as45,46: We denote cos(\u2217) and sin(\u2217) with c\u2217 and s\u2217, respectively. Note that with respect to the visual features model in Toibero et al.45 and Vas- sallo et al.,46 the expression of the middle point changes, due to the introduction of the lateral and longitudinal displacement, xw c and yw c , respectively, of the camera frame with respect to the car frame."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Under these assumptions, using P, the abscissas of the vanishing and middle point, respectively, denoted by xv and xm, can be expressed as45,46:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b4e7f18-8a1e-4dd0-907d-a1bbb7b8aa91": {"__data__": {"id_": "6b4e7f18-8a1e-4dd0-907d-a1bbb7b8aa91", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Furthermore, P has -coordinatesofthefeatures,sincethey do not affect the control task. Under these assumptions, using P, the abscissas of the vanishing and middle point, respectively, denoted by xv and xm, can be expressed as45,46: We denote cos(\u2217) and sin(\u2217) with c\u2217 and s\u2217, respectively. Note that with respect to the visual features model in Toibero et al.45 and Vas- sallo et al.,46 the expression of the middle point changes, due to the introduction of the lateral and longitudinal displacement, xw c and yw c , respectively, of the camera frame with respect to the car frame. As a consequence, to regulate the car position to the road center, we must de\ufb01ne a new visual feature \u0304xm = xm \u2212 k4."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We denote cos(\u2217) and sin(\u2217) with c\u2217 and s\u2217, respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ebbe811-be80-4785-ae3c-f50a1d82b761": {"__data__": {"id_": "6ebbe811-be80-4785-ae3c-f50a1d82b761", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Under these assumptions, using P, the abscissas of the vanishing and middle point, respectively, denoted by xv and xm, can be expressed as45,46: We denote cos(\u2217) and sin(\u2217) with c\u2217 and s\u2217, respectively. Note that with respect to the visual features model in Toibero et al.45 and Vas- sallo et al.,46 the expression of the middle point changes, due to the introduction of the lateral and longitudinal displacement, xw c and yw c , respectively, of the camera frame with respect to the car frame. As a consequence, to regulate the car position to the road center, we must de\ufb01ne a new visual feature \u0304xm = xm \u2212 k4. Then, the navigation task (6) is equivalent to the following visual task:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that with respect to the visual features model in Toibero et al.45 and Vas- sallo et al.,46 the expression of the middle point changes, due to the introduction of the lateral and longitudinal displacement, xw c and yw c , respectively, of the camera frame with respect to the car frame.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0bbebdd-7faa-4009-ad04-dc51a502832e": {"__data__": {"id_": "a0bbebdd-7faa-4009-ad04-dc51a502832e", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We denote cos(\u2217) and sin(\u2217) with c\u2217 and s\u2217, respectively. Note that with respect to the visual features model in Toibero et al.45 and Vas- sallo et al.,46 the expression of the middle point changes, due to the introduction of the lateral and longitudinal displacement, xw c and yw c , respectively, of the camera frame with respect to the car frame. As a consequence, to regulate the car position to the road center, we must de\ufb01ne a new visual feature \u0304xm = xm \u2212 k4. Then, the navigation task (6) is equivalent to the following visual task: according to (18), asymptotic convergence of xv and \u0304xm to zero implies convergence of x and \ud835\udf03 to zero, achieving the desired path fol- lowing task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a consequence, to regulate the car position to the road center, we must de\ufb01ne a new visual feature \u0304xm = xm \u2212 k4.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0da114be-0755-4206-8239-46fd644b8fde": {"__data__": {"id_": "0da114be-0755-4206-8239-46fd644b8fde", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note that with respect to the visual features model in Toibero et al.45 and Vas- sallo et al.,46 the expression of the middle point changes, due to the introduction of the lateral and longitudinal displacement, xw c and yw c , respectively, of the camera frame with respect to the car frame. As a consequence, to regulate the car position to the road center, we must de\ufb01ne a new visual feature \u0304xm = xm \u2212 k4. Then, the navigation task (6) is equivalent to the following visual task: according to (18), asymptotic convergence of xv and \u0304xm to zero implies convergence of x and \ud835\udf03 to zero, achieving the desired path fol- lowing task. Feedback stabilization of the dynamics of \u0304xm is given by the follow- ing angular velocity controller45:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the navigation task (6) is equivalent to the following visual task:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57042e45-0596-4fcc-8ba9-2eb45f9cf693": {"__data__": {"id_": "57042e45-0596-4fcc-8ba9-2eb45f9cf693", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "As a consequence, to regulate the car position to the road center, we must de\ufb01ne a new visual feature \u0304xm = xm \u2212 k4. Then, the navigation task (6) is equivalent to the following visual task: according to (18), asymptotic convergence of xv and \u0304xm to zero implies convergence of x and \ud835\udf03 to zero, achieving the desired path fol- lowing task. Feedback stabilization of the dynamics of \u0304xm is given by the follow- ing angular velocity controller45: This controller guarantees asymptotic convergenceofboth \u0304xm andxv tozero,undertheconditionsthatv > 0, and that k2 and k3 have the same sign, which is always true if (i) \ud835\udefe \u2208 (0,\ud835\udf0b\u22152)and(ii)yw c > \u2212zw c \u2215tan\ud835\udefe,twoconditionsalwaysveri\ufb01edwiththe proposed setup."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "according to (18), asymptotic convergence of xv and \u0304xm to zero implies convergence of x and \ud835\udf03 to zero, achieving the desired path fol- lowing task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7141f8d0-27db-4144-9473-227327c9ee54": {"__data__": {"id_": "7141f8d0-27db-4144-9473-227327c9ee54", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Then, the navigation task (6) is equivalent to the following visual task: according to (18), asymptotic convergence of xv and \u0304xm to zero implies convergence of x and \ud835\udf03 to zero, achieving the desired path fol- lowing task. Feedback stabilization of the dynamics of \u0304xm is given by the follow- ing angular velocity controller45: This controller guarantees asymptotic convergenceofboth \u0304xm andxv tozero,undertheconditionsthatv > 0, and that k2 and k3 have the same sign, which is always true if (i) \ud835\udefe \u2208 (0,\ud835\udf0b\u22152)and(ii)yw c > \u2212zw c \u2215tan\ud835\udefe,twoconditionsalwaysveri\ufb01edwiththe proposed setup. To realize the desired \ud835\udf14 in (20), the steering wheel must be turned according to (3):"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Feedback stabilization of the dynamics of \u0304xm is given by the follow- ing angular velocity controller45:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89e9060b-6249-4639-928f-fabc3900663a": {"__data__": {"id_": "89e9060b-6249-4639-928f-fabc3900663a", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "according to (18), asymptotic convergence of xv and \u0304xm to zero implies convergence of x and \ud835\udf03 to zero, achieving the desired path fol- lowing task. Feedback stabilization of the dynamics of \u0304xm is given by the follow- ing angular velocity controller45: This controller guarantees asymptotic convergenceofboth \u0304xm andxv tozero,undertheconditionsthatv > 0, and that k2 and k3 have the same sign, which is always true if (i) \ud835\udefe \u2208 (0,\ud835\udf0b\u22152)and(ii)yw c > \u2212zw c \u2215tan\ud835\udefe,twoconditionsalwaysveri\ufb01edwiththe proposed setup. To realize the desired \ud835\udf14 in (20), the steering wheel must be turned according to (3): where \u0304xm and xv are obtained by the image-processing algorithm of Section 4.1, whereas the value of v is estimated through the velocity estimation module presented in Section 4.2."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This controller guarantees asymptotic convergenceofboth \u0304xm andxv tozero,undertheconditionsthatv > 0, and that k2 and k3 have the same sign, which is always true if (i) \ud835\udefe \u2208 (0,\ud835\udf0b\u22152)and(ii)yw c > \u2212zw c \u2215tan\ud835\udefe,twoconditionsalwaysveri\ufb01edwiththe proposed setup.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fea33201-2eac-4cbc-95e4-175b60f09fdb": {"__data__": {"id_": "fea33201-2eac-4cbc-95e4-175b60f09fdb", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Feedback stabilization of the dynamics of \u0304xm is given by the follow- ing angular velocity controller45: This controller guarantees asymptotic convergenceofboth \u0304xm andxv tozero,undertheconditionsthatv > 0, and that k2 and k3 have the same sign, which is always true if (i) \ud835\udefe \u2208 (0,\ud835\udf0b\u22152)and(ii)yw c > \u2212zw c \u2215tan\ud835\udefe,twoconditionsalwaysveri\ufb01edwiththe proposed setup. To realize the desired \ud835\udf14 in (20), the steering wheel must be turned according to (3): where \u0304xm and xv are obtained by the image-processing algorithm of Section 4.1, whereas the value of v is estimated through the velocity estimation module presented in Section 4.2. and by virtue of the linear relationship between the car acceleration and the pedal angle (Eq."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To realize the desired \ud835\udf14 in (20), the steering wheel must be turned according to (3):", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56944e45-6aab-4f43-b579-e41229097380": {"__data__": {"id_": "56944e45-6aab-4f43-b579-e41229097380", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This controller guarantees asymptotic convergenceofboth \u0304xm andxv tozero,undertheconditionsthatv > 0, and that k2 and k3 have the same sign, which is always true if (i) \ud835\udefe \u2208 (0,\ud835\udf0b\u22152)and(ii)yw c > \u2212zw c \u2215tan\ud835\udefe,twoconditionsalwaysveri\ufb01edwiththe proposed setup. To realize the desired \ud835\udf14 in (20), the steering wheel must be turned according to (3): where \u0304xm and xv are obtained by the image-processing algorithm of Section 4.1, whereas the value of v is estimated through the velocity estimation module presented in Section 4.2. and by virtue of the linear relationship between the car acceleration and the pedal angle (Eq. (4)) to track a desired car linear velocity v\u2217 we designed a propor- tional integral derivative (PID) feedback controller to compute the gas pedal command:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where \u0304xm and xv are obtained by the image-processing algorithm of Section 4.1, whereas the value of v is estimated through the velocity estimation module presented in Section 4.2.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87987577-35a9-4135-87d8-64e484e7d842": {"__data__": {"id_": "87987577-35a9-4135-87d8-64e484e7d842", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "To realize the desired \ud835\udf14 in (20), the steering wheel must be turned according to (3): where \u0304xm and xv are obtained by the image-processing algorithm of Section 4.1, whereas the value of v is estimated through the velocity estimation module presented in Section 4.2. and by virtue of the linear relationship between the car acceleration and the pedal angle (Eq. (4)) to track a desired car linear velocity v\u2217 we designed a propor- tional integral derivative (PID) feedback controller to compute the gas pedal command: ev = (v\u2217 \u2212 v) is the difference between the desired and current value of the velocity, as computed by the car velocity estimation block, whereas kv,p, kv,i, and kv,d are the positive proportional, integral, and derivative gains, respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and by virtue of the linear relationship between the car acceleration and the pedal angle (Eq.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "397e2e26-fbf8-40fa-81f9-dcb507486f38": {"__data__": {"id_": "397e2e26-fbf8-40fa-81f9-dcb507486f38", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "where \u0304xm and xv are obtained by the image-processing algorithm of Section 4.1, whereas the value of v is estimated through the velocity estimation module presented in Section 4.2. and by virtue of the linear relationship between the car acceleration and the pedal angle (Eq. (4)) to track a desired car linear velocity v\u2217 we designed a propor- tional integral derivative (PID) feedback controller to compute the gas pedal command: ev = (v\u2217 \u2212 v) is the difference between the desired and current value of the velocity, as computed by the car velocity estimation block, whereas kv,p, kv,i, and kv,d are the positive proportional, integral, and derivative gains, respectively. In the design of the velocity control law, we decided to insert an integral action to compensate for constant dis- turbances(like,e.g., )atsteadystate.The derivative term helped achieving a damped control action."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(4)) to track a desired car linear velocity v\u2217 we designed a propor- tional integral derivative (PID) feedback controller to compute the gas pedal command:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8be3f661-f6b3-4ddc-8b23-3af31a692129": {"__data__": {"id_": "8be3f661-f6b3-4ddc-8b23-3af31a692129", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "and by virtue of the linear relationship between the car acceleration and the pedal angle (Eq. (4)) to track a desired car linear velocity v\u2217 we designed a propor- tional integral derivative (PID) feedback controller to compute the gas pedal command: ev = (v\u2217 \u2212 v) is the difference between the desired and current value of the velocity, as computed by the car velocity estimation block, whereas kv,p, kv,i, and kv,d are the positive proportional, integral, and derivative gains, respectively. In the design of the velocity control law, we decided to insert an integral action to compensate for constant dis- turbances(like,e.g., )atsteadystate.The derivative term helped achieving a damped control action. The desired velocity v\u2217 is set constant here."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ev = (v\u2217 \u2212 v) is the difference between the desired and current value of the velocity, as computed by the car velocity estimation block, whereas kv,p, kv,i, and kv,d are the positive proportional, integral, and derivative gains, respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e85b00d-9045-44a9-be7f-71c586c52c9d": {"__data__": {"id_": "2e85b00d-9045-44a9-be7f-71c586c52c9d", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "(4)) to track a desired car linear velocity v\u2217 we designed a propor- tional integral derivative (PID) feedback controller to compute the gas pedal command: ev = (v\u2217 \u2212 v) is the difference between the desired and current value of the velocity, as computed by the car velocity estimation block, whereas kv,p, kv,i, and kv,d are the positive proportional, integral, and derivative gains, respectively. In the design of the velocity control law, we decided to insert an integral action to compensate for constant dis- turbances(like,e.g., )atsteadystate.The derivative term helped achieving a damped control action. The desired velocity v\u2217 is set constant here. which enables the humanoid robot to turn the driving wheel by \ud835\udefc, and push the pedal by \ud835\udf01."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the design of the velocity control law, we decided to insert an integral action to compensate for constant dis- turbances(like,e.g., )atsteadystate.The derivative term helped achieving a damped control action.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e249e863-94b8-4de5-a947-6429cb03d698": {"__data__": {"id_": "e249e863-94b8-4de5-a947-6429cb03d698", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "ev = (v\u2217 \u2212 v) is the difference between the desired and current value of the velocity, as computed by the car velocity estimation block, whereas kv,p, kv,i, and kv,d are the positive proportional, integral, and derivative gains, respectively. In the design of the velocity control law, we decided to insert an integral action to compensate for constant dis- turbances(like,e.g., )atsteadystate.The derivative term helped achieving a damped control action. The desired velocity v\u2217 is set constant here. which enables the humanoid robot to turn the driving wheel by \ud835\udefc, and push the pedal by \ud835\udf01. The reference steering angle \ud835\udefc is converted to the reference pose of the hand grasping the wheel, through the rigid transformation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The desired velocity v\u2217 is set constant here.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e52e6af-3397-4d0e-96f9-7b75f5cdea4e": {"__data__": {"id_": "6e52e6af-3397-4d0e-96f9-7b75f5cdea4e", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In the design of the velocity control law, we decided to insert an integral action to compensate for constant dis- turbances(like,e.g., )atsteadystate.The derivative term helped achieving a damped control action. The desired velocity v\u2217 is set constant here. which enables the humanoid robot to turn the driving wheel by \ud835\udefc, and push the pedal by \ud835\udf01. The reference steering angle \ud835\udefc is converted to the reference pose of the hand grasping the wheel, through the rigid transformation Tb\u2217 h and Tb s are the transformation matrices expressing, respec- tively, the poses of frames h and s in Figure 3 with respect to b in Figure 2(a)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which enables the humanoid robot to turn the driving wheel by \ud835\udefc, and push the pedal by \ud835\udf01.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8ade469-80db-48bb-b180-67e0b744b18c": {"__data__": {"id_": "b8ade469-80db-48bb-b180-67e0b744b18c", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The desired velocity v\u2217 is set constant here. which enables the humanoid robot to turn the driving wheel by \ud835\udefc, and push the pedal by \ud835\udf01. The reference steering angle \ud835\udefc is converted to the reference pose of the hand grasping the wheel, through the rigid transformation Tb\u2217 h and Tb s are the transformation matrices expressing, respec- tively, the poses of frames h and s in Figure 3 with respect to b in Figure 2(a). Constant matrix Ts h expresses the pose of h with respect to s and depends on the steering wheel radius r, and on the angle \ud835\udefd parameterizing the hand position on the wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reference steering angle \ud835\udefc is converted to the reference pose of the hand grasping the wheel, through the rigid transformation", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b7270b2-ef2b-4688-858e-bc40c97f810c": {"__data__": {"id_": "0b7270b2-ef2b-4688-858e-bc40c97f810c", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "which enables the humanoid robot to turn the driving wheel by \ud835\udefc, and push the pedal by \ud835\udf01. The reference steering angle \ud835\udefc is converted to the reference pose of the hand grasping the wheel, through the rigid transformation Tb\u2217 h and Tb s are the transformation matrices expressing, respec- tively, the poses of frames h and s in Figure 3 with respect to b in Figure 2(a). Constant matrix Ts h expresses the pose of h with respect to s and depends on the steering wheel radius r, and on the angle \ud835\udefd parameterizing the hand position on the wheel. where f and f\u2217 are, respectively, the sensed and desired generalized interaction forces in h; M, B, and K \u2208 \u211d6\u00d76 are, respectively, the mass, damping, and stiffness diagonal matrices."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tb\u2217 h and Tb s are the transformation matrices expressing, respec- tively, the poses of frames h and s in Figure 3 with respect to b in Figure 2(a).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86c361b6-fab9-42f9-ad44-cc362baf0c4c": {"__data__": {"id_": "86c361b6-fab9-42f9-ad44-cc362baf0c4c", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The reference steering angle \ud835\udefc is converted to the reference pose of the hand grasping the wheel, through the rigid transformation Tb\u2217 h and Tb s are the transformation matrices expressing, respec- tively, the poses of frames h and s in Figure 3 with respect to b in Figure 2(a). Constant matrix Ts h expresses the pose of h with respect to s and depends on the steering wheel radius r, and on the angle \ud835\udefd parameterizing the hand position on the wheel. where f and f\u2217 are, respectively, the sensed and desired generalized interaction forces in h; M, B, and K \u2208 \u211d6\u00d76 are, respectively, the mass, damping, and stiffness diagonal matrices. (23)generatesvariationsofpose\u0394x,velocity\u0394 \u0307xandacceler- ation \u0394\u0308x of h with respect to s. Thus, the solution of (23) leads to the vector \u0394x that can be used to compute the transformation matrix \u0394T and to build up the new desired pose for the robot hands:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Constant matrix Ts h expresses the pose of h with respect to s and depends on the steering wheel radius r, and on the angle \ud835\udefd parameterizing the hand position on the wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aed2c5c9-07ab-4e95-af9b-d6a843b88369": {"__data__": {"id_": "aed2c5c9-07ab-4e95-af9b-d6a843b88369", "embedding": null, "metadata": {"page_number": 9, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Tb\u2217 h and Tb s are the transformation matrices expressing, respec- tively, the poses of frames h and s in Figure 3 with respect to b in Figure 2(a). Constant matrix Ts h expresses the pose of h with respect to s and depends on the steering wheel radius r, and on the angle \ud835\udefd parameterizing the hand position on the wheel. where f and f\u2217 are, respectively, the sensed and desired generalized interaction forces in h; M, B, and K \u2208 \u211d6\u00d76 are, respectively, the mass, damping, and stiffness diagonal matrices. (23)generatesvariationsofpose\u0394x,velocity\u0394 \u0307xandacceler- ation \u0394\u0308x of h with respect to s. Thus, the solution of (23) leads to the vector \u0394x that can be used to compute the transformation matrix \u0394T and to build up the new desired pose for the robot hands: to operate the gas pedalitissuf\ufb01cienttomovetheanklejointangleqa.From(22),wecom- pute the command for the robot ankle\u2019s angle as"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where f and f\u2217 are, respectively, the sensed and desired generalized interaction forces in h; M, B, and K \u2208 \u211d6\u00d76 are, respectively, the mass, damping, and stiffness diagonal matrices.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cef55299-7a81-4da8-b929-3c0812844529": {"__data__": {"id_": "cef55299-7a81-4da8-b929-3c0812844529", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Constant matrix Ts h expresses the pose of h with respect to s and depends on the steering wheel radius r, and on the angle \ud835\udefd parameterizing the hand position on the wheel. where f and f\u2217 are, respectively, the sensed and desired generalized interaction forces in h; M, B, and K \u2208 \u211d6\u00d76 are, respectively, the mass, damping, and stiffness diagonal matrices. (23)generatesvariationsofpose\u0394x,velocity\u0394 \u0307xandacceler- ation \u0394\u0308x of h with respect to s. Thus, the solution of (23) leads to the vector \u0394x that can be used to compute the transformation matrix \u0394T and to build up the new desired pose for the robot hands: to operate the gas pedalitissuf\ufb01cienttomovetheanklejointangleqa.From(22),wecom- pute the command for the robot ankle\u2019s angle as max is the robot ankle con\ufb01guration, at which the foot pushes the gas pedal, producing a signi\ufb01cant car acceleration."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(23)generatesvariationsofpose\u0394x,velocity\u0394 \u0307xandacceler- ation \u0394\u0308x of h with respect to s. Thus, the solution of (23) leads to the vector \u0394x that can be used to compute the transformation matrix \u0394T and to build up the new desired pose for the robot hands:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9670508-3ad2-42d6-a5d2-bb3e3bbda5d2": {"__data__": {"id_": "c9670508-3ad2-42d6-a5d2-bb3e3bbda5d2", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "where f and f\u2217 are, respectively, the sensed and desired generalized interaction forces in h; M, B, and K \u2208 \u211d6\u00d76 are, respectively, the mass, damping, and stiffness diagonal matrices. (23)generatesvariationsofpose\u0394x,velocity\u0394 \u0307xandacceler- ation \u0394\u0308x of h with respect to s. Thus, the solution of (23) leads to the vector \u0394x that can be used to compute the transformation matrix \u0394T and to build up the new desired pose for the robot hands: to operate the gas pedalitissuf\ufb01cienttomovetheanklejointangleqa.From(22),wecom- pute the command for the robot ankle\u2019s angle as max is the robot ankle con\ufb01guration, at which the foot pushes the gas pedal, producing a signi\ufb01cant car acceleration. A calibration procedure is run before starting driving, to identify the proper values of qa,min and qa,max."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to operate the gas pedalitissuf\ufb01cienttomovetheanklejointangleqa.From(22),wecom- pute the command for the robot ankle\u2019s angle as", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "964e0e81-35ad-448a-bcb3-279a45c96279": {"__data__": {"id_": "964e0e81-35ad-448a-bcb3-279a45c96279", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "(23)generatesvariationsofpose\u0394x,velocity\u0394 \u0307xandacceler- ation \u0394\u0308x of h with respect to s. Thus, the solution of (23) leads to the vector \u0394x that can be used to compute the transformation matrix \u0394T and to build up the new desired pose for the robot hands: to operate the gas pedalitissuf\ufb01cienttomovetheanklejointangleqa.From(22),wecom- pute the command for the robot ankle\u2019s angle as max is the robot ankle con\ufb01guration, at which the foot pushes the gas pedal, producing a signi\ufb01cant car acceleration. A calibration procedure is run before starting driving, to identify the proper values of qa,min and qa,max. Finally, \ud835\udf01 max is set to avoid large accelerations, while saturating the control action."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "max is the robot ankle con\ufb01guration, at which the foot pushes the gas pedal, producing a signi\ufb01cant car acceleration.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49369c25-b556-4dc1-b342-ec57e326aa56": {"__data__": {"id_": "49369c25-b556-4dc1-b342-ec57e326aa56", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "to operate the gas pedalitissuf\ufb01cienttomovetheanklejointangleqa.From(22),wecom- pute the command for the robot ankle\u2019s angle as max is the robot ankle con\ufb01guration, at which the foot pushes the gas pedal, producing a signi\ufb01cant car acceleration. A calibration procedure is run before starting driving, to identify the proper values of qa,min and qa,max. Finally, \ud835\udf01 max is set to avoid large accelerations, while saturating the control action. in the operational space (by de\ufb01ning a desired hand pose Tb h) and in the articular space (via the desired ankle joint angle qa)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A calibration procedure is run before starting driving, to identify the proper values of qa,min and qa,max.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b1243a7-e923-40a7-9424-0146dc865843": {"__data__": {"id_": "2b1243a7-e923-40a7-9424-0146dc865843", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "max is the robot ankle con\ufb01guration, at which the foot pushes the gas pedal, producing a signi\ufb01cant car acceleration. A calibration procedure is run before starting driving, to identify the proper values of qa,min and qa,max. Finally, \ud835\udf01 max is set to avoid large accelerations, while saturating the control action. in the operational space (by de\ufb01ning a desired hand pose Tb h) and in the articular space (via the desired ankle joint angle qa). Both can be real- izedusingourtask-basedQPcontroller, asladderclimbing.49 - lated as errors that appear among the sum of weighted least-squares termsintheQPcostfunction."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, \ud835\udf01 max is set to avoid large accelerations, while saturating the control action.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6537f27-53e1-4bad-abb7-8a5b85da1766": {"__data__": {"id_": "e6537f27-53e1-4bad-abb7-8a5b85da1766", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "A calibration procedure is run before starting driving, to identify the proper values of qa,min and qa,max. Finally, \ud835\udf01 max is set to avoid large accelerations, while saturating the control action. in the operational space (by de\ufb01ning a desired hand pose Tb h) and in the articular space (via the desired ankle joint angle qa). Both can be real- izedusingourtask-basedQPcontroller, asladderclimbing.49 - lated as errors that appear among the sum of weighted least-squares termsintheQPcostfunction. - mulated as linear expressions of the QP variables and appear in the constraints."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in the operational space (by de\ufb01ning a desired hand pose Tb h) and in the articular space (via the desired ankle joint angle qa).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "44044170-3cfd-4b0b-8b7d-5bca19871389": {"__data__": {"id_": "44044170-3cfd-4b0b-8b7d-5bca19871389", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Finally, \ud835\udf01 max is set to avoid large accelerations, while saturating the control action. in the operational space (by de\ufb01ning a desired hand pose Tb h) and in the articular space (via the desired ankle joint angle qa). Both can be real- izedusingourtask-basedQPcontroller, asladderclimbing.49 - lated as errors that appear among the sum of weighted least-squares termsintheQPcostfunction. - mulated as linear expressions of the QP variables and appear in the constraints. The QP controller is solved at each control step."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both can be real- izedusingourtask-basedQPcontroller, asladderclimbing.49 - lated as errors that appear among the sum of weighted least-squares termsintheQPcostfunction.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb349c86-c539-4371-8419-fd719cbed565": {"__data__": {"id_": "cb349c86-c539-4371-8419-fd719cbed565", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "in the operational space (by de\ufb01ning a desired hand pose Tb h) and in the articular space (via the desired ankle joint angle qa). Both can be real- izedusingourtask-basedQPcontroller, asladderclimbing.49 - lated as errors that appear among the sum of weighted least-squares termsintheQPcostfunction. - mulated as linear expressions of the QP variables and appear in the constraints. The QP controller is solved at each control step. The QP variable vector x = (\u0308qT,\ud835\udf40T)T gathers the joint acceleration \u0308q, and the linearized friction cones\u2019 base weights \ud835\udf40, such that the contact forces f are equal to K f \ud835\udf40 (with K f the discretized friction cone matrix)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- mulated as linear expressions of the QP variables and appear in the constraints.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2419ddd2-6b75-4664-9685-c73a6de49113": {"__data__": {"id_": "2419ddd2-6b75-4664-9685-c73a6de49113", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Both can be real- izedusingourtask-basedQPcontroller, asladderclimbing.49 - lated as errors that appear among the sum of weighted least-squares termsintheQPcostfunction. - mulated as linear expressions of the QP variables and appear in the constraints. The QP controller is solved at each control step. The QP variable vector x = (\u0308qT,\ud835\udf40T)T gathers the joint acceleration \u0308q, and the linearized friction cones\u2019 base weights \ud835\udf40, such that the contact forces f are equal to K f \ud835\udf40 (with K f the discretized friction cone matrix). The desired acceleration \u0308q is integrated twice to feed the low-level built- in PD control of HRP-2Kai."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The QP controller is solved at each control step.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e2b4fdd-8cdf-4a6d-8584-9b72f228f8f2": {"__data__": {"id_": "9e2b4fdd-8cdf-4a6d-8584-9b72f228f8f2", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "- mulated as linear expressions of the QP variables and appear in the constraints. The QP controller is solved at each control step. The QP variable vector x = (\u0308qT,\ud835\udf40T)T gathers the joint acceleration \u0308q, and the linearized friction cones\u2019 base weights \ud835\udf40, such that the contact forces f are equal to K f \ud835\udf40 (with K f the discretized friction cone matrix). The desired acceleration \u0308q is integrated twice to feed the low-level built- in PD control of HRP-2Kai. The driving task with the QP controller is written as follows:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The QP variable vector x = (\u0308qT,\ud835\udf40T)T gathers the joint acceleration \u0308q, and the linearized friction cones\u2019 base weights \ud835\udf40, such that the contact forces f are equal to K f \ud835\udf40 (with K f the discretized friction cone matrix).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d146f9d-5f5f-4ade-98db-e0e9bf65b443": {"__data__": {"id_": "9d146f9d-5f5f-4ade-98db-e0e9bf65b443", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The QP controller is solved at each control step. The QP variable vector x = (\u0308qT,\ud835\udf40T)T gathers the joint acceleration \u0308q, and the linearized friction cones\u2019 base weights \ud835\udf40, such that the contact forces f are equal to K f \ud835\udf40 (with K f the discretized friction cone matrix). The desired acceleration \u0308q is integrated twice to feed the low-level built- in PD control of HRP-2Kai. The driving task with the QP controller is written as follows: where wi and w\ud835\udf06 are task weights or gains and E i(q, \u0307q, \u0308q) is the error in the task space."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The desired acceleration \u0308q is integrated twice to feed the low-level built- in PD control of HRP-2Kai.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a19fe5de-b74f-4703-a629-ed8ba2ee5d22": {"__data__": {"id_": "a19fe5de-b74f-4703-a629-ed8ba2ee5d22", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The QP variable vector x = (\u0308qT,\ud835\udf40T)T gathers the joint acceleration \u0308q, and the linearized friction cones\u2019 base weights \ud835\udf40, such that the contact forces f are equal to K f \ud835\udf40 (with K f the discretized friction cone matrix). The desired acceleration \u0308q is integrated twice to feed the low-level built- in PD control of HRP-2Kai. The driving task with the QP controller is written as follows: where wi and w\ud835\udf06 are task weights or gains and E i(q, \u0307q, \u0308q) is the error in the task space. Details on the QP constraints (since they are common to most tasks) can be found in Vaillant et al.49"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The driving task with the QP controller is written as follows:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9083feef-9822-449f-a969-0c749060fb62": {"__data__": {"id_": "9083feef-9822-449f-a969-0c749060fb62", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The desired acceleration \u0308q is integrated twice to feed the low-level built- in PD control of HRP-2Kai. The driving task with the QP controller is written as follows: where wi and w\ud835\udf06 are task weights or gains and E i(q, \u0307q, \u0308q) is the error in the task space. Details on the QP constraints (since they are common to most tasks) can be found in Vaillant et al.49 we explicit the tasks used speci\ufb01cally during the driving (i.e., after the driving posture is reached)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where wi and w\ud835\udf06 are task weights or gains and E i(q, \u0307q, \u0308q) is the error in the task space.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6866a6cc-060b-45bd-a3e3-e6d04dd71bef": {"__data__": {"id_": "6866a6cc-060b-45bd-a3e3-e6d04dd71bef", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The driving task with the QP controller is written as follows: where wi and w\ud835\udf06 are task weights or gains and E i(q, \u0307q, \u0308q) is the error in the task space. Details on the QP constraints (since they are common to most tasks) can be found in Vaillant et al.49 we explicit the tasks used speci\ufb01cally during the driving (i.e., after the driving posture is reached). We use four (N = 4) set-point objective tasks; Each task (i) is de\ufb01ned by its associated task-error \ud835\udf50 so that E i = Kpi \ud835\udf50 i + Kvi \u0307\ud835\udf50 i + \u0308\ud835\udf50 i. i"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Details on the QP constraints (since they are common to most tasks) can be found in Vaillant et al.49", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3573fdea-2fa4-44a4-aa12-2a8a58779da2": {"__data__": {"id_": "3573fdea-2fa4-44a4-aa12-2a8a58779da2", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "where wi and w\ud835\udf06 are task weights or gains and E i(q, \u0307q, \u0308q) is the error in the task space. Details on the QP constraints (since they are common to most tasks) can be found in Vaillant et al.49 we explicit the tasks used speci\ufb01cally during the driving (i.e., after the driving posture is reached). We use four (N = 4) set-point objective tasks; Each task (i) is de\ufb01ned by its associated task-error \ud835\udf50 so that E i = Kpi \ud835\udf50 i + Kvi \u0307\ud835\udf50 i + \u0308\ud835\udf50 i. i The driving wheel of the car has been modeled as another \u201crobot\u201d having one joint (rotation)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we explicit the tasks used speci\ufb01cally during the driving (i.e., after the driving posture is reached).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdb65a88-e09a-439a-b019-b7ba6cd45b35": {"__data__": {"id_": "bdb65a88-e09a-439a-b019-b7ba6cd45b35", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Details on the QP constraints (since they are common to most tasks) can be found in Vaillant et al.49 we explicit the tasks used speci\ufb01cally during the driving (i.e., after the driving posture is reached). We use four (N = 4) set-point objective tasks; Each task (i) is de\ufb01ned by its associated task-error \ud835\udf50 so that E i = Kpi \ud835\udf50 i + Kvi \u0307\ud835\udf50 i + \u0308\ud835\udf50 i. i The driving wheel of the car has been modeled as another \u201crobot\u201d having one joint (rotation). We then merged the model of the driving wheel to that of the humanoid and linked them, through a position and orientationconstraint, sothatthe desired drivingwheelsteering angle \ud835\udefc, as computed by (24), induces a motion on the robot (right arm) grip- per."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use four (N = 4) set-point objective tasks; Each task (i) is de\ufb01ned by its associated task-error \ud835\udf50 so that E i = Kpi \ud835\udf50 i + Kvi \u0307\ud835\udf50 i + \u0308\ud835\udf50 i. i", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6858b31e-c16e-4082-afee-e29801ed4a60": {"__data__": {"id_": "6858b31e-c16e-4082-afee-e29801ed4a60", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we explicit the tasks used speci\ufb01cally during the driving (i.e., after the driving posture is reached). We use four (N = 4) set-point objective tasks; Each task (i) is de\ufb01ned by its associated task-error \ud835\udf50 so that E i = Kpi \ud835\udf50 i + Kvi \u0307\ud835\udf50 i + \u0308\ud835\udf50 i. i The driving wheel of the car has been modeled as another \u201crobot\u201d having one joint (rotation). We then merged the model of the driving wheel to that of the humanoid and linked them, through a position and orientationconstraint, sothatthe desired drivingwheelsteering angle \ud835\udefc, as computed by (24), induces a motion on the robot (right arm) grip- per. \u201crobot\u201dis , (e.g., buttock on the car seat, thighs, left foot)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The driving wheel of the car has been modeled as another \u201crobot\u201d having one joint (rotation).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "668539ba-989b-4e33-86ed-8a6d93abde41": {"__data__": {"id_": "668539ba-989b-4e33-86ed-8a6d93abde41", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We use four (N = 4) set-point objective tasks; Each task (i) is de\ufb01ned by its associated task-error \ud835\udf50 so that E i = Kpi \ud835\udf50 i + Kvi \u0307\ud835\udf50 i + \u0308\ud835\udf50 i. i The driving wheel of the car has been modeled as another \u201crobot\u201d having one joint (rotation). We then merged the model of the driving wheel to that of the humanoid and linked them, through a position and orientationconstraint, sothatthe desired drivingwheelsteering angle \ud835\udefc, as computed by (24), induces a motion on the robot (right arm) grip- per. \u201crobot\u201dis , (e.g., buttock on the car seat, thighs, left foot). The steering angle \ud835\udefc (i.e., the posture of the driving wheel robot) is a set-point task (E 1)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We then merged the model of the driving wheel to that of the humanoid and linked them, through a position and orientationconstraint, sothatthe desired drivingwheelsteering angle \ud835\udefc, as computed by (24), induces a motion on the robot (right arm) grip- per.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61541ddb-c964-4fda-bdec-7c87130ba617": {"__data__": {"id_": "61541ddb-c964-4fda-bdec-7c87130ba617", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The driving wheel of the car has been modeled as another \u201crobot\u201d having one joint (rotation). We then merged the model of the driving wheel to that of the humanoid and linked them, through a position and orientationconstraint, sothatthe desired drivingwheelsteering angle \ud835\udefc, as computed by (24), induces a motion on the robot (right arm) grip- per. \u201crobot\u201dis , (e.g., buttock on the car seat, thighs, left foot). The steering angle \ud835\udefc (i.e., the posture of the driving wheel robot) is a set-point task (E 1). The robot whole-body posture including the right anklejointcontrol(pedal)isalsoaset-pointtask(E 2),whichrealizesthe angle qa provided by (25)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u201crobot\u201dis , (e.g., buttock on the car seat, thighs, left foot).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "858c47ea-f1db-4538-88f0-09ba1b8113ae": {"__data__": {"id_": "858c47ea-f1db-4538-88f0-09ba1b8113ae", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We then merged the model of the driving wheel to that of the humanoid and linked them, through a position and orientationconstraint, sothatthe desired drivingwheelsteering angle \ud835\udefc, as computed by (24), induces a motion on the robot (right arm) grip- per. \u201crobot\u201dis , (e.g., buttock on the car seat, thighs, left foot). The steering angle \ud835\udefc (i.e., the posture of the driving wheel robot) is a set-point task (E 1). The robot whole-body posture including the right anklejointcontrol(pedal)isalsoaset-pointtask(E 2),whichrealizesthe angle qa provided by (25). Additional tasks were set to keep the gaze direction constant (E 3), and to \ufb01x the left arm, to avoid collisions with the car cockpit during the driving operation (E 4)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The steering angle \ud835\udefc (i.e., the posture of the driving wheel robot) is a set-point task (E 1).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ece23999-0cb5-43c8-a9b3-8a6cd1a16beb": {"__data__": {"id_": "ece23999-0cb5-43c8-a9b3-8a6cd1a16beb", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "\u201crobot\u201dis , (e.g., buttock on the car seat, thighs, left foot). The steering angle \ud835\udefc (i.e., the posture of the driving wheel robot) is a set-point task (E 1). The robot whole-body posture including the right anklejointcontrol(pedal)isalsoaset-pointtask(E 2),whichrealizesthe angle qa provided by (25). Additional tasks were set to keep the gaze direction constant (E 3), and to \ufb01x the left arm, to avoid collisions with the car cockpit during the driving operation (E 4). We tested our driving framework with the full-size humanoid robot HRP-2Kai built by Kawada Industries."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot whole-body posture including the right anklejointcontrol(pedal)isalsoaset-pointtask(E 2),whichrealizesthe angle qa provided by (25).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a888623-54da-4293-92f8-741accd4c161": {"__data__": {"id_": "3a888623-54da-4293-92f8-741accd4c161", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The steering angle \ud835\udefc (i.e., the posture of the driving wheel robot) is a set-point task (E 1). The robot whole-body posture including the right anklejointcontrol(pedal)isalsoaset-pointtask(E 2),whichrealizesthe angle qa provided by (25). Additional tasks were set to keep the gaze direction constant (E 3), and to \ufb01x the left arm, to avoid collisions with the car cockpit during the driving operation (E 4). We tested our driving framework with the full-size humanoid robot HRP-2Kai built by Kawada Industries. For the experiments, we used the Polaris Ranger XP900, the same utility vehicle employed at the DRC."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additional tasks were set to keep the gaze direction constant (E 3), and to \ufb01x the left arm, to avoid collisions with the car cockpit during the driving operation (E 4).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da52540a-5ccf-4db4-8f6f-022502024328": {"__data__": {"id_": "da52540a-5ccf-4db4-8f6f-022502024328", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The robot whole-body posture including the right anklejointcontrol(pedal)isalsoaset-pointtask(E 2),whichrealizesthe angle qa provided by (25). Additional tasks were set to keep the gaze direction constant (E 3), and to \ufb01x the left arm, to avoid collisions with the car cockpit during the driving operation (E 4). We tested our driving framework with the full-size humanoid robot HRP-2Kai built by Kawada Industries. For the experiments, we used the Polaris Ranger XP900, the same utility vehicle employed at the DRC. HRP-2Kai has 32 degrees of freedom, is 1.71 m tall and weighs 65 kg."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We tested our driving framework with the full-size humanoid robot HRP-2Kai built by Kawada Industries.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bbbb75b2-f363-4937-b5c4-11b541cf3a33": {"__data__": {"id_": "bbbb75b2-f363-4937-b5c4-11b541cf3a33", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Additional tasks were set to keep the gaze direction constant (E 3), and to \ufb01x the left arm, to avoid collisions with the car cockpit during the driving operation (E 4). We tested our driving framework with the full-size humanoid robot HRP-2Kai built by Kawada Industries. For the experiments, we used the Polaris Ranger XP900, the same utility vehicle employed at the DRC. HRP-2Kai has 32 degrees of freedom, is 1.71 m tall and weighs 65 kg. It is equipped with an Asus Xtion Pro 3D sensor, mounted on its head and used in this work as a monocular camera."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the experiments, we used the Polaris Ranger XP900, the same utility vehicle employed at the DRC.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efab3ed5-b83e-491e-a616-20e35d588633": {"__data__": {"id_": "efab3ed5-b83e-491e-a616-20e35d588633", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We tested our driving framework with the full-size humanoid robot HRP-2Kai built by Kawada Industries. For the experiments, we used the Polaris Ranger XP900, the same utility vehicle employed at the DRC. HRP-2Kai has 32 degrees of freedom, is 1.71 m tall and weighs 65 kg. It is equipped with an Asus Xtion Pro 3D sensor, mounted on its head and used in this work as a monocular camera. The Xtion cam- era provides images at 30 Hz with a resolution of 640 \u00d7 480 pixels."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "HRP-2Kai has 32 degrees of freedom, is 1.71 m tall and weighs 65 kg.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7f454f5-364d-4a83-9bef-f27015435587": {"__data__": {"id_": "f7f454f5-364d-4a83-9bef-f27015435587", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "For the experiments, we used the Polaris Ranger XP900, the same utility vehicle employed at the DRC. HRP-2Kai has 32 degrees of freedom, is 1.71 m tall and weighs 65 kg. It is equipped with an Asus Xtion Pro 3D sensor, mounted on its head and used in this work as a monocular camera. The Xtion cam- era provides images at 30 Hz with a resolution of 640 \u00d7 480 pixels. In the pre- sented experiments, xw c = \u22120.4 m, yw c = 1 m and zw c = 1.5 m were man- ually measured."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is equipped with an Asus Xtion Pro 3D sensor, mounted on its head and used in this work as a monocular camera.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ec63400-f511-42cd-96ed-c17525e3ffd0": {"__data__": {"id_": "9ec63400-f511-42cd-96ed-c17525e3ffd0", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "HRP-2Kai has 32 degrees of freedom, is 1.71 m tall and weighs 65 kg. It is equipped with an Asus Xtion Pro 3D sensor, mounted on its head and used in this work as a monocular camera. The Xtion cam- era provides images at 30 Hz with a resolution of 640 \u00d7 480 pixels. In the pre- sented experiments, xw c = \u22120.4 m, yw c = 1 m and zw c = 1.5 m were man- ually measured. However, it would be possible to estimate the robot camera position, with respect to the car frame, by localization of the humanoid50 or by using the geometric information of the car (that can be known, e.g., in the form of a CAD model, as shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Xtion cam- era provides images at 30 Hz with a resolution of 640 \u00d7 480 pixels.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4287913-94b4-45a8-8ae2-bdf3893d3256": {"__data__": {"id_": "b4287913-94b4-45a8-8ae2-bdf3893d3256", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "It is equipped with an Asus Xtion Pro 3D sensor, mounted on its head and used in this work as a monocular camera. The Xtion cam- era provides images at 30 Hz with a resolution of 640 \u00d7 480 pixels. In the pre- sented experiments, xw c = \u22120.4 m, yw c = 1 m and zw c = 1.5 m were man- ually measured. However, it would be possible to estimate the robot camera position, with respect to the car frame, by localization of the humanoid50 or by using the geometric information of the car (that can be known, e.g., in the form of a CAD model, as shown in Fig. Accelerometer data have been merged with the optical \ufb02ow to esti- mate the car linear velocity, as explained in Section 4.2."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the pre- sented experiments, xw c = \u22120.4 m, yw c = 1 m and zw c = 1.5 m were man- ually measured.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bbaee7b-fb09-4aa7-a87e-921152aecdec": {"__data__": {"id_": "0bbaee7b-fb09-4aa7-a87e-921152aecdec", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The Xtion cam- era provides images at 30 Hz with a resolution of 640 \u00d7 480 pixels. In the pre- sented experiments, xw c = \u22120.4 m, yw c = 1 m and zw c = 1.5 m were man- ually measured. However, it would be possible to estimate the robot camera position, with respect to the car frame, by localization of the humanoid50 or by using the geometric information of the car (that can be known, e.g., in the form of a CAD model, as shown in Fig. Accelerometer data have been merged with the optical \ufb02ow to esti- mate the car linear velocity, as explained in Section 4.2. Furthermore, a built-in \ufb01lter processes the IMU data to provide an accurate measure- ment of the robot chest orientation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, it would be possible to estimate the robot camera position, with respect to the car frame, by localization of the humanoid50 or by using the geometric information of the car (that can be known, e.g., in the form of a CAD model, as shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65d0fdad-6670-4310-aa0f-4e2a50171db8": {"__data__": {"id_": "65d0fdad-6670-4310-aa0f-4e2a50171db8", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In the pre- sented experiments, xw c = \u22120.4 m, yw c = 1 m and zw c = 1.5 m were man- ually measured. However, it would be possible to estimate the robot camera position, with respect to the car frame, by localization of the humanoid50 or by using the geometric information of the car (that can be known, e.g., in the form of a CAD model, as shown in Fig. Accelerometer data have been merged with the optical \ufb02ow to esti- mate the car linear velocity, as explained in Section 4.2. Furthermore, a built-in \ufb01lter processes the IMU data to provide an accurate measure- ment of the robot chest orientation. This is kinematically propagated uptotheXtionsensortoget\ud835\udefe, to the ground."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Accelerometer data have been merged with the optical \ufb02ow to esti- mate the car linear velocity, as explained in Section 4.2.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ba19d05-0439-4d0a-a877-140bdde9d36f": {"__data__": {"id_": "7ba19d05-0439-4d0a-a877-140bdde9d36f", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "However, it would be possible to estimate the robot camera position, with respect to the car frame, by localization of the humanoid50 or by using the geometric information of the car (that can be known, e.g., in the form of a CAD model, as shown in Fig. Accelerometer data have been merged with the optical \ufb02ow to esti- mate the car linear velocity, as explained in Section 4.2. Furthermore, a built-in \ufb01lter processes the IMU data to provide an accurate measure- ment of the robot chest orientation. This is kinematically propagated uptotheXtionsensortoget\ud835\udefe, to the ground. The task-based control is realized through the QP framework (see Section 6.3) which allows to easily set different tasks that can be achieved concurrently by the robot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, a built-in \ufb01lter processes the IMU data to provide an accurate measure- ment of the robot chest orientation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63c47e38-d519-4aa6-abc7-eb56ac59db94": {"__data__": {"id_": "63c47e38-d519-4aa6-abc7-eb56ac59db94", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Accelerometer data have been merged with the optical \ufb02ow to esti- mate the car linear velocity, as explained in Section 4.2. Furthermore, a built-in \ufb01lter processes the IMU data to provide an accurate measure- ment of the robot chest orientation. This is kinematically propagated uptotheXtionsensortoget\ud835\udefe, to the ground. The task-based control is realized through the QP framework (see Section 6.3) which allows to easily set different tasks that can be achieved concurrently by the robot. of the four set-point tasks described in Section 6.3."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is kinematically propagated uptotheXtionsensortoget\ud835\udefe, to the ground.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbf8f52f-0da0-4377-91a0-04329bcedc7b": {"__data__": {"id_": "dbf8f52f-0da0-4377-91a0-04329bcedc7b", "embedding": null, "metadata": {"page_number": 10, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Furthermore, a built-in \ufb01lter processes the IMU data to provide an accurate measure- ment of the robot chest orientation. This is kinematically propagated uptotheXtionsensortoget\ud835\udefe, to the ground. The task-based control is realized through the QP framework (see Section 6.3) which allows to easily set different tasks that can be achieved concurrently by the robot. of the four set-point tasks described in Section 6.3. As for the gains in Section 5, we set kv,p = 10\u22128, kv,d = 3 \u00d7 10\u22129 and kv,i = 2 \u00d7 10\u22129 \u2217,whereasinthesteer- = 3,andwesettheparame- terk\ud835\udefc = \u22125."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The task-based control is realized through the QP framework (see Section 6.3) which allows to easily set different tasks that can be achieved concurrently by the robot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fa50f28-461c-4b89-b4b7-3a6e799a0fb0": {"__data__": {"id_": "4fa50f28-461c-4b89-b4b7-3a6e799a0fb0", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This is kinematically propagated uptotheXtionsensortoget\ud835\udefe, to the ground. The task-based control is realized through the QP framework (see Section 6.3) which allows to easily set different tasks that can be achieved concurrently by the robot. of the four set-point tasks described in Section 6.3. As for the gains in Section 5, we set kv,p = 10\u22128, kv,d = 3 \u00d7 10\u22129 and kv,i = 2 \u00d7 10\u22129 \u2217,whereasinthesteer- = 3,andwesettheparame- terk\ud835\udefc = \u22125. -off between reactivity and control effort, the parameter k\ud835\udefc was roughly estimated."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of the four set-point tasks described in Section 6.3.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54b8c599-8116-462a-9cae-7320968c40b4": {"__data__": {"id_": "54b8c599-8116-462a-9cae-7320968c40b4", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The task-based control is realized through the QP framework (see Section 6.3) which allows to easily set different tasks that can be achieved concurrently by the robot. of the four set-point tasks described in Section 6.3. As for the gains in Section 5, we set kv,p = 10\u22128, kv,d = 3 \u00d7 10\u22129 and kv,i = 2 \u00d7 10\u22129 \u2217,whereasinthesteer- = 3,andwesettheparame- terk\ud835\udefc = \u22125. -off between reactivity and control effort, the parameter k\ud835\udefc was roughly estimated. Given the considered scenario, an exact knowledge of this parameter isgenerallynot possible, since it depends on the car charac- teristics."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As for the gains in Section 5, we set kv,p = 10\u22128, kv,d = 3 \u00d7 10\u22129 and kv,i = 2 \u00d7 10\u22129 \u2217,whereasinthesteer- = 3,andwesettheparame- terk\ud835\udefc = \u22125.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3329fdb8-f99a-42db-b056-189fb1d354a0": {"__data__": {"id_": "3329fdb8-f99a-42db-b056-189fb1d354a0", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "of the four set-point tasks described in Section 6.3. As for the gains in Section 5, we set kv,p = 10\u22128, kv,d = 3 \u00d7 10\u22129 and kv,i = 2 \u00d7 10\u22129 \u2217,whereasinthesteer- = 3,andwesettheparame- terk\ud835\udefc = \u22125. -off between reactivity and control effort, the parameter k\ud835\udefc was roughly estimated. Given the considered scenario, an exact knowledge of this parameter isgenerallynot possible, since it depends on the car charac- teristics. this would imply a bound on the parameter uncertainty to be satis\ufb01ed to pre- serve local stability."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "-off between reactivity and control effort, the parameter k\ud835\udefc was roughly estimated.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e6c2cdc-8ed5-49c1-8e5d-1f7c4199831c": {"__data__": {"id_": "0e6c2cdc-8ed5-49c1-8e5d-1f7c4199831c", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "As for the gains in Section 5, we set kv,p = 10\u22128, kv,d = 3 \u00d7 10\u22129 and kv,i = 2 \u00d7 10\u22129 \u2217,whereasinthesteer- = 3,andwesettheparame- terk\ud835\udefc = \u22125. -off between reactivity and control effort, the parameter k\ud835\udefc was roughly estimated. Given the considered scenario, an exact knowledge of this parameter isgenerallynot possible, since it depends on the car charac- teristics. this would imply a bound on the parameter uncertainty to be satis\ufb01ed to pre- serve local stability. While this analysis is beyond the scope of this paper, we also note that in practice it is not possible to limit the parameter uncertainty that depends on the car and the environment characteristics."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given the considered scenario, an exact knowledge of this parameter isgenerallynot possible, since it depends on the car charac- teristics.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e44ef91c-a872-42c9-b632-9402ac5d34d2": {"__data__": {"id_": "e44ef91c-a872-42c9-b632-9402ac5d34d2", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "-off between reactivity and control effort, the parameter k\ud835\udefc was roughly estimated. Given the considered scenario, an exact knowledge of this parameter isgenerallynot possible, since it depends on the car charac- teristics. this would imply a bound on the parameter uncertainty to be satis\ufb01ed to pre- serve local stability. While this analysis is beyond the scope of this paper, we also note that in practice it is not possible to limit the parameter uncertainty that depends on the car and the environment characteristics. Therefore, we rely on the experimental veri\ufb01cation of the vision-based controller robustness, delegating to the framework modes (autonomous, shared-autonomy, or teleoperated) the task of taking the controller within its region of local asymptotic stability."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "this would imply a bound on the parameter uncertainty to be satis\ufb01ed to pre- serve local stability.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3bc85fd-de3f-49bd-a68c-81da11120f08": {"__data__": {"id_": "b3bc85fd-de3f-49bd-a68c-81da11120f08", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Given the considered scenario, an exact knowledge of this parameter isgenerallynot possible, since it depends on the car charac- teristics. this would imply a bound on the parameter uncertainty to be satis\ufb01ed to pre- serve local stability. While this analysis is beyond the scope of this paper, we also note that in practice it is not possible to limit the parameter uncertainty that depends on the car and the environment characteristics. Therefore, we rely on the experimental veri\ufb01cation of the vision-based controller robustness, delegating to the framework modes (autonomous, shared-autonomy, or teleoperated) the task of taking the controller within its region of local asymptotic stability. In otherwords, - , ,theusercan always resort to the other driving modes."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While this analysis is beyond the scope of this paper, we also note that in practice it is not possible to limit the parameter uncertainty that depends on the car and the environment characteristics.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e88fd4e-8a32-49fc-b4b5-ea574ebcd425": {"__data__": {"id_": "0e88fd4e-8a32-49fc-b4b5-ea574ebcd425", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "this would imply a bound on the parameter uncertainty to be satis\ufb01ed to pre- serve local stability. While this analysis is beyond the scope of this paper, we also note that in practice it is not possible to limit the parameter uncertainty that depends on the car and the environment characteristics. Therefore, we rely on the experimental veri\ufb01cation of the vision-based controller robustness, delegating to the framework modes (autonomous, shared-autonomy, or teleoperated) the task of taking the controller within its region of local asymptotic stability. In otherwords, - , ,theusercan always resort to the other driving modes. the process and the measurement noise covariance matrices are set to diag(1e\u22124,1e\u22124) and diag(1e2,1e2), respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, we rely on the experimental veri\ufb01cation of the vision-based controller robustness, delegating to the framework modes (autonomous, shared-autonomy, or teleoperated) the task of taking the controller within its region of local asymptotic stability.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aaa824e3-2702-4d4e-afc3-aa25f2a3c6ac": {"__data__": {"id_": "aaa824e3-2702-4d4e-afc3-aa25f2a3c6ac", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "While this analysis is beyond the scope of this paper, we also note that in practice it is not possible to limit the parameter uncertainty that depends on the car and the environment characteristics. Therefore, we rely on the experimental veri\ufb01cation of the vision-based controller robustness, delegating to the framework modes (autonomous, shared-autonomy, or teleoperated) the task of taking the controller within its region of local asymptotic stability. In otherwords, - , ,theusercan always resort to the other driving modes. the process and the measurement noise covariance matrices are set to diag(1e\u22124,1e\u22124) and diag(1e2,1e2), respectively. Since the forward axis of the robot frame is aligned with the forward axis of the vehicle frame, to get aIMU"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In otherwords, - , ,theusercan always resort to the other driving modes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c81c6e2-85e1-4474-9e7b-909a34c96aed": {"__data__": {"id_": "3c81c6e2-85e1-4474-9e7b-909a34c96aed", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Therefore, we rely on the experimental veri\ufb01cation of the vision-based controller robustness, delegating to the framework modes (autonomous, shared-autonomy, or teleoperated) the task of taking the controller within its region of local asymptotic stability. In otherwords, - , ,theusercan always resort to the other driving modes. the process and the measurement noise covariance matrices are set to diag(1e\u22124,1e\u22124) and diag(1e2,1e2), respectively. Since the forward axis of the robot frame is aligned with the forward axis of the vehicle frame, to get aIMU we did not apply the transformation (16), but we simply collected the acceleration along the forward axis of the robot frame, as given by the accelerometers."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the process and the measurement noise covariance matrices are set to diag(1e\u22124,1e\u22124) and diag(1e2,1e2), respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c29b878d-0c41-4be7-a96c-20c344097d3b": {"__data__": {"id_": "c29b878d-0c41-4be7-a96c-20c344097d3b", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In otherwords, - , ,theusercan always resort to the other driving modes. the process and the measurement noise covariance matrices are set to diag(1e\u22124,1e\u22124) and diag(1e2,1e2), respectively. Since the forward axis of the robot frame is aligned with the forward axis of the vehicle frame, to get aIMU we did not apply the transformation (16), but we simply collected the acceleration along the forward axis of the robot frame, as given by the accelerometers. The sampling time of the KF was set to \u0394T = 0.002 s (being 500 Hz the frequency of the IMU measurements, the \ufb01lter runs at the same rate)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the forward axis of the robot frame is aligned with the forward axis of the vehicle frame, to get aIMU", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5c81d68-109c-432f-a125-e927a9151787": {"__data__": {"id_": "d5c81d68-109c-432f-a125-e927a9151787", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the process and the measurement noise covariance matrices are set to diag(1e\u22124,1e\u22124) and diag(1e2,1e2), respectively. Since the forward axis of the robot frame is aligned with the forward axis of the vehicle frame, to get aIMU we did not apply the transformation (16), but we simply collected the acceleration along the forward axis of the robot frame, as given by the accelerometers. The sampling time of the KF was set to \u0394T = 0.002 s (being 500 Hz the frequency of the IMU measurements, the \ufb01lter runs at the same rate). The cutoff frequencies of the low-pass \ufb01lters were applied to the visual features, and the car velocity estimate were set to 8 and 2.5 Hz, respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we did not apply the transformation (16), but we simply collected the acceleration along the forward axis of the robot frame, as given by the accelerometers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1c837f5-d983-4e02-acae-fa6cef336a5d": {"__data__": {"id_": "b1c837f5-d983-4e02-acae-fa6cef336a5d", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Since the forward axis of the robot frame is aligned with the forward axis of the vehicle frame, to get aIMU we did not apply the transformation (16), but we simply collected the acceleration along the forward axis of the robot frame, as given by the accelerometers. The sampling time of the KF was set to \u0394T = 0.002 s (being 500 Hz the frequency of the IMU measurements, the \ufb01lter runs at the same rate). The cutoff frequencies of the low-pass \ufb01lters were applied to the visual features, and the car velocity estimate were set to 8 and 2.5 Hz, respectively. we arrange the robot in the correct driving posture in the car as shown in Figure 9(a)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sampling time of the KF was set to \u0394T = 0.002 s (being 500 Hz the frequency of the IMU measurements, the \ufb01lter runs at the same rate).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f4a7f6c-9b3a-4cd0-af60-c7cc271cea10": {"__data__": {"id_": "8f4a7f6c-9b3a-4cd0-af60-c7cc271cea10", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we did not apply the transformation (16), but we simply collected the acceleration along the forward axis of the robot frame, as given by the accelerometers. The sampling time of the KF was set to \u0394T = 0.002 s (being 500 Hz the frequency of the IMU measurements, the \ufb01lter runs at the same rate). The cutoff frequencies of the low-pass \ufb01lters were applied to the visual features, and the car velocity estimate were set to 8 and 2.5 Hz, respectively. we arrange the robot in the correct driving posture in the car as shown in Figure 9(a). This posture (except for the driving leg and arm) is assumed constant during driving: All control parameters are kept constant."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The cutoff frequencies of the low-pass \ufb01lters were applied to the visual features, and the car velocity estimate were set to 8 and 2.5 Hz, respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cba3e429-938e-48c9-8e25-7f9d9647e03b": {"__data__": {"id_": "cba3e429-938e-48c9-8e25-7f9d9647e03b", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The sampling time of the KF was set to \u0394T = 0.002 s (being 500 Hz the frequency of the IMU measurements, the \ufb01lter runs at the same rate). The cutoff frequencies of the low-pass \ufb01lters were applied to the visual features, and the car velocity estimate were set to 8 and 2.5 Hz, respectively. we arrange the robot in the correct driving posture in the car as shown in Figure 9(a). This posture (except for the driving leg and arm) is assumed constant during driving: All control parameters are kept constant. At initializa- tion, we also correct eventual bad orientations of the camera with respect to the ground plane, by applying a rotation to the acquired image, and by regulating the pitch and yaw angles of the robot neck, so as to align the focal axis with the forward axis of the car reference frame."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we arrange the robot in the correct driving posture in the car as shown in Figure 9(a).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac3b0665-f62d-473d-bf4f-891cf31e44d0": {"__data__": {"id_": "ac3b0665-f62d-473d-bf4f-891cf31e44d0", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The cutoff frequencies of the low-pass \ufb01lters were applied to the visual features, and the car velocity estimate were set to 8 and 2.5 Hz, respectively. we arrange the robot in the correct driving posture in the car as shown in Figure 9(a). This posture (except for the driving leg and arm) is assumed constant during driving: All control parameters are kept constant. At initializa- tion, we also correct eventual bad orientations of the camera with respect to the ground plane, by applying a rotation to the acquired image, and by regulating the pitch and yaw angles of the robot neck, so as to align the focal axis with the forward axis of the car reference frame. The right foot is positioned on the gas pedal, and the calibra- tion procedure described in Section 6.2 is used to obtain qa,max and qa,min."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This posture (except for the driving leg and arm) is assumed constant during driving: All control parameters are kept constant.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d3e5dde-545b-41e5-90fe-80ca9ade55c1": {"__data__": {"id_": "8d3e5dde-545b-41e5-90fe-80ca9ade55c1", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we arrange the robot in the correct driving posture in the car as shown in Figure 9(a). This posture (except for the driving leg and arm) is assumed constant during driving: All control parameters are kept constant. At initializa- tion, we also correct eventual bad orientations of the camera with respect to the ground plane, by applying a rotation to the acquired image, and by regulating the pitch and yaw angles of the robot neck, so as to align the focal axis with the forward axis of the car reference frame. The right foot is positioned on the gas pedal, and the calibra- tion procedure described in Section 6.2 is used to obtain qa,max and qa,min. 9(a)], allowing the alignment of the wrist axis with that of the steer."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At initializa- tion, we also correct eventual bad orientations of the camera with respect to the ground plane, by applying a rotation to the acquired image, and by regulating the pitch and yaw angles of the robot neck, so as to align the focal axis with the forward axis of the car reference frame.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7455ba65-e26c-4ff3-99e8-0ca866f2840a": {"__data__": {"id_": "7455ba65-e26c-4ff3-99e8-0ca866f2840a", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This posture (except for the driving leg and arm) is assumed constant during driving: All control parameters are kept constant. At initializa- tion, we also correct eventual bad orientations of the camera with respect to the ground plane, by applying a rotation to the acquired image, and by regulating the pitch and yaw angles of the robot neck, so as to align the focal axis with the forward axis of the car reference frame. The right foot is positioned on the gas pedal, and the calibra- tion procedure described in Section 6.2 is used to obtain qa,max and qa,min. 9(a)], allowing the alignment of the wrist axis with that of the steer. With reference to Figure 3, this corresponds to con\ufb01guring the hand grasp with r = 0 and, to comply with the shape of the steering wheel, \ud835\udefd = 0.57 rad."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right foot is positioned on the gas pedal, and the calibra- tion procedure described in Section 6.2 is used to obtain qa,max and qa,min.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f64d9c32-bb21-4486-8a2f-4adaf7acbd9f": {"__data__": {"id_": "f64d9c32-bb21-4486-8a2f-4adaf7acbd9f", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "At initializa- tion, we also correct eventual bad orientations of the camera with respect to the ground plane, by applying a rotation to the acquired image, and by regulating the pitch and yaw angles of the robot neck, so as to align the focal axis with the forward axis of the car reference frame. The right foot is positioned on the gas pedal, and the calibra- tion procedure described in Section 6.2 is used to obtain qa,max and qa,min. 9(a)], allowing the alignment of the wrist axis with that of the steer. With reference to Figure 3, this corresponds to con\ufb01guring the hand grasp with r = 0 and, to comply with the shape of the steering wheel, \ud835\udefd = 0.57 rad. Owing to the robot kinematicconstraints, , imposed by our driving con\ufb01guration, the range of the steering angle \ud835\udefc is restricted from approximately \u20132 to \u20133 rad."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9(a)], allowing the alignment of the wrist axis with that of the steer.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52e9727c-5971-41e8-a5b4-85bc52398287": {"__data__": {"id_": "52e9727c-5971-41e8-a5b4-85bc52398287", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The right foot is positioned on the gas pedal, and the calibra- tion procedure described in Section 6.2 is used to obtain qa,max and qa,min. 9(a)], allowing the alignment of the wrist axis with that of the steer. With reference to Figure 3, this corresponds to con\ufb01guring the hand grasp with r = 0 and, to comply with the shape of the steering wheel, \ud835\udefd = 0.57 rad. Owing to the robot kinematicconstraints, , imposed by our driving con\ufb01guration, the range of the steering angle \ud835\udefc is restricted from approximately \u20132 to \u20133 rad. These limits cause bounds on the maximum curvature realizable by the car."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With reference to Figure 3, this corresponds to con\ufb01guring the hand grasp with r = 0 and, to comply with the shape of the steering wheel, \ud835\udefd = 0.57 rad.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd0032d2-e316-4110-9b80-fc900de7562b": {"__data__": {"id_": "fd0032d2-e316-4110-9b80-fc900de7562b", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "9(a)], allowing the alignment of the wrist axis with that of the steer. With reference to Figure 3, this corresponds to con\ufb01guring the hand grasp with r = 0 and, to comply with the shape of the steering wheel, \ud835\udefd = 0.57 rad. Owing to the robot kinematicconstraints, , imposed by our driving con\ufb01guration, the range of the steering angle \ud835\udefc is restricted from approximately \u20132 to \u20133 rad. These limits cause bounds on the maximum curvature realizable by the car. Nevertheless, .Formore challenging maneuvers, grasp recon\ufb01guration should be integrated in the framework."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Owing to the robot kinematicconstraints, , imposed by our driving con\ufb01guration, the range of the steering angle \ud835\udefc is restricted from approximately \u20132 to \u20133 rad.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb8bf0c1-1c92-4de7-a793-b69da6e0b3d2": {"__data__": {"id_": "cb8bf0c1-1c92-4de7-a793-b69da6e0b3d2", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "With reference to Figure 3, this corresponds to con\ufb01guring the hand grasp with r = 0 and, to comply with the shape of the steering wheel, \ud835\udefd = 0.57 rad. Owing to the robot kinematicconstraints, , imposed by our driving con\ufb01guration, the range of the steering angle \ud835\udefc is restricted from approximately \u20132 to \u20133 rad. These limits cause bounds on the maximum curvature realizable by the car. Nevertheless, .Formore challenging maneuvers, grasp recon\ufb01guration should be integrated in the framework. we showed in Paolillo et al.15 that the admittance control can be easily plugged in our frame- work, whenever needed."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These limits cause bounds on the maximum curvature realizable by the car.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "660b086f-4187-49a0-9392-971d279c4737": {"__data__": {"id_": "660b086f-4187-49a0-9392-971d279c4737", "embedding": null, "metadata": {"page_number": 11, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Owing to the robot kinematicconstraints, , imposed by our driving con\ufb01guration, the range of the steering angle \ud835\udefc is restricted from approximately \u20132 to \u20133 rad. These limits cause bounds on the maximum curvature realizable by the car. Nevertheless, .Formore challenging maneuvers, grasp recon\ufb01guration should be integrated in the framework. we showed in Paolillo et al.15 that the admittance control can be easily plugged in our frame- work, whenever needed. In fact, in that work, an HRP-4, from Kawada Industries, turns the steering wheel with a more \u201chuman-like\u201d grasp [r = 0.2 m and \ud835\udefd = 1.05 rad; see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless, .Formore challenging maneuvers, grasp recon\ufb01guration should be integrated in the framework.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8fa835a-0a79-4924-8da3-ae4ed5149512": {"__data__": {"id_": "a8fa835a-0a79-4924-8da3-ae4ed5149512", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "These limits cause bounds on the maximum curvature realizable by the car. Nevertheless, .Formore challenging maneuvers, grasp recon\ufb01guration should be integrated in the framework. we showed in Paolillo et al.15 that the admittance control can be easily plugged in our frame- work, whenever needed. In fact, in that work, an HRP-4, from Kawada Industries, turns the steering wheel with a more \u201chuman-like\u201d grasp [r = 0.2 m and \ud835\udefd = 1.05 rad; see Fig. Owing to the characteris- tics of both the grasp and the HRP-4 hand, admittance control is nec- essary."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we showed in Paolillo et al.15 that the admittance control can be easily plugged in our frame- work, whenever needed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ee43df9-67e5-4b98-b19e-bbc4a3bf6878": {"__data__": {"id_": "0ee43df9-67e5-4b98-b19e-bbc4a3bf6878", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Nevertheless, .Formore challenging maneuvers, grasp recon\ufb01guration should be integrated in the framework. we showed in Paolillo et al.15 that the admittance control can be easily plugged in our frame- work, whenever needed. In fact, in that work, an HRP-4, from Kawada Industries, turns the steering wheel with a more \u201chuman-like\u201d grasp [r = 0.2 m and \ud835\udefd = 1.05 rad; see Fig. Owing to the characteris- tics of both the grasp and the HRP-4 hand, admittance control is nec- essary. For sake of completeness, we report in Figures 8(b)\u20138(d) plots of the admittance behavior relative to that experiment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, in that work, an HRP-4, from Kawada Industries, turns the steering wheel with a more \u201chuman-like\u201d grasp [r = 0.2 m and \ud835\udefd = 1.05 rad; see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f2bf6a3-209a-49c8-b26f-1f887ccb6449": {"__data__": {"id_": "7f2bf6a3-209a-49c8-b26f-1f887ccb6449", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we showed in Paolillo et al.15 that the admittance control can be easily plugged in our frame- work, whenever needed. In fact, in that work, an HRP-4, from Kawada Industries, turns the steering wheel with a more \u201chuman-like\u201d grasp [r = 0.2 m and \ud835\udefd = 1.05 rad; see Fig. Owing to the characteris- tics of both the grasp and the HRP-4 hand, admittance control is nec- essary. For sake of completeness, we report in Figures 8(b)\u20138(d) plots of the admittance behavior relative to that experiment. In particular, to have good tracking of the steering angle \ud835\udefc, while complying with the steering wheel geometric constraint, we designed a fast (stiff) behav- ior along the z-axis of the hand frame, h and a slow (compliant) along the x- and y-axes."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Owing to the characteris- tics of both the grasp and the HRP-4 hand, admittance control is nec- essary.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62981326-88f6-47d7-822c-ca4147a7bdd0": {"__data__": {"id_": "62981326-88f6-47d7-822c-ca4147a7bdd0", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In fact, in that work, an HRP-4, from Kawada Industries, turns the steering wheel with a more \u201chuman-like\u201d grasp [r = 0.2 m and \ud835\udefd = 1.05 rad; see Fig. Owing to the characteris- tics of both the grasp and the HRP-4 hand, admittance control is nec- essary. For sake of completeness, we report in Figures 8(b)\u20138(d) plots of the admittance behavior relative to that experiment. In particular, to have good tracking of the steering angle \ud835\udefc, while complying with the steering wheel geometric constraint, we designed a fast (stiff) behav- ior along the z-axis of the hand frame, h and a slow (compliant) along the x- and y-axes. To this end, we set the admittance parameters: mx = my = 2000kg,mz = 10kg,bx = by = 1600kg/s,bz = 240kg/s,andkx = ky = 20 kg/s2, kz = 1000 kg/s2."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For sake of completeness, we report in Figures 8(b)\u20138(d) plots of the admittance behavior relative to that experiment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8913fbf-4f2c-4cf2-a84d-3b28b71d0369": {"__data__": {"id_": "a8913fbf-4f2c-4cf2-a84d-3b28b71d0369", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Owing to the characteris- tics of both the grasp and the HRP-4 hand, admittance control is nec- essary. For sake of completeness, we report in Figures 8(b)\u20138(d) plots of the admittance behavior relative to that experiment. In particular, to have good tracking of the steering angle \ud835\udefc, while complying with the steering wheel geometric constraint, we designed a fast (stiff) behav- ior along the z-axis of the hand frame, h and a slow (compliant) along the x- and y-axes. To this end, we set the admittance parameters: mx = my = 2000kg,mz = 10kg,bx = by = 1600kg/s,bz = 240kg/s,andkx = ky = 20 kg/s2, kz = 1000 kg/s2. Furthermore, we set the desired forces f\u2217 x = f\u2217 z = 0 N, while along the y-axis of the hand frame f\u2217 y = 5 N, to improve the grasping stability."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, to have good tracking of the steering angle \ud835\udefc, while complying with the steering wheel geometric constraint, we designed a fast (stiff) behav- ior along the z-axis of the hand frame, h and a slow (compliant) along the x- and y-axes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2207b831-b3bd-47ab-b2a6-98cb28612181": {"__data__": {"id_": "2207b831-b3bd-47ab-b2a6-98cb28612181", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "For sake of completeness, we report in Figures 8(b)\u20138(d) plots of the admittance behavior relative to that experiment. In particular, to have good tracking of the steering angle \ud835\udefc, while complying with the steering wheel geometric constraint, we designed a fast (stiff) behav- ior along the z-axis of the hand frame, h and a slow (compliant) along the x- and y-axes. To this end, we set the admittance parameters: mx = my = 2000kg,mz = 10kg,bx = by = 1600kg/s,bz = 240kg/s,andkx = ky = 20 kg/s2, kz = 1000 kg/s2. Furthermore, we set the desired forces f\u2217 x = f\u2217 z = 0 N, while along the y-axis of the hand frame f\u2217 y = 5 N, to improve the grasping stability. Note that the evolution of the displace- ments along the x- and y-axes [plots in Figs."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To this end, we set the admittance parameters: mx = my = 2000kg,mz = 10kg,bx = by = 1600kg/s,bz = 240kg/s,andkx = ky = 20 kg/s2, kz = 1000 kg/s2.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bef7599-9bb5-4b57-b44f-21df3160d0f0": {"__data__": {"id_": "3bef7599-9bb5-4b57-b44f-21df3160d0f0", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In particular, to have good tracking of the steering angle \ud835\udefc, while complying with the steering wheel geometric constraint, we designed a fast (stiff) behav- ior along the z-axis of the hand frame, h and a slow (compliant) along the x- and y-axes. To this end, we set the admittance parameters: mx = my = 2000kg,mz = 10kg,bx = by = 1600kg/s,bz = 240kg/s,andkx = ky = 20 kg/s2, kz = 1000 kg/s2. Furthermore, we set the desired forces f\u2217 x = f\u2217 z = 0 N, while along the y-axis of the hand frame f\u2217 y = 5 N, to improve the grasping stability. Note that the evolution of the displace- ments along the x- and y-axes [plots in Figs. 8(b)\u20138(c)] is the results of a dynamic behavior that \ufb01lters the high frequency of the input forces, whereas along the z-axis the response of the system is more reactive."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, we set the desired forces f\u2217 x = f\u2217 z = 0 N, while along the y-axis of the hand frame f\u2217 y = 5 N, to improve the grasping stability.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc3b3376-fe75-4d4c-8b9c-eb23cd817639": {"__data__": {"id_": "bc3b3376-fe75-4d4c-8b9c-eb23cd817639", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "To this end, we set the admittance parameters: mx = my = 2000kg,mz = 10kg,bx = by = 1600kg/s,bz = 240kg/s,andkx = ky = 20 kg/s2, kz = 1000 kg/s2. Furthermore, we set the desired forces f\u2217 x = f\u2217 z = 0 N, while along the y-axis of the hand frame f\u2217 y = 5 N, to improve the grasping stability. Note that the evolution of the displace- ments along the x- and y-axes [plots in Figs. 8(b)\u20138(c)] is the results of a dynamic behavior that \ufb01lters the high frequency of the input forces, whereas along the z-axis the response of the system is more reactive. In particular, we present the results of the experiments performed at the authorized portion of the AIST campus in Tsukuba, Japan."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the evolution of the displace- ments along the x- and y-axes [plots in Figs.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3564d56b-33eb-488a-a9ca-717ab6a6f971": {"__data__": {"id_": "3564d56b-33eb-488a-a9ca-717ab6a6f971", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Furthermore, we set the desired forces f\u2217 x = f\u2217 z = 0 N, while along the y-axis of the hand frame f\u2217 y = 5 N, to improve the grasping stability. Note that the evolution of the displace- ments along the x- and y-axes [plots in Figs. 8(b)\u20138(c)] is the results of a dynamic behavior that \ufb01lters the high frequency of the input forces, whereas along the z-axis the response of the system is more reactive. In particular, we present the results of the experiments performed at the authorized portion of the AIST campus in Tsukuba, Japan. A top view of this experimental \ufb01eld is shown in Figure 9(b)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8(b)\u20138(c)] is the results of a dynamic behavior that \ufb01lters the high frequency of the input forces, whereas along the z-axis the response of the system is more reactive.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82976e87-3c86-4eb6-b59d-69e2a88ccb66": {"__data__": {"id_": "82976e87-3c86-4eb6-b59d-69e2a88ccb66", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note that the evolution of the displace- ments along the x- and y-axes [plots in Figs. 8(b)\u20138(c)] is the results of a dynamic behavior that \ufb01lters the high frequency of the input forces, whereas along the z-axis the response of the system is more reactive. In particular, we present the results of the experiments performed at the authorized portion of the AIST campus in Tsukuba, Japan. A top view of this experimental \ufb01eld is shown in Figure 9(b). The areashighlightedinredand yellowcorrespond tothe pathsdriven using the autonomous and teleoperated mode, respectively, as fur- ther described below."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, we present the results of the experiments performed at the authorized portion of the AIST campus in Tsukuba, Japan.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0397d77-e454-4f58-b3ca-0d43f9acedcc": {"__data__": {"id_": "c0397d77-e454-4f58-b3ca-0d43f9acedcc", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "8(b)\u20138(c)] is the results of a dynamic behavior that \ufb01lters the high frequency of the input forces, whereas along the z-axis the response of the system is more reactive. In particular, we present the results of the experiments performed at the authorized portion of the AIST campus in Tsukuba, Japan. A top view of this experimental \ufb01eld is shown in Figure 9(b). The areashighlightedinredand yellowcorrespond tothe pathsdriven using the autonomous and teleoperated mode, respectively, as fur- ther described below. Furthermore, we present an experiment per- formed at the DRC \ufb01nal, showing the effectiveness of the shared- autonomydrivingmode.Foraquantitative evaluationoftheapproach, .Thesameexperiments are shown in the video available at https://youtu.be/SYHI2JmJ-lk that also allows a qualitative evaluation of the online image processing."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A top view of this experimental \ufb01eld is shown in Figure 9(b).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b073eec-1bb8-42c7-aca2-511152d9a5b8": {"__data__": {"id_": "1b073eec-1bb8-42c7-aca2-511152d9a5b8", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In particular, we present the results of the experiments performed at the authorized portion of the AIST campus in Tsukuba, Japan. A top view of this experimental \ufb01eld is shown in Figure 9(b). The areashighlightedinredand yellowcorrespond tothe pathsdriven using the autonomous and teleoperated mode, respectively, as fur- ther described below. Furthermore, we present an experiment per- formed at the DRC \ufb01nal, showing the effectiveness of the shared- autonomydrivingmode.Foraquantitative evaluationoftheapproach, .Thesameexperiments are shown in the video available at https://youtu.be/SYHI2JmJ-lk that also allows a qualitative evaluation of the online image processing. Quantitatively, we successfully carried out 14 experiments over of"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The areashighlightedinredand yellowcorrespond tothe pathsdriven using the autonomous and teleoperated mode, respectively, as fur- ther described below.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24a863c1-34a8-4062-a11a-09220a0e0068": {"__data__": {"id_": "24a863c1-34a8-4062-a11a-09220a0e0068", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "A top view of this experimental \ufb01eld is shown in Figure 9(b). The areashighlightedinredand yellowcorrespond tothe pathsdriven using the autonomous and teleoperated mode, respectively, as fur- ther described below. Furthermore, we present an experiment per- formed at the DRC \ufb01nal, showing the effectiveness of the shared- autonomydrivingmode.Foraquantitative evaluationoftheapproach, .Thesameexperiments are shown in the video available at https://youtu.be/SYHI2JmJ-lk that also allows a qualitative evaluation of the online image processing. Quantitatively, we successfully carried out 14 experiments over of 15 repetitions, executed at different times, between 10:30 a.m. and 4 p.m., proving image-processing robustness in different days and time (i.e., under different light conditions)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, we present an experiment per- formed at the DRC \ufb01nal, showing the effectiveness of the shared- autonomydrivingmode.Foraquantitative evaluationoftheapproach, .Thesameexperiments are shown in the video available at https://youtu.be/SYHI2JmJ-lk that also allows a qualitative evaluation of the online image processing.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb30c30c-de17-4f6f-9244-43ae169794f2": {"__data__": {"id_": "eb30c30c-de17-4f6f-9244-43ae169794f2", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The areashighlightedinredand yellowcorrespond tothe pathsdriven using the autonomous and teleoperated mode, respectively, as fur- ther described below. Furthermore, we present an experiment per- formed at the DRC \ufb01nal, showing the effectiveness of the shared- autonomydrivingmode.Foraquantitative evaluationoftheapproach, .Thesameexperiments are shown in the video available at https://youtu.be/SYHI2JmJ-lk that also allows a qualitative evaluation of the online image processing. Quantitatively, we successfully carried out 14 experiments over of 15 repetitions, executed at different times, between 10:30 a.m. and 4 p.m., proving image-processing robustness in different days and time (i.e., under different light conditions). In the \ufb01rst experiment, we tested the autonomous mode, that is, the effectiveness of our framework to make a humanoid robot drive a car autonomously."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Quantitatively, we successfully carried out 14 experiments over of", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a0f5758-324a-4162-a640-2ff9ffe82daf": {"__data__": {"id_": "7a0f5758-324a-4162-a640-2ff9ffe82daf", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Furthermore, we present an experiment per- formed at the DRC \ufb01nal, showing the effectiveness of the shared- autonomydrivingmode.Foraquantitative evaluationoftheapproach, .Thesameexperiments are shown in the video available at https://youtu.be/SYHI2JmJ-lk that also allows a qualitative evaluation of the online image processing. Quantitatively, we successfully carried out 14 experiments over of 15 repetitions, executed at different times, between 10:30 a.m. and 4 p.m., proving image-processing robustness in different days and time (i.e., under different light conditions). In the \ufb01rst experiment, we tested the autonomous mode, that is, the effectiveness of our framework to make a humanoid robot drive a car autonomously. For this experiment, we choose v\u2217 = 1.2 m/s, whereas the foot calibration procedure gave qa,max = \u22120.44 rad and qa,min = \u22120.5 rad."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15 repetitions, executed at different times, between 10:30 a.m. and 4 p.m., proving image-processing robustness in different days and time (i.e., under different light conditions).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c09089c-638c-4cca-9e05-8270767285ac": {"__data__": {"id_": "1c09089c-638c-4cca-9e05-8270767285ac", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Quantitatively, we successfully carried out 14 experiments over of 15 repetitions, executed at different times, between 10:30 a.m. and 4 p.m., proving image-processing robustness in different days and time (i.e., under different light conditions). In the \ufb01rst experiment, we tested the autonomous mode, that is, the effectiveness of our framework to make a humanoid robot drive a car autonomously. For this experiment, we choose v\u2217 = 1.2 m/s, whereas the foot calibration procedure gave qa,max = \u22120.44 rad and qa,min = \u22120.5 rad. The car starts with an initial lateral offset, that is corrected after a few meters."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the \ufb01rst experiment, we tested the autonomous mode, that is, the effectiveness of our framework to make a humanoid robot drive a car autonomously.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0754997f-2f92-4755-97c5-28d2bd78e6ef": {"__data__": {"id_": "0754997f-2f92-4755-97c5-28d2bd78e6ef", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "15 repetitions, executed at different times, between 10:30 a.m. and 4 p.m., proving image-processing robustness in different days and time (i.e., under different light conditions). In the \ufb01rst experiment, we tested the autonomous mode, that is, the effectiveness of our framework to make a humanoid robot drive a car autonomously. For this experiment, we choose v\u2217 = 1.2 m/s, whereas the foot calibration procedure gave qa,max = \u22120.44 rad and qa,min = \u22120.5 rad. The car starts with an initial lateral offset, that is corrected after a few meters. The snapshots (as well as the video) of the experi- ment show that the car correctly travels at the center of a curved path, for about 100 m. Furthermore, one can observe that the differences in the light conditions (due to the tree shadows) and in the color of the road do not jeopardize the correct detection of the borders and, consequently, the driving performance."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this experiment, we choose v\u2217 = 1.2 m/s, whereas the foot calibration procedure gave qa,max = \u22120.44 rad and qa,min = \u22120.5 rad.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e29dc5e8-9d27-4358-a41b-ed0437e9cc00": {"__data__": {"id_": "e29dc5e8-9d27-4358-a41b-ed0437e9cc00", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In the \ufb01rst experiment, we tested the autonomous mode, that is, the effectiveness of our framework to make a humanoid robot drive a car autonomously. For this experiment, we choose v\u2217 = 1.2 m/s, whereas the foot calibration procedure gave qa,max = \u22120.44 rad and qa,min = \u22120.5 rad. The car starts with an initial lateral offset, that is corrected after a few meters. The snapshots (as well as the video) of the experi- ment show that the car correctly travels at the center of a curved path, for about 100 m. Furthermore, one can observe that the differences in the light conditions (due to the tree shadows) and in the color of the road do not jeopardize the correct detection of the borders and, consequently, the driving performance. On the top, we plot aIMU, the accelera- tion along the forward axis of the car, as reconstructed from the robot accelerometers."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The car starts with an initial lateral offset, that is corrected after a few meters.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d3e1973-7606-4925-8473-54100070756a": {"__data__": {"id_": "4d3e1973-7606-4925-8473-54100070756a", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "For this experiment, we choose v\u2217 = 1.2 m/s, whereas the foot calibration procedure gave qa,max = \u22120.44 rad and qa,min = \u22120.5 rad. The car starts with an initial lateral offset, that is corrected after a few meters. The snapshots (as well as the video) of the experi- ment show that the car correctly travels at the center of a curved path, for about 100 m. Furthermore, one can observe that the differences in the light conditions (due to the tree shadows) and in the color of the road do not jeopardize the correct detection of the borders and, consequently, the driving performance. On the top, we plot aIMU, the accelera- tion along the forward axis of the car, as reconstructed from the robot accelerometers. The center plot shows the car speed measured with the optical \ufb02ow-based method (vOF), whereas the bottom plot gives the trace of the car speed v obtained by fusing aIMU and vOF."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The snapshots (as well as the video) of the experi- ment show that the car correctly travels at the center of a curved path, for about 100 m. Furthermore, one can observe that the differences in the light conditions (due to the tree shadows) and in the color of the road do not jeopardize the correct detection of the borders and, consequently, the driving performance.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb8aa0ec-5f8a-406e-8c07-79687bedeec3": {"__data__": {"id_": "bb8aa0ec-5f8a-406e-8c07-79687bedeec3", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The car starts with an initial lateral offset, that is corrected after a few meters. The snapshots (as well as the video) of the experi- ment show that the car correctly travels at the center of a curved path, for about 100 m. Furthermore, one can observe that the differences in the light conditions (due to the tree shadows) and in the color of the road do not jeopardize the correct detection of the borders and, consequently, the driving performance. On the top, we plot aIMU, the accelera- tion along the forward axis of the car, as reconstructed from the robot accelerometers. The center plot shows the car speed measured with the optical \ufb02ow-based method (vOF), whereas the bottom plot gives the trace of the car speed v obtained by fusing aIMU and vOF. Note that the KF reduces the noise of the vOF signal, a very important feature for keeping the derivative action in the velocity control law (22)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the top, we plot aIMU, the accelera- tion along the forward axis of the car, as reconstructed from the robot accelerometers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f10340a6-1610-4d25-b9e5-fda75e92afd2": {"__data__": {"id_": "f10340a6-1610-4d25-b9e5-fda75e92afd2", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The snapshots (as well as the video) of the experi- ment show that the car correctly travels at the center of a curved path, for about 100 m. Furthermore, one can observe that the differences in the light conditions (due to the tree shadows) and in the color of the road do not jeopardize the correct detection of the borders and, consequently, the driving performance. On the top, we plot aIMU, the accelera- tion along the forward axis of the car, as reconstructed from the robot accelerometers. The center plot shows the car speed measured with the optical \ufb02ow-based method (vOF), whereas the bottom plot gives the trace of the car speed v obtained by fusing aIMU and vOF. Note that the KF reduces the noise of the vOF signal, a very important feature for keeping the derivative action in the velocity control law (22). reconstruction from vision (e.g., the \u201cstructure from motion\u201d problem) suffers from a scale problem, in the translation vec- tor estimate.47 This issue, due to the loss of information in mapping two-dimensionalto3Ddata,isalsopresentinoptical\ufb02owvelocityesti- mation methods."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The center plot shows the car speed measured with the optical \ufb02ow-based method (vOF), whereas the bottom plot gives the trace of the car speed v obtained by fusing aIMU and vOF.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df75f5af-260f-4938-98a4-4ad1e0740de9": {"__data__": {"id_": "df75f5af-260f-4938-98a4-4ad1e0740de9", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "On the top, we plot aIMU, the accelera- tion along the forward axis of the car, as reconstructed from the robot accelerometers. The center plot shows the car speed measured with the optical \ufb02ow-based method (vOF), whereas the bottom plot gives the trace of the car speed v obtained by fusing aIMU and vOF. Note that the KF reduces the noise of the vOF signal, a very important feature for keeping the derivative action in the velocity control law (22). reconstruction from vision (e.g., the \u201cstructure from motion\u201d problem) suffers from a scale problem, in the translation vec- tor estimate.47 This issue, due to the loss of information in mapping two-dimensionalto3Ddata,isalsopresentinoptical\ufb02owvelocityesti- mation methods. Here, this can lead to a scaled estimate of the car velocity.Forthisreason, - tion in the estimation process: the acceleration provided by the IMU."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the KF reduces the noise of the vOF signal, a very important feature for keeping the derivative action in the velocity control law (22).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76e92a0f-0517-454d-8b2e-12a3207250be": {"__data__": {"id_": "76e92a0f-0517-454d-8b2e-12a3207250be", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The center plot shows the car speed measured with the optical \ufb02ow-based method (vOF), whereas the bottom plot gives the trace of the car speed v obtained by fusing aIMU and vOF. Note that the KF reduces the noise of the vOF signal, a very important feature for keeping the derivative action in the velocity control law (22). reconstruction from vision (e.g., the \u201cstructure from motion\u201d problem) suffers from a scale problem, in the translation vec- tor estimate.47 This issue, due to the loss of information in mapping two-dimensionalto3Ddata,isalsopresentinoptical\ufb02owvelocityesti- mation methods. Here, this can lead to a scaled estimate of the car velocity.Forthisreason, - tion in the estimation process: the acceleration provided by the IMU. Note, however, that in the current state of the work, the velocity esti- mate accuracy has been only evaluated qualitatively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "reconstruction from vision (e.g., the \u201cstructure from motion\u201d problem) suffers from a scale problem, in the translation vec- tor estimate.47 This issue, due to the loss of information in mapping two-dimensionalto3Ddata,isalsopresentinoptical\ufb02owvelocityesti- mation methods.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4de356d-14e3-457e-b57c-a235293d106a": {"__data__": {"id_": "c4de356d-14e3-457e-b57c-a235293d106a", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note that the KF reduces the noise of the vOF signal, a very important feature for keeping the derivative action in the velocity control law (22). reconstruction from vision (e.g., the \u201cstructure from motion\u201d problem) suffers from a scale problem, in the translation vec- tor estimate.47 This issue, due to the loss of information in mapping two-dimensionalto3Ddata,isalsopresentinoptical\ufb02owvelocityesti- mation methods. Here, this can lead to a scaled estimate of the car velocity.Forthisreason, - tion in the estimation process: the acceleration provided by the IMU. Note, however, that in the current state of the work, the velocity esti- mate accuracy has been only evaluated qualitatively. In fact, that high accuracy is only important in the transient phases (initial error recov- ery and curve negotiation)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, this can lead to a scaled estimate of the car velocity.Forthisreason, - tion in the estimation process: the acceleration provided by the IMU.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f955a54-2b47-41e3-9c29-96e1404e413a": {"__data__": {"id_": "0f955a54-2b47-41e3-9c29-96e1404e413a", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "reconstruction from vision (e.g., the \u201cstructure from motion\u201d problem) suffers from a scale problem, in the translation vec- tor estimate.47 This issue, due to the loss of information in mapping two-dimensionalto3Ddata,isalsopresentinoptical\ufb02owvelocityesti- mation methods. Here, this can lead to a scaled estimate of the car velocity.Forthisreason, - tion in the estimation process: the acceleration provided by the IMU. Note, however, that in the current state of the work, the velocity esti- mate accuracy has been only evaluated qualitatively. In fact, that high accuracy is only important in the transient phases (initial error recov- ery and curve negotiation). Instead, it can be easily shown that the perturbation induced by velocity estimate inaccuracy on the features driving task, and that by limiting the uncertainty on the velocity value, it is possible to preserve local stability."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note, however, that in the current state of the work, the velocity esti- mate accuracy has been only evaluated qualitatively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a0070d5-44ff-4da9-88e0-57c3f852ebbb": {"__data__": {"id_": "2a0070d5-44ff-4da9-88e0-57c3f852ebbb", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Here, this can lead to a scaled estimate of the car velocity.Forthisreason, - tion in the estimation process: the acceleration provided by the IMU. Note, however, that in the current state of the work, the velocity esti- mate accuracy has been only evaluated qualitatively. In fact, that high accuracy is only important in the transient phases (initial error recov- ery and curve negotiation). Instead, it can be easily shown that the perturbation induced by velocity estimate inaccuracy on the features driving task, and that by limiting the uncertainty on the velocity value, it is possible to preserve local stability. In fact, the driving performance showed that the estimation was accurate enough, for the considered scenario."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, that high accuracy is only important in the transient phases (initial error recov- ery and curve negotiation).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2687904-e25f-4264-9896-8eb4e04c7e09": {"__data__": {"id_": "b2687904-e25f-4264-9896-8eb4e04c7e09", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note, however, that in the current state of the work, the velocity esti- mate accuracy has been only evaluated qualitatively. In fact, that high accuracy is only important in the transient phases (initial error recov- ery and curve negotiation). Instead, it can be easily shown that the perturbation induced by velocity estimate inaccuracy on the features driving task, and that by limiting the uncertainty on the velocity value, it is possible to preserve local stability. In fact, the driving performance showed that the estimation was accurate enough, for the considered scenario. In different conditions, \ufb01ner tuning of the velocity estimator may be necessary."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead, it can be easily shown that the perturbation induced by velocity estimate inaccuracy on the features driving task, and that by limiting the uncertainty on the velocity value, it is possible to preserve local stability.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e286df0d-9b0c-45cc-a3d6-8587c244a9c1": {"__data__": {"id_": "e286df0d-9b0c-45cc-a3d6-8587c244a9c1", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In fact, that high accuracy is only important in the transient phases (initial error recov- ery and curve negotiation). Instead, it can be easily shown that the perturbation induced by velocity estimate inaccuracy on the features driving task, and that by limiting the uncertainty on the velocity value, it is possible to preserve local stability. In fact, the driving performance showed that the estimation was accurate enough, for the considered scenario. In different conditions, \ufb01ner tuning of the velocity estimator may be necessary. Plots related to the steering wheel control are shown in Figure 12(a)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, the driving performance showed that the estimation was accurate enough, for the considered scenario.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4bc6fee-a95d-4dbd-ae62-b643d63b590a": {"__data__": {"id_": "b4bc6fee-a95d-4dbd-ae62-b643d63b590a", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Instead, it can be easily shown that the perturbation induced by velocity estimate inaccuracy on the features driving task, and that by limiting the uncertainty on the velocity value, it is possible to preserve local stability. In fact, the driving performance showed that the estimation was accurate enough, for the considered scenario. In different conditions, \ufb01ner tuning of the velocity estimator may be necessary. Plots related to the steering wheel control are shown in Figure 12(a). The steering control is activated about 8 s after the start of the experiment and, after a transient time of a few seconds, it leads the car to the road center."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In different conditions, \ufb01ner tuning of the velocity estimator may be necessary.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79ea31ab-c6a9-4bf4-b032-7799953c5198": {"__data__": {"id_": "79ea31ab-c6a9-4bf4-b032-7799953c5198", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In fact, the driving performance showed that the estimation was accurate enough, for the considered scenario. In different conditions, \ufb01ner tuning of the velocity estimator may be necessary. Plots related to the steering wheel control are shown in Figure 12(a). The steering control is activated about 8 s after the start of the experiment and, after a transient time of a few seconds, it leads the car to the road center. Thus, the middle and vanishing points (the top and center plots, respectively) correctly converge to the desired values, that is, xm goes to k4 = 30 pixels (since \ud835\udefe = 0.2145 rad; see expression of k4 in Section 5.1) and xv to 0."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Plots related to the steering wheel control are shown in Figure 12(a).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d97a7ab6-2671-4af6-9073-761bfff34fe0": {"__data__": {"id_": "d97a7ab6-2671-4af6-9073-761bfff34fe0", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In different conditions, \ufb01ner tuning of the velocity estimator may be necessary. Plots related to the steering wheel control are shown in Figure 12(a). The steering control is activated about 8 s after the start of the experiment and, after a transient time of a few seconds, it leads the car to the road center. Thus, the middle and vanishing points (the top and center plots, respectively) correctly converge to the desired values, that is, xm goes to k4 = 30 pixels (since \ud835\udefe = 0.2145 rad; see expression of k4 in Section 5.1) and xv to 0. The bottom plot shows the trend of the desired steering command \ud835\udefc, as computed from the visual features and from the estimated car speed according to (21)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The steering control is activated about 8 s after the start of the experiment and, after a transient time of a few seconds, it leads the car to the road center.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ceb016dd-6799-4b65-984f-be29a622babc": {"__data__": {"id_": "ceb016dd-6799-4b65-984f-be29a622babc", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Plots related to the steering wheel control are shown in Figure 12(a). The steering control is activated about 8 s after the start of the experiment and, after a transient time of a few seconds, it leads the car to the road center. Thus, the middle and vanishing points (the top and center plots, respectively) correctly converge to the desired values, that is, xm goes to k4 = 30 pixels (since \ud835\udefe = 0.2145 rad; see expression of k4 in Section 5.1) and xv to 0. The bottom plot shows the trend of the desired steering command \ud835\udefc, as computed from the visual features and from the estimated car speed according to (21). reconstructed from the encoders (black dashed line), shows that the steering command is smoothed by the task-based QP control, avoiding undesirable fast signal variations."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, the middle and vanishing points (the top and center plots, respectively) correctly converge to the desired values, that is, xm goes to k4 = 30 pixels (since \ud835\udefe = 0.2145 rad; see expression of k4 in Section 5.1) and xv to 0.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6850cb54-0f3d-4073-b184-ef2aabfe0f79": {"__data__": {"id_": "6850cb54-0f3d-4073-b184-ef2aabfe0f79", "embedding": null, "metadata": {"page_number": 12, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The steering control is activated about 8 s after the start of the experiment and, after a transient time of a few seconds, it leads the car to the road center. Thus, the middle and vanishing points (the top and center plots, respectively) correctly converge to the desired values, that is, xm goes to k4 = 30 pixels (since \ud835\udefe = 0.2145 rad; see expression of k4 in Section 5.1) and xv to 0. The bottom plot shows the trend of the desired steering command \ud835\udefc, as computed from the visual features and from the estimated car speed according to (21). reconstructed from the encoders (black dashed line), shows that the steering command is smoothed by the task-based QP control, avoiding undesirable fast signal variations. the car speed con- verges to the nominal desired values (no ground truth was avail-"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The bottom plot shows the trend of the desired steering command \ud835\udefc, as computed from the visual features and from the estimated car speed according to (21).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72733cd3-72d5-4452-880e-b613a76f269c": {"__data__": {"id_": "72733cd3-72d5-4452-880e-b613a76f269c", "embedding": null, "metadata": {"page_number": 13, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Thus, the middle and vanishing points (the top and center plots, respectively) correctly converge to the desired values, that is, xm goes to k4 = 30 pixels (since \ud835\udefe = 0.2145 rad; see expression of k4 in Section 5.1) and xv to 0. The bottom plot shows the trend of the desired steering command \ud835\udefc, as computed from the visual features and from the estimated car speed according to (21). reconstructed from the encoders (black dashed line), shows that the steering command is smoothed by the task-based QP control, avoiding undesirable fast signal variations. the car speed con- verges to the nominal desired values (no ground truth was avail- The oscillations observable at steady state are due to the fact that the resolution of the ankle joint is coarser than that of the gas pedal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "reconstructed from the encoders (black dashed line), shows that the steering command is smoothed by the task-based QP control, avoiding undesirable fast signal variations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8cf47e23-5175-49c7-bab8-547667d8ed67": {"__data__": {"id_": "8cf47e23-5175-49c7-bab8-547667d8ed67", "embedding": null, "metadata": {"page_number": 13, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The bottom plot shows the trend of the desired steering command \ud835\udefc, as computed from the visual features and from the estimated car speed according to (21). reconstructed from the encoders (black dashed line), shows that the steering command is smoothed by the task-based QP control, avoiding undesirable fast signal variations. the car speed con- verges to the nominal desired values (no ground truth was avail- The oscillations observable at steady state are due to the fact that the resolution of the ankle joint is coarser than that of the gas pedal. Note, in fact, that even if the robot ankle moves in a small range, the car speed changes signi\ufb01cantly."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the car speed con- verges to the nominal desired values (no ground truth was avail-", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa4fea19-6a41-460e-93d2-4d3843f595d0": {"__data__": {"id_": "fa4fea19-6a41-460e-93d2-4d3843f595d0", "embedding": null, "metadata": {"page_number": 13, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "reconstructed from the encoders (black dashed line), shows that the steering command is smoothed by the task-based QP control, avoiding undesirable fast signal variations. the car speed con- verges to the nominal desired values (no ground truth was avail- The oscillations observable at steady state are due to the fact that the resolution of the ankle joint is coarser than that of the gas pedal. Note, in fact, that even if the robot ankle moves in a small range, the car speed changes signi\ufb01cantly. The noise on the ankle command, as well as the initial peak, are due to the derivative term of the gas pedal control (22)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The oscillations observable at steady state are due to the fact that the resolution of the ankle joint is coarser than that of the gas pedal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51cf25f2-a8a5-4c6d-aa4a-4645955e5a91": {"__data__": {"id_": "51cf25f2-a8a5-4c6d-aa4a-4645955e5a91", "embedding": null, "metadata": {"page_number": 13, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the car speed con- verges to the nominal desired values (no ground truth was avail- The oscillations observable at steady state are due to the fact that the resolution of the ankle joint is coarser than that of the gas pedal. Note, in fact, that even if the robot ankle moves in a small range, the car speed changes signi\ufb01cantly. The noise on the ankle command, as well as the initial peak, are due to the derivative term of the gas pedal control (22). However, the signal is smoothed by the task-based QP control (see the dashed black line, i.e., the signal reconstructed by encoder readings), preventing jerky motion of the robot foot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note, in fact, that even if the robot ankle moves in a small range, the car speed changes signi\ufb01cantly.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82512e86-d83c-423c-96fe-f45e8eb17969": {"__data__": {"id_": "82512e86-d83c-423c-96fe-f45e8eb17969", "embedding": null, "metadata": {"page_number": 13, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The oscillations observable at steady state are due to the fact that the resolution of the ankle joint is coarser than that of the gas pedal. Note, in fact, that even if the robot ankle moves in a small range, the car speed changes signi\ufb01cantly. The noise on the ankle command, as well as the initial peak, are due to the derivative term of the gas pedal control (22). However, the signal is smoothed by the task-based QP control (see the dashed black line, i.e., the signal reconstructed by encoder readings), preventing jerky motion of the robot foot. In nine of them (including the one presented just above), the robot successfully drove the car for the entire path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The noise on the ankle command, as well as the initial peak, are due to the derivative term of the gas pedal control (22).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba0ddf5e-1ccc-4c5a-a37b-c6dd84b9ff82": {"__data__": {"id_": "ba0ddf5e-1ccc-4c5a-a37b-c6dd84b9ff82", "embedding": null, "metadata": {"page_number": 13, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note, in fact, that even if the robot ankle moves in a small range, the car speed changes signi\ufb01cantly. The noise on the ankle command, as well as the initial peak, are due to the derivative term of the gas pedal control (22). However, the signal is smoothed by the task-based QP control (see the dashed black line, i.e., the signal reconstructed by encoder readings), preventing jerky motion of the robot foot. In nine of them (including the one presented just above), the robot successfully drove the car for the entire path. One of the experiments failed due to a critical failure of the image processing."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the signal is smoothed by the task-based QP control (see the dashed black line, i.e., the signal reconstructed by encoder readings), preventing jerky motion of the robot foot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2a780b2-67c8-473a-b434-5578a5fbbdb4": {"__data__": {"id_": "b2a780b2-67c8-473a-b434-5578a5fbbdb4", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The noise on the ankle command, as well as the initial peak, are due to the derivative term of the gas pedal control (22). However, the signal is smoothed by the task-based QP control (see the dashed black line, i.e., the signal reconstructed by encoder readings), preventing jerky motion of the robot foot. In nine of them (including the one presented just above), the robot successfully drove the car for the entire path. One of the experiments failed due to a critical failure of the image processing. It was not possible to perform experi- ments on other tracks (with different road shapes and environmental conditions), because our application was rejected after complex administrative paperwork required to access other roads in the campus."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In nine of them (including the one presented just above), the robot successfully drove the car for the entire path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2fc74fc-f684-4ace-831c-bbc1e1ea3bde": {"__data__": {"id_": "e2fc74fc-f684-4ace-831c-bbc1e1ea3bde", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "However, the signal is smoothed by the task-based QP control (see the dashed black line, i.e., the signal reconstructed by encoder readings), preventing jerky motion of the robot foot. In nine of them (including the one presented just above), the robot successfully drove the car for the entire path. One of the experiments failed due to a critical failure of the image processing. It was not possible to perform experi- ments on other tracks (with different road shapes and environmental conditions), because our application was rejected after complex administrative paperwork required to access other roads in the campus. teleoperated mode (nonshadowed areas of the plots), the visual fea- tures are not considered and the steering command is sent to the robot via keyboard or joystick by the user."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One of the experiments failed due to a critical failure of the image processing.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "647c33f9-296d-4f01-b5d6-0faf922a68b2": {"__data__": {"id_": "647c33f9-296d-4f01-b5d6-0faf922a68b2", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In nine of them (including the one presented just above), the robot successfully drove the car for the entire path. One of the experiments failed due to a critical failure of the image processing. It was not possible to perform experi- ments on other tracks (with different road shapes and environmental conditions), because our application was rejected after complex administrative paperwork required to access other roads in the campus. teleoperated mode (nonshadowed areas of the plots), the visual fea- tures are not considered and the steering command is sent to the robot via keyboard or joystick by the user. Between 75 and 100 s, the user controlled the robot (in teleoperated mode) to make it steer on the right as much as possible."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It was not possible to perform experi- ments on other tracks (with different road shapes and environmental conditions), because our application was rejected after complex administrative paperwork required to access other roads in the campus.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7674dbc-3c9f-490a-86ed-d48fae4c4e6c": {"__data__": {"id_": "d7674dbc-3c9f-490a-86ed-d48fae4c4e6c", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "One of the experiments failed due to a critical failure of the image processing. It was not possible to perform experi- ments on other tracks (with different road shapes and environmental conditions), because our application was rejected after complex administrative paperwork required to access other roads in the campus. teleoperated mode (nonshadowed areas of the plots), the visual fea- tures are not considered and the steering command is sent to the robot via keyboard or joystick by the user. Between 75 and 100 s, the user controlled the robot (in teleoperated mode) to make it steer on the right as much as possible. Because of the kinematic limits and of the grasping con\ufb01guration, the robot saturated the steering angle at about \u20132 rad even if the user asked a wider steering."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "teleoperated mode (nonshadowed areas of the plots), the visual fea- tures are not considered and the steering command is sent to the robot via keyboard or joystick by the user.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9e30a0d-7c49-4289-95d9-99030ba34fe0": {"__data__": {"id_": "a9e30a0d-7c49-4289-95d9-99030ba34fe0", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "It was not possible to perform experi- ments on other tracks (with different road shapes and environmental conditions), because our application was rejected after complex administrative paperwork required to access other roads in the campus. teleoperated mode (nonshadowed areas of the plots), the visual fea- tures are not considered and the steering command is sent to the robot via keyboard or joystick by the user. Between 75 and 100 s, the user controlled the robot (in teleoperated mode) to make it steer on the right as much as possible. Because of the kinematic limits and of the grasping con\ufb01guration, the robot saturated the steering angle at about \u20132 rad even if the user asked a wider steering. This is evident on the plot of the steering angle command of Figure 14(a) (bottom): note the difference between the command (blue continuous curve) and the steering angle reconstructed from the encoders (black dashed curve)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Between 75 and 100 s, the user controlled the robot (in teleoperated mode) to make it steer on the right as much as possible.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "096020da-fc4f-40a4-bae0-9d5b6477bb48": {"__data__": {"id_": "096020da-fc4f-40a4-bae0-9d5b6477bb48", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "teleoperated mode (nonshadowed areas of the plots), the visual fea- tures are not considered and the steering command is sent to the robot via keyboard or joystick by the user. Between 75 and 100 s, the user controlled the robot (in teleoperated mode) to make it steer on the right as much as possible. Because of the kinematic limits and of the grasping con\ufb01guration, the robot saturated the steering angle at about \u20132 rad even if the user asked a wider steering. This is evident on the plot of the steering angle command of Figure 14(a) (bottom): note the difference between the command (blue continuous curve) and the steering angle reconstructed from the encoders (black dashed curve). 7.2 Second experiment: switching between teleoperated and autonomous modes"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because of the kinematic limits and of the grasping con\ufb01guration, the robot saturated the steering angle at about \u20132 rad even if the user asked a wider steering.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37426114-fb64-4769-90bd-1b286ab23183": {"__data__": {"id_": "37426114-fb64-4769-90bd-1b286ab23183", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Between 75 and 100 s, the user controlled the robot (in teleoperated mode) to make it steer on the right as much as possible. Because of the kinematic limits and of the grasping con\ufb01guration, the robot saturated the steering angle at about \u20132 rad even if the user asked a wider steering. This is evident on the plot of the steering angle command of Figure 14(a) (bottom): note the difference between the command (blue continuous curve) and the steering angle reconstructed from the encoders (black dashed curve). 7.2 Second experiment: switching between teleoperated and autonomous modes it is important to allow a user to supervise the driving operation and control the car if required."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is evident on the plot of the steering angle command of Figure 14(a) (bottom): note the difference between the command (blue continuous curve) and the steering angle reconstructed from the encoders (black dashed curve).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "794aece4-9924-42f2-bf63-d966d7fdb76c": {"__data__": {"id_": "794aece4-9924-42f2-bf63-d966d7fdb76c", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Because of the kinematic limits and of the grasping con\ufb01guration, the robot saturated the steering angle at about \u20132 rad even if the user asked a wider steering. This is evident on the plot of the steering angle command of Figure 14(a) (bottom): note the difference between the command (blue continuous curve) and the steering angle reconstructed from the encoders (black dashed curve). 7.2 Second experiment: switching between teleoperated and autonomous modes it is important to allow a user to supervise the driving operation and control the car if required. As described in Section 2, our framework allows a human user to inter- vene at any time, during the driving operation, to select a particular driving strategy."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.2 Second experiment: switching between teleoperated and autonomous modes", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c44ed328-e42a-4ec3-b291-8867685eac7e": {"__data__": {"id_": "c44ed328-e42a-4ec3-b291-8867685eac7e", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This is evident on the plot of the steering angle command of Figure 14(a) (bottom): note the difference between the command (blue continuous curve) and the steering angle reconstructed from the encoders (black dashed curve). 7.2 Second experiment: switching between teleoperated and autonomous modes it is important to allow a user to supervise the driving operation and control the car if required. As described in Section 2, our framework allows a human user to inter- vene at any time, during the driving operation, to select a particular driving strategy. The second experiment shows the switching between .Inparticular,insomephases of the experiment, the human takes control of the robot, by selecting the teleoperated mode."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it is important to allow a user to supervise the driving operation and control the car if required.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20cf7e9b-ef05-41a2-a576-67d82427c8d5": {"__data__": {"id_": "20cf7e9b-ef05-41a2-a576-67d82427c8d5", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "7.2 Second experiment: switching between teleoperated and autonomous modes it is important to allow a user to supervise the driving operation and control the car if required. As described in Section 2, our framework allows a human user to inter- vene at any time, during the driving operation, to select a particular driving strategy. The second experiment shows the switching between .Inparticular,insomephases of the experiment, the human takes control of the robot, by selecting the teleoperated mode. In these phases, proper commands are sent to therobot, ,connectingtwo straight roads traveled in autonomous mode."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As described in Section 2, our framework allows a human user to inter- vene at any time, during the driving operation, to select a particular driving strategy.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1f5c754-8a61-4cb0-b780-9f7d53e6e03e": {"__data__": {"id_": "d1f5c754-8a61-4cb0-b780-9f7d53e6e03e", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "it is important to allow a user to supervise the driving operation and control the car if required. As described in Section 2, our framework allows a human user to inter- vene at any time, during the driving operation, to select a particular driving strategy. The second experiment shows the switching between .Inparticular,insomephases of the experiment, the human takes control of the robot, by selecting the teleoperated mode. In these phases, proper commands are sent to therobot, ,connectingtwo straight roads traveled in autonomous mode. Snapshots of this second experiment are shown in Figure 13."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second experiment shows the switching between .Inparticular,insomephases of the experiment, the human takes control of the robot, by selecting the teleoperated mode.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37d0ee96-5989-4d5f-852b-f5cd31d80c50": {"__data__": {"id_": "37d0ee96-5989-4d5f-852b-f5cd31d80c50", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "As described in Section 2, our framework allows a human user to inter- vene at any time, during the driving operation, to select a particular driving strategy. The second experiment shows the switching between .Inparticular,insomephases of the experiment, the human takes control of the robot, by selecting the teleoperated mode. In these phases, proper commands are sent to therobot, ,connectingtwo straight roads traveled in autonomous mode. Snapshots of this second experiment are shown in Figure 13. we set v\u2217 = 1.5 m/s, while after the initial cali- bration of the gas pedal, qa,min = \u22120.5 rad and qa,max = \u22120.43 rad."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In these phases, proper commands are sent to therobot, ,connectingtwo straight roads traveled in autonomous mode.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0239874-dc57-401a-a5cf-7d43084a9682": {"__data__": {"id_": "c0239874-dc57-401a-a5cf-7d43084a9682", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The second experiment shows the switching between .Inparticular,insomephases of the experiment, the human takes control of the robot, by selecting the teleoperated mode. In these phases, proper commands are sent to therobot, ,connectingtwo straight roads traveled in autonomous mode. Snapshots of this second experiment are shown in Figure 13. we set v\u2217 = 1.5 m/s, while after the initial cali- bration of the gas pedal, qa,min = \u22120.5 rad and qa,max = \u22120.43 rad. Note that the difference in the admissible ankle range with respect to the previous experiment is due to a slightly different position of the robot foot on the gas pedal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Snapshots of this second experiment are shown in Figure 13.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "190b1579-9f63-43b6-b503-43aea6b81946": {"__data__": {"id_": "190b1579-9f63-43b6-b503-43aea6b81946", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In these phases, proper commands are sent to therobot, ,connectingtwo straight roads traveled in autonomous mode. Snapshots of this second experiment are shown in Figure 13. we set v\u2217 = 1.5 m/s, while after the initial cali- bration of the gas pedal, qa,min = \u22120.5 rad and qa,max = \u22120.43 rad. Note that the difference in the admissible ankle range with respect to the previous experiment is due to a slightly different position of the robot foot on the gas pedal. When the gas pedal control is enabled, the desired car speed is properly tracked by operating the robot ankle joint [shadowed areas of the top plot in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we set v\u2217 = 1.5 m/s, while after the initial cali- bration of the gas pedal, qa,min = \u22120.5 rad and qa,max = \u22120.43 rad.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d509f709-546e-43d6-beaf-5b3fa41db220": {"__data__": {"id_": "d509f709-546e-43d6-beaf-5b3fa41db220", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Snapshots of this second experiment are shown in Figure 13. we set v\u2217 = 1.5 m/s, while after the initial cali- bration of the gas pedal, qa,min = \u22120.5 rad and qa,max = \u22120.43 rad. Note that the difference in the admissible ankle range with respect to the previous experiment is due to a slightly different position of the robot foot on the gas pedal. When the gas pedal control is enabled, the desired car speed is properly tracked by operating the robot ankle joint [shadowed areas of the top plot in Fig. On the other hand, when the control is disabled (nonshadowed areas of the plots), the ankle command [blue curve in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the difference in the admissible ankle range with respect to the previous experiment is due to a slightly different position of the robot foot on the gas pedal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d25b3644-e682-4350-b141-6a1b75315211": {"__data__": {"id_": "d25b3644-e682-4350-b141-6a1b75315211", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we set v\u2217 = 1.5 m/s, while after the initial cali- bration of the gas pedal, qa,min = \u22120.5 rad and qa,max = \u22120.43 rad. Note that the difference in the admissible ankle range with respect to the previous experiment is due to a slightly different position of the robot foot on the gas pedal. When the gas pedal control is enabled, the desired car speed is properly tracked by operating the robot ankle joint [shadowed areas of the top plot in Fig. On the other hand, when the control is disabled (nonshadowed areas of the plots), the ankle command [blue curve in Fig. 14(b), bottom], as computed by (25), is not considered, and the robot ankle is tele- operated with the keyboard/joystick interface, as noticeable from the encoder plot (black dashed curve)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When the gas pedal control is enabled, the desired car speed is properly tracked by operating the robot ankle joint [shadowed areas of the top plot in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0318ba3b-80c6-4a6c-bec0-b76619502727": {"__data__": {"id_": "0318ba3b-80c6-4a6c-bec0-b76619502727", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note that the difference in the admissible ankle range with respect to the previous experiment is due to a slightly different position of the robot foot on the gas pedal. When the gas pedal control is enabled, the desired car speed is properly tracked by operating the robot ankle joint [shadowed areas of the top plot in Fig. On the other hand, when the control is disabled (nonshadowed areas of the plots), the ankle command [blue curve in Fig. 14(b), bottom], as computed by (25), is not considered, and the robot ankle is tele- operated with the keyboard/joystick interface, as noticeable from the encoder plot (black dashed curve). the robot could perform the entire experiment (along a path of 130 m ca."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, when the control is disabled (nonshadowed areas of the plots), the ankle command [blue curve in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a765570b-784f-48da-94dc-0ec4acfd8912": {"__data__": {"id_": "a765570b-784f-48da-94dc-0ec4acfd8912", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "When the gas pedal control is enabled, the desired car speed is properly tracked by operating the robot ankle joint [shadowed areas of the top plot in Fig. On the other hand, when the control is disabled (nonshadowed areas of the plots), the ankle command [blue curve in Fig. 14(b), bottom], as computed by (25), is not considered, and the robot ankle is tele- operated with the keyboard/joystick interface, as noticeable from the encoder plot (black dashed curve). the robot could perform the entire experiment (along a path of 130 m ca. for more than 160 s) without the need to stop the car."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14(b), bottom], as computed by (25), is not considered, and the robot ankle is tele- operated with the keyboard/joystick interface, as noticeable from the encoder plot (black dashed curve).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd6d482d-16a3-4257-b881-f56e90763856": {"__data__": {"id_": "dd6d482d-16a3-4257-b881-f56e90763856", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "On the other hand, when the control is disabled (nonshadowed areas of the plots), the ankle command [blue curve in Fig. 14(b), bottom], as computed by (25), is not considered, and the robot ankle is tele- operated with the keyboard/joystick interface, as noticeable from the encoder plot (black dashed curve). the robot could perform the entire experiment (along a path of 130 m ca. for more than 160 s) without the need to stop the car. This was achieved thanks to two main design choices."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot could perform the entire experiment (along a path of 130 m ca.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e115ada-9c54-439e-bfe3-b2e1b1e307ca": {"__data__": {"id_": "7e115ada-9c54-439e-bfe3-b2e1b1e307ca", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "14(b), bottom], as computed by (25), is not considered, and the robot ankle is tele- operated with the keyboard/joystick interface, as noticeable from the encoder plot (black dashed curve). the robot could perform the entire experiment (along a path of 130 m ca. for more than 160 s) without the need to stop the car. This was achieved thanks to two main design choices. First, from a perception viewpoint, monocular camera and IMU data are light to be processed, allowing a fast and reactive behavior."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for more than 160 s) without the need to stop the car.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32a65d81-984b-49a5-8acd-84e3ee54e9de": {"__data__": {"id_": "32a65d81-984b-49a5-8acd-84e3ee54e9de", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the robot could perform the entire experiment (along a path of 130 m ca. for more than 160 s) without the need to stop the car. This was achieved thanks to two main design choices. First, from a perception viewpoint, monocular camera and IMU data are light to be processed, allowing a fast and reactive behavior. Second, the control framework at all the stages (from the higher level visual control to the low-level kinematic control) guarantees smooth signals, even at the switching moments."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This was achieved thanks to two main design choices.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78b73576-715c-4a2e-9850-39ab2b6d8f0c": {"__data__": {"id_": "78b73576-715c-4a2e-9850-39ab2b6d8f0c", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "for more than 160 s) without the need to stop the car. This was achieved thanks to two main design choices. First, from a perception viewpoint, monocular camera and IMU data are light to be processed, allowing a fast and reactive behavior. Second, the control framework at all the stages (from the higher level visual control to the low-level kinematic control) guarantees smooth signals, even at the switching moments. In particular, one can observe that when the control is enabled (shadowed areas of the plots) there is the same correct behavior of the system seen in the \ufb01rst experiment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, from a perception viewpoint, monocular camera and IMU data are light to be processed, allowing a fast and reactive behavior.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11c9839f-70d5-4ebe-b64a-9b95ae790d84": {"__data__": {"id_": "11c9839f-70d5-4ebe-b64a-9b95ae790d84", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This was achieved thanks to two main design choices. First, from a perception viewpoint, monocular camera and IMU data are light to be processed, allowing a fast and reactive behavior. Second, the control framework at all the stages (from the higher level visual control to the low-level kinematic control) guarantees smooth signals, even at the switching moments. In particular, one can observe that when the control is enabled (shadowed areas of the plots) there is the same correct behavior of the system seen in the \ufb01rst experiment. The same experiment presented just above was performed \ufb01ve other times, during the same day."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, the control framework at all the stages (from the higher level visual control to the low-level kinematic control) guarantees smooth signals, even at the switching moments.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "835f285f-037f-4740-965b-23b60dd9c1c4": {"__data__": {"id_": "835f285f-037f-4740-965b-23b60dd9c1c4", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "First, from a perception viewpoint, monocular camera and IMU data are light to be processed, allowing a fast and reactive behavior. Second, the control framework at all the stages (from the higher level visual control to the low-level kinematic control) guarantees smooth signals, even at the switching moments. In particular, one can observe that when the control is enabled (shadowed areas of the plots) there is the same correct behavior of the system seen in the \ufb01rst experiment. The same experiment presented just above was performed \ufb01ve other times, during the same day. Four experiments resulted success- ful, whereas two failed due to human errors during teleoperation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, one can observe that when the control is enabled (shadowed areas of the plots) there is the same correct behavior of the system seen in the \ufb01rst experiment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e210df4-e1f9-49d8-917b-18b29cb2db56": {"__data__": {"id_": "7e210df4-e1f9-49d8-917b-18b29cb2db56", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Second, the control framework at all the stages (from the higher level visual control to the low-level kinematic control) guarantees smooth signals, even at the switching moments. In particular, one can observe that when the control is enabled (shadowed areas of the plots) there is the same correct behavior of the system seen in the \ufb01rst experiment. The same experiment presented just above was performed \ufb01ve other times, during the same day. Four experiments resulted success- ful, whereas two failed due to human errors during teleoperation. 7.3 Third experiment: shared-autonomy driving at the DRC \ufb01nals"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same experiment presented just above was performed \ufb01ve other times, during the same day.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67f01724-d1ef-448e-9c09-89050724321b": {"__data__": {"id_": "67f01724-d1ef-448e-9c09-89050724321b", "embedding": null, "metadata": {"page_number": 14, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In particular, one can observe that when the control is enabled (shadowed areas of the plots) there is the same correct behavior of the system seen in the \ufb01rst experiment. The same experiment presented just above was performed \ufb01ve other times, during the same day. Four experiments resulted success- ful, whereas two failed due to human errors during teleoperation. 7.3 Third experiment: shared-autonomy driving at the DRC \ufb01nals There are situations in which the application of the autonomous mode is too risky (e.g., because of bad quality images) and the full teleopera- tion mode is too stressful for the operator who must operate continu- ously along all the experiment a joystick or a keyboard."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Four experiments resulted success- ful, whereas two failed due to human errors during teleoperation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30a8d2ee-6bf1-4f5c-9775-1c7988795c59": {"__data__": {"id_": "30a8d2ee-6bf1-4f5c-9775-1c7988795c59", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The same experiment presented just above was performed \ufb01ve other times, during the same day. Four experiments resulted success- ful, whereas two failed due to human errors during teleoperation. 7.3 Third experiment: shared-autonomy driving at the DRC \ufb01nals There are situations in which the application of the autonomous mode is too risky (e.g., because of bad quality images) and the full teleopera- tion mode is too stressful for the operator who must operate continu- ously along all the experiment a joystick or a keyboard. In these cases, it is convenient to provide the user with the shared-autonomy mode described in Section 2 to help with the steering operations."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.3 Third experiment: shared-autonomy driving at the DRC \ufb01nals", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34b8e486-53cb-4297-a38f-652b39b70f41": {"__data__": {"id_": "34b8e486-53cb-4297-a38f-652b39b70f41", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Four experiments resulted success- ful, whereas two failed due to human errors during teleoperation. 7.3 Third experiment: shared-autonomy driving at the DRC \ufb01nals There are situations in which the application of the autonomous mode is too risky (e.g., because of bad quality images) and the full teleopera- tion mode is too stressful for the operator who must operate continu- ously along all the experiment a joystick or a keyboard. In these cases, it is convenient to provide the user with the shared-autonomy mode described in Section 2 to help with the steering operations. This experiment shows the effectiveness of such shared-autonomy mode."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are situations in which the application of the autonomous mode is too risky (e.g., because of bad quality images) and the full teleopera- tion mode is too stressful for the operator who must operate continu- ously along all the experiment a joystick or a keyboard.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0f6f059-cb4f-40d4-acd7-b8e498938666": {"__data__": {"id_": "d0f6f059-cb4f-40d4-acd7-b8e498938666", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "7.3 Third experiment: shared-autonomy driving at the DRC \ufb01nals There are situations in which the application of the autonomous mode is too risky (e.g., because of bad quality images) and the full teleopera- tion mode is too stressful for the operator who must operate continu- ously along all the experiment a joystick or a keyboard. In these cases, it is convenient to provide the user with the shared-autonomy mode described in Section 2 to help with the steering operations. This experiment shows the effectiveness of such shared-autonomy mode. This strategy was very useful to make the robot drive at the DRC \ufb01nals."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In these cases, it is convenient to provide the user with the shared-autonomy mode described in Section 2 to help with the steering operations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bfcaf9a-9d99-4943-b1d3-6f7f79de10d7": {"__data__": {"id_": "3bfcaf9a-9d99-4943-b1d3-6f7f79de10d7", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "There are situations in which the application of the autonomous mode is too risky (e.g., because of bad quality images) and the full teleopera- tion mode is too stressful for the operator who must operate continu- ously along all the experiment a joystick or a keyboard. In these cases, it is convenient to provide the user with the shared-autonomy mode described in Section 2 to help with the steering operations. This experiment shows the effectiveness of such shared-autonomy mode. This strategy was very useful to make the robot drive at the DRC \ufb01nals. The challenge presented a lot of uncertainties since the"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This experiment shows the effectiveness of such shared-autonomy mode.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "207cbd31-efc9-4ad4-a64f-3e1ec209d6c7": {"__data__": {"id_": "207cbd31-efc9-4ad4-a64f-3e1ec209d6c7", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In these cases, it is convenient to provide the user with the shared-autonomy mode described in Section 2 to help with the steering operations. This experiment shows the effectiveness of such shared-autonomy mode. This strategy was very useful to make the robot drive at the DRC \ufb01nals. The challenge presented a lot of uncertainties since the we used shared-autonomy that allows de\ufb01ning high target controller."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This strategy was very useful to make the robot drive at the DRC \ufb01nals.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d0633a1-973e-4192-8dd3-c5cee737cb6b": {"__data__": {"id_": "4d0633a1-973e-4192-8dd3-c5cee737cb6b", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This experiment shows the effectiveness of such shared-autonomy mode. This strategy was very useful to make the robot drive at the DRC \ufb01nals. The challenge presented a lot of uncertainties since the we used shared-autonomy that allows de\ufb01ning high target controller. This frees the user from having to continuously teleoper- ate the robot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The challenge presented a lot of uncertainties since the", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76c8b93a-a8aa-4eab-b7de-63332b877649": {"__data__": {"id_": "76c8b93a-a8aa-4eab-b7de-63332b877649", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This strategy was very useful to make the robot drive at the DRC \ufb01nals. The challenge presented a lot of uncertainties since the we used shared-autonomy that allows de\ufb01ning high target controller. This frees the user from having to continuously teleoper- ate the robot. We successfully completed the task on both DRC days (in 1:28 and 1:32 minutes) by using the shared-autonomy mode."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we used shared-autonomy that allows de\ufb01ning high target controller.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c47351b-a548-4736-8caa-a477ab4f8dc0": {"__data__": {"id_": "3c47351b-a548-4736-8caa-a477ab4f8dc0", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The challenge presented a lot of uncertainties since the we used shared-autonomy that allows de\ufb01ning high target controller. This frees the user from having to continuously teleoper- ate the robot. We successfully completed the task on both DRC days (in 1:28 and 1:32 minutes) by using the shared-autonomy mode. Snap- shots taken from the DRC \ufb01nals of\ufb01cial video51 are shown in Figure 15 ( ).Thehumanusertele- operatedHRP-2Kairemotely, camera as the only feedback from the challenge \ufb01eld."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This frees the user from having to continuously teleoper- ate the robot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "343bbbe2-b8d6-42ff-9bb7-0318d02bd4f7": {"__data__": {"id_": "343bbbe2-b8d6-42ff-9bb7-0318d02bd4f7", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we used shared-autonomy that allows de\ufb01ning high target controller. This frees the user from having to continuously teleoper- ate the robot. We successfully completed the task on both DRC days (in 1:28 and 1:32 minutes) by using the shared-autonomy mode. Snap- shots taken from the DRC \ufb01nals of\ufb01cial video51 are shown in Figure 15 ( ).Thehumanusertele- operatedHRP-2Kairemotely, camera as the only feedback from the challenge \ufb01eld. the proper arti\ufb01cial road borders (red lines in the \ufb01gure) to steer the car along the path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We successfully completed the task on both DRC days (in 1:28 and 1:32 minutes) by using the shared-autonomy mode.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac2c1ab9-44c0-47bf-8577-491d44cdc39e": {"__data__": {"id_": "ac2c1ab9-44c0-47bf-8577-491d44cdc39e", "embedding": null, "metadata": {"page_number": 15, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This frees the user from having to continuously teleoper- ate the robot. We successfully completed the task on both DRC days (in 1:28 and 1:32 minutes) by using the shared-autonomy mode. Snap- shots taken from the DRC \ufb01nals of\ufb01cial video51 are shown in Figure 15 ( ).Thehumanusertele- operatedHRP-2Kairemotely, camera as the only feedback from the challenge \ufb01eld. the proper arti\ufb01cial road borders (red lines in the \ufb01gure) to steer the car along the path. Note that these arti\ufb01cial road borders, manually set by the user, may not correspond to the real borders of the road."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Snap- shots taken from the DRC \ufb01nals of\ufb01cial video51 are shown in Figure 15 ( ).Thehumanusertele- operatedHRP-2Kairemotely, camera as the only feedback from the challenge \ufb01eld.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8698582d-8c1f-45af-9797-04872b28ded7": {"__data__": {"id_": "8698582d-8c1f-45af-9797-04872b28ded7", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We successfully completed the task on both DRC days (in 1:28 and 1:32 minutes) by using the shared-autonomy mode. Snap- shots taken from the DRC \ufb01nals of\ufb01cial video51 are shown in Figure 15 ( ).Thehumanusertele- operatedHRP-2Kairemotely, camera as the only feedback from the challenge \ufb01eld. the proper arti\ufb01cial road borders (red lines in the \ufb01gure) to steer the car along the path. Note that these arti\ufb01cial road borders, manually set by the user, may not correspond to the real borders of the road. In fact, they just represent geometri- calreferences\u2014moreintuitiveforhumans\u2014toeasilyde\ufb01nethevanish- ing and middle points and steer the car using (21)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the proper arti\ufb01cial road borders (red lines in the \ufb01gure) to steer the car along the path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23700668-70dd-4b5d-a4c3-1f16a863e212": {"__data__": {"id_": "23700668-70dd-4b5d-a4c3-1f16a863e212", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Snap- shots taken from the DRC \ufb01nals of\ufb01cial video51 are shown in Figure 15 ( ).Thehumanusertele- operatedHRP-2Kairemotely, camera as the only feedback from the challenge \ufb01eld. the proper arti\ufb01cial road borders (red lines in the \ufb01gure) to steer the car along the path. Note that these arti\ufb01cial road borders, manually set by the user, may not correspond to the real borders of the road. In fact, they just represent geometri- calreferences\u2014moreintuitiveforhumans\u2014toeasilyde\ufb01nethevanish- ing and middle points and steer the car using (21). Concurrently, the .Inother words, with reference to the block diagram of Figure 1, the user pro- - ence to the pedal operation block."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that these arti\ufb01cial road borders, manually set by the user, may not correspond to the real borders of the road.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7b2a662-9a86-44b2-94ea-d3e7ae9c650c": {"__data__": {"id_": "e7b2a662-9a86-44b2-94ea-d3e7ae9c650c", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "the proper arti\ufb01cial road borders (red lines in the \ufb01gure) to steer the car along the path. Note that these arti\ufb01cial road borders, manually set by the user, may not correspond to the real borders of the road. In fact, they just represent geometri- calreferences\u2014moreintuitiveforhumans\u2014toeasilyde\ufb01nethevanish- ing and middle points and steer the car using (21). Concurrently, the .Inother words, with reference to the block diagram of Figure 1, the user pro- - ence to the pedal operation block. Basically, s/he takes the place of the road detection and car velocity estimation/control blocks."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, they just represent geometri- calreferences\u2014moreintuitiveforhumans\u2014toeasilyde\ufb01nethevanish- ing and middle points and steer the car using (21).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3b9f378-7cdb-44ae-9118-b36a0696a1da": {"__data__": {"id_": "f3b9f378-7cdb-44ae-9118-b36a0696a1da", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Note that these arti\ufb01cial road borders, manually set by the user, may not correspond to the real borders of the road. In fact, they just represent geometri- calreferences\u2014moreintuitiveforhumans\u2014toeasilyde\ufb01nethevanish- ing and middle points and steer the car using (21). Concurrently, the .Inother words, with reference to the block diagram of Figure 1, the user pro- - ence to the pedal operation block. Basically, s/he takes the place of the road detection and car velocity estimation/control blocks. The shared- autonomy mode can be seen as a sort of shared control between the robot and the a human supervisor and allows the human to interfere with the robot operation if required."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Concurrently, the .Inother words, with reference to the block diagram of Figure 1, the user pro- - ence to the pedal operation block.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df4bccf5-c030-4e74-9fd2-96973157d186": {"__data__": {"id_": "df4bccf5-c030-4e74-9fd2-96973157d186", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "In fact, they just represent geometri- calreferences\u2014moreintuitiveforhumans\u2014toeasilyde\ufb01nethevanish- ing and middle points and steer the car using (21). Concurrently, the .Inother words, with reference to the block diagram of Figure 1, the user pro- - ence to the pedal operation block. Basically, s/he takes the place of the road detection and car velocity estimation/control blocks. The shared- autonomy mode can be seen as a sort of shared control between the robot and the a human supervisor and allows the human to interfere with the robot operation if required. Only one person is required to operate in shared-autonomy or teleoperation modes."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Basically, s/he takes the place of the road detection and car velocity estimation/control blocks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "194b9cf6-6db2-4640-a962-a18d5c0e92b4": {"__data__": {"id_": "194b9cf6-6db2-4640-a962-a18d5c0e92b4", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Concurrently, the .Inother words, with reference to the block diagram of Figure 1, the user pro- - ence to the pedal operation block. Basically, s/he takes the place of the road detection and car velocity estimation/control blocks. The shared- autonomy mode can be seen as a sort of shared control between the robot and the a human supervisor and allows the human to interfere with the robot operation if required. Only one person is required to operate in shared-autonomy or teleoperation modes. Furthermore, in the DRC experiment, the user had to intervene only 15 times along all .This results in a more comfortable steering experience (with respect to the teleoperationmode)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The shared- autonomy mode can be seen as a sort of shared control between the robot and the a human supervisor and allows the human to interfere with the robot operation if required.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89dbce7d-8794-4deb-b404-4875bbab8a03": {"__data__": {"id_": "89dbce7d-8794-4deb-b404-4875bbab8a03", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Basically, s/he takes the place of the road detection and car velocity estimation/control blocks. The shared- autonomy mode can be seen as a sort of shared control between the robot and the a human supervisor and allows the human to interfere with the robot operation if required. Only one person is required to operate in shared-autonomy or teleoperation modes. Furthermore, in the DRC experiment, the user had to intervene only 15 times along all .This results in a more comfortable steering experience (with respect to the teleoperationmode). ,atanytime,dur- ing the execution of the driving experience, the user can instantly and smoothly switch to one of the other two driving modes."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Only one person is required to operate in shared-autonomy or teleoperation modes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cbc34bfc-e78b-4045-a8a3-2da30cc53cc9": {"__data__": {"id_": "cbc34bfc-e78b-4045-a8a3-2da30cc53cc9", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The shared- autonomy mode can be seen as a sort of shared control between the robot and the a human supervisor and allows the human to interfere with the robot operation if required. Only one person is required to operate in shared-autonomy or teleoperation modes. Furthermore, in the DRC experiment, the user had to intervene only 15 times along all .This results in a more comfortable steering experience (with respect to the teleoperationmode). ,atanytime,dur- ing the execution of the driving experience, the user can instantly and smoothly switch to one of the other two driving modes. we have proposed a reactive control architecture for car driving by a humanoid robot on unknown roads."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, in the DRC experiment, the user had to intervene only 15 times along all .This results in a more comfortable steering experience (with respect to the teleoperationmode).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94cd962f-738e-4fce-b29d-0eecfe11904c": {"__data__": {"id_": "94cd962f-738e-4fce-b29d-0eecfe11904c", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Only one person is required to operate in shared-autonomy or teleoperation modes. Furthermore, in the DRC experiment, the user had to intervene only 15 times along all .This results in a more comfortable steering experience (with respect to the teleoperationmode). ,atanytime,dur- ing the execution of the driving experience, the user can instantly and smoothly switch to one of the other two driving modes. we have proposed a reactive control architecture for car driving by a humanoid robot on unknown roads. is controlled by estimating the car speed using visual and inertial data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ",atanytime,dur- ing the execution of the driving experience, the user can instantly and smoothly switch to one of the other two driving modes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "948e37ac-67b2-4281-8ed0-eecf860a3480": {"__data__": {"id_": "948e37ac-67b2-4281-8ed0-eecf860a3480", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Furthermore, in the DRC experiment, the user had to intervene only 15 times along all .This results in a more comfortable steering experience (with respect to the teleoperationmode). ,atanytime,dur- ing the execution of the driving experience, the user can instantly and smoothly switch to one of the other two driving modes. we have proposed a reactive control architecture for car driving by a humanoid robot on unknown roads. is controlled by estimating the car speed using visual and inertial data. Three different driving modes (autonomous, shared-autonomy, and teleoperated) extend the versa- tility of our framework."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we have proposed a reactive control architecture for car driving by a humanoid robot on unknown roads.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "981d7e86-b614-4576-9136-c3a7ac444e6e": {"__data__": {"id_": "981d7e86-b614-4576-9136-c3a7ac444e6e", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": ",atanytime,dur- ing the execution of the driving experience, the user can instantly and smoothly switch to one of the other two driving modes. we have proposed a reactive control architecture for car driving by a humanoid robot on unknown roads. is controlled by estimating the car speed using visual and inertial data. Three different driving modes (autonomous, shared-autonomy, and teleoperated) extend the versa- tility of our framework. The experimental results carried out with the humanoid robot HRP-2Kai have shown the effectiveness of the pro- posed approach."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "is controlled by estimating the car speed using visual and inertial data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20557f7e-7de3-4e05-aa98-cc590ac7849e": {"__data__": {"id_": "20557f7e-7de3-4e05-aa98-cc590ac7849e", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "we have proposed a reactive control architecture for car driving by a humanoid robot on unknown roads. is controlled by estimating the car speed using visual and inertial data. Three different driving modes (autonomous, shared-autonomy, and teleoperated) extend the versa- tility of our framework. The experimental results carried out with the humanoid robot HRP-2Kai have shown the effectiveness of the pro- posed approach. The shared-autonomy mode was successfully used to complete the driving task at the DRC \ufb01nals."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Three different driving modes (autonomous, shared-autonomy, and teleoperated) extend the versa- tility of our framework.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07acf763-22fe-4698-9353-c8343da7c08b": {"__data__": {"id_": "07acf763-22fe-4698-9353-c8343da7c08b", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "is controlled by estimating the car speed using visual and inertial data. Three different driving modes (autonomous, shared-autonomy, and teleoperated) extend the versa- tility of our framework. The experimental results carried out with the humanoid robot HRP-2Kai have shown the effectiveness of the pro- posed approach. The shared-autonomy mode was successfully used to complete the driving task at the DRC \ufb01nals. limited the range of application of our methods."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experimental results carried out with the humanoid robot HRP-2Kai have shown the effectiveness of the pro- posed approach.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df02f0d6-303a-467e-8873-709d585c5a35": {"__data__": {"id_": "df02f0d6-303a-467e-8873-709d585c5a35", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Three different driving modes (autonomous, shared-autonomy, and teleoperated) extend the versa- tility of our framework. The experimental results carried out with the humanoid robot HRP-2Kai have shown the effectiveness of the pro- posed approach. The shared-autonomy mode was successfully used to complete the driving task at the DRC \ufb01nals. limited the range of application of our methods. Future work will be done to make the autonomous mode work ef\ufb01ciently in the presence of sharp curves."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The shared-autonomy mode was successfully used to complete the driving task at the DRC \ufb01nals.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8278dca8-920b-4722-8907-921d82fb1f36": {"__data__": {"id_": "8278dca8-920b-4722-8907-921d82fb1f36", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The experimental results carried out with the humanoid robot HRP-2Kai have shown the effectiveness of the pro- posed approach. The shared-autonomy mode was successfully used to complete the driving task at the DRC \ufb01nals. limited the range of application of our methods. Future work will be done to make the autonomous mode work ef\ufb01ciently in the presence of sharp curves. To this end and to overcome the problem of limited steering motions, we plan to include, in the framework, the planning of variable grasping con\ufb01gu- rations to achieve more complex maneuvers."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "limited the range of application of our methods.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfa2610b-e202-4511-bc05-ee03c184a5c4": {"__data__": {"id_": "bfa2610b-e202-4511-bc05-ee03c184a5c4", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "The shared-autonomy mode was successfully used to complete the driving task at the DRC \ufb01nals. limited the range of application of our methods. Future work will be done to make the autonomous mode work ef\ufb01ciently in the presence of sharp curves. To this end and to overcome the problem of limited steering motions, we plan to include, in the framework, the planning of variable grasping con\ufb01gu- rations to achieve more complex maneuvers. We are also planning to go to driving on uneven terrains, where the robot has also to sustain its attitude, with respect to sharp changes of the car orientation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Future work will be done to make the autonomous mode work ef\ufb01ciently in the presence of sharp curves.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a366d406-d83c-446b-aaf8-d88d01516925": {"__data__": {"id_": "a366d406-d83c-446b-aaf8-d88d01516925", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "limited the range of application of our methods. Future work will be done to make the autonomous mode work ef\ufb01ciently in the presence of sharp curves. To this end and to overcome the problem of limited steering motions, we plan to include, in the framework, the planning of variable grasping con\ufb01gu- rations to achieve more complex maneuvers. We are also planning to go to driving on uneven terrains, where the robot has also to sustain its attitude, with respect to sharp changes of the car orientation. based on optical \ufb02ow, will improve the driving safety."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To this end and to overcome the problem of limited steering motions, we plan to include, in the framework, the planning of variable grasping con\ufb01gu- rations to achieve more complex maneuvers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "993ee037-9883-4cda-8406-5866a66cacf5": {"__data__": {"id_": "993ee037-9883-4cda-8406-5866a66cacf5", "embedding": null, "metadata": {"page_number": 16, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Future work will be done to make the autonomous mode work ef\ufb01ciently in the presence of sharp curves. To this end and to overcome the problem of limited steering motions, we plan to include, in the framework, the planning of variable grasping con\ufb01gu- rations to achieve more complex maneuvers. We are also planning to go to driving on uneven terrains, where the robot has also to sustain its attitude, with respect to sharp changes of the car orientation. based on optical \ufb02ow, will improve the driving safety. Finally, we plan to add brake control and to perform the entire driving task, including car ingress and egress."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We are also planning to go to driving on uneven terrains, where the robot has also to sustain its attitude, with respect to sharp changes of the car orientation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df84b4b6-0024-4524-94df-e87ea5def2f4": {"__data__": {"id_": "df84b4b6-0024-4524-94df-e87ea5def2f4", "embedding": null, "metadata": {"page_number": 17, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "To this end and to overcome the problem of limited steering motions, we plan to include, in the framework, the planning of variable grasping con\ufb01gu- rations to achieve more complex maneuvers. We are also planning to go to driving on uneven terrains, where the robot has also to sustain its attitude, with respect to sharp changes of the car orientation. based on optical \ufb02ow, will improve the driving safety. Finally, we plan to add brake control and to perform the entire driving task, including car ingress and egress. This work is supported by the EU FP7 strep project KOROIBOT http://www.koroibot.eu, - ence (JSPS) Grant-in-Aid for Scienti\ufb01c Research (B) No."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "based on optical \ufb02ow, will improve the driving safety.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1af094a0-3b5c-4c54-89d5-6f9a954c0a28": {"__data__": {"id_": "1af094a0-3b5c-4c54-89d5-6f9a954c0a28", "embedding": null, "metadata": {"page_number": 17, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "We are also planning to go to driving on uneven terrains, where the robot has also to sustain its attitude, with respect to sharp changes of the car orientation. based on optical \ufb02ow, will improve the driving safety. Finally, we plan to add brake control and to perform the entire driving task, including car ingress and egress. This work is supported by the EU FP7 strep project KOROIBOT http://www.koroibot.eu, - ence (JSPS) Grant-in-Aid for Scienti\ufb01c Research (B) No. This work was also in part supported by the CNRS PICS Project ViNCI."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we plan to add brake control and to perform the entire driving task, including car ingress and egress.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb6a4f43-98db-4f66-9275-afe97f3d36b1": {"__data__": {"id_": "fb6a4f43-98db-4f66-9275-afe97f3d36b1", "embedding": null, "metadata": {"page_number": 17, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "based on optical \ufb02ow, will improve the driving safety. Finally, we plan to add brake control and to perform the entire driving task, including car ingress and egress. This work is supported by the EU FP7 strep project KOROIBOT http://www.koroibot.eu, - ence (JSPS) Grant-in-Aid for Scienti\ufb01c Research (B) No. This work was also in part supported by the CNRS PICS Project ViNCI. without which the experiments could not be conducted; Dr. Fumio Kanehiro for lending the car and promoting this research; Herv\u00e9 Audren and Arnaud Tanguy for their kind support during the experiments."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This work is supported by the EU FP7 strep project KOROIBOT http://www.koroibot.eu, - ence (JSPS) Grant-in-Aid for Scienti\ufb01c Research (B) No.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a5cd0aa-c8a4-4679-8cc6-f2c4db7a7a10": {"__data__": {"id_": "0a5cd0aa-c8a4-4679-8cc6-f2c4db7a7a10", "embedding": null, "metadata": {"page_number": 17, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "Finally, we plan to add brake control and to perform the entire driving task, including car ingress and egress. This work is supported by the EU FP7 strep project KOROIBOT http://www.koroibot.eu, - ence (JSPS) Grant-in-Aid for Scienti\ufb01c Research (B) No. This work was also in part supported by the CNRS PICS Project ViNCI. without which the experiments could not be conducted; Dr. Fumio Kanehiro for lending the car and promoting this research; Herv\u00e9 Audren and Arnaud Tanguy for their kind support during the experiments."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This work was also in part supported by the CNRS PICS Project ViNCI.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d816f1e-4736-4fc3-9a64-2cadd922ea34": {"__data__": {"id_": "6d816f1e-4736-4fc3-9a64-2cadd922ea34", "embedding": null, "metadata": {"page_number": 17, "source": "Journal of Field Robotics - 2017 - Paolillo - Autonomous car driving by a humanoid robot.pdf", "doi": "10.1002/rob.21731", "window": "This work is supported by the EU FP7 strep project KOROIBOT http://www.koroibot.eu, - ence (JSPS) Grant-in-Aid for Scienti\ufb01c Research (B) No. This work was also in part supported by the CNRS PICS Project ViNCI. without which the experiments could not be conducted; Dr. Fumio Kanehiro for lending the car and promoting this research; Herv\u00e9 Audren and Arnaud Tanguy for their kind support during the experiments."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "without which the experiments could not be conducted; Dr. Fumio Kanehiro for lending the car and promoting this research; Herv\u00e9 Audren and Arnaud Tanguy for their kind support during the experiments.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6720815-519c-46a9-9eb8-d57576617a97": {"__data__": {"id_": "c6720815-519c-46a9-9eb8-d57576617a97", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Perception and Control Strategies for Driving Utility Vehicles with a Humanoid Robot Christopher Rasmussen!, Kiwon Sohn 2 Qiaosong Wang!, and Paul Oh? Abstract\u2014 This paper describes the hardware and software components of a general-purpose humanoid robot system for autonomously driving several different types of utility vehicles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Perception and Control Strategies for Driving Utility Vehicles with a Humanoid Robot", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f8bb31c-dd0d-479e-8a6d-b2f3cc3f0ade": {"__data__": {"id_": "3f8bb31c-dd0d-479e-8a6d-b2f3cc3f0ade", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Perception and Control Strategies for Driving Utility Vehicles with a Humanoid Robot Christopher Rasmussen!, Kiwon Sohn 2 Qiaosong Wang!, and Paul Oh? Abstract\u2014 This paper describes the hardware and software components of a general-purpose humanoid robot system for autonomously driving several different types of utility vehicles. The robot recognizes which vehicle it is in, localizes itself with respect to the dashboard, and self-aligns in order to interface with the steering wheel and accelerator pedal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Christopher Rasmussen!, Kiwon Sohn 2 Qiaosong Wang!, and Paul Oh?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3aa03d24-2c7e-4870-96f0-d6131be653be": {"__data__": {"id_": "3aa03d24-2c7e-4870-96f0-d6131be653be", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Perception and Control Strategies for Driving Utility Vehicles with a Humanoid Robot Christopher Rasmussen!, Kiwon Sohn 2 Qiaosong Wang!, and Paul Oh? Abstract\u2014 This paper describes the hardware and software components of a general-purpose humanoid robot system for autonomously driving several different types of utility vehicles. The robot recognizes which vehicle it is in, localizes itself with respect to the dashboard, and self-aligns in order to interface with the steering wheel and accelerator pedal. Low- and higher- level methods are presented for speed control, environment perception, and trajectory planning and following suitable for operation in planar areas with discrete obstacles as well as along road-like paths."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Abstract\u2014 This paper describes the hardware and software components of a general-purpose humanoid robot system for autonomously driving several different types of utility vehicles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68365893-73a4-47c2-af43-9a72d1c9b479": {"__data__": {"id_": "68365893-73a4-47c2-af43-9a72d1c9b479", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Christopher Rasmussen!, Kiwon Sohn 2 Qiaosong Wang!, and Paul Oh? Abstract\u2014 This paper describes the hardware and software components of a general-purpose humanoid robot system for autonomously driving several different types of utility vehicles. The robot recognizes which vehicle it is in, localizes itself with respect to the dashboard, and self-aligns in order to interface with the steering wheel and accelerator pedal. Low- and higher- level methods are presented for speed control, environment perception, and trajectory planning and following suitable for operation in planar areas with discrete obstacles as well as along road-like paths. As part of the recently concluded DARPA Robotics Chal- lenge (DRC) trials , contestant robots needed to carry out a number of navigation and manipulation tasks."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot recognizes which vehicle it is in, localizes itself with respect to the dashboard, and self-aligns in order to interface with the steering wheel and accelerator pedal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fed331cc-a5f7-42a2-8df0-6f21e35c3b28": {"__data__": {"id_": "fed331cc-a5f7-42a2-8df0-6f21e35c3b28", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Abstract\u2014 This paper describes the hardware and software components of a general-purpose humanoid robot system for autonomously driving several different types of utility vehicles. The robot recognizes which vehicle it is in, localizes itself with respect to the dashboard, and self-aligns in order to interface with the steering wheel and accelerator pedal. Low- and higher- level methods are presented for speed control, environment perception, and trajectory planning and following suitable for operation in planar areas with discrete obstacles as well as along road-like paths. As part of the recently concluded DARPA Robotics Chal- lenge (DRC) trials , contestant robots needed to carry out a number of navigation and manipulation tasks. These tasks were meant to represent a set of skills sufficient for a robot to move from the edge of a disaster zone such as a damaged nuclear power plant to its interior, where it could assess and possibly repair critical systems."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Low- and higher- level methods are presented for speed control, environment perception, and trajectory planning and following suitable for operation in planar areas with discrete obstacles as well as along road-like paths.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f77f31e2-3e86-4ae9-9d19-3e0934090a5e": {"__data__": {"id_": "f77f31e2-3e86-4ae9-9d19-3e0934090a5e", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The robot recognizes which vehicle it is in, localizes itself with respect to the dashboard, and self-aligns in order to interface with the steering wheel and accelerator pedal. Low- and higher- level methods are presented for speed control, environment perception, and trajectory planning and following suitable for operation in planar areas with discrete obstacles as well as along road-like paths. As part of the recently concluded DARPA Robotics Chal- lenge (DRC) trials , contestant robots needed to carry out a number of navigation and manipulation tasks. These tasks were meant to represent a set of skills sufficient for a robot to move from the edge of a disaster zone such as a damaged nuclear power plant to its interior, where it could assess and possibly repair critical systems. The Vehicle stage of the challenge called for the robot to drive a golf-cart- like utility vehicle around obstacles to a target location, get out, and walk away (aka egress)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As part of the recently concluded DARPA Robotics Chal- lenge (DRC) trials , contestant robots needed to carry out a number of navigation and manipulation tasks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a3d0aad-170e-4a1c-b64e-a9adbaab2b03": {"__data__": {"id_": "1a3d0aad-170e-4a1c-b64e-a9adbaab2b03", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Low- and higher- level methods are presented for speed control, environment perception, and trajectory planning and following suitable for operation in planar areas with discrete obstacles as well as along road-like paths. As part of the recently concluded DARPA Robotics Chal- lenge (DRC) trials , contestant robots needed to carry out a number of navigation and manipulation tasks. These tasks were meant to represent a set of skills sufficient for a robot to move from the edge of a disaster zone such as a damaged nuclear power plant to its interior, where it could assess and possibly repair critical systems. The Vehicle stage of the challenge called for the robot to drive a golf-cart- like utility vehicle around obstacles to a target location, get out, and walk away (aka egress). In this paper we present techniques for autonomously driv- ing several different utility vehicles using a humanoid robot (the DRC-Hubo robot and specific vehicles are described in Sec."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These tasks were meant to represent a set of skills sufficient for a robot to move from the edge of a disaster zone such as a damaged nuclear power plant to its interior, where it could assess and possibly repair critical systems.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3134533b-e744-470b-a885-edaad3796eb5": {"__data__": {"id_": "3134533b-e744-470b-a885-edaad3796eb5", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "As part of the recently concluded DARPA Robotics Chal- lenge (DRC) trials , contestant robots needed to carry out a number of navigation and manipulation tasks. These tasks were meant to represent a set of skills sufficient for a robot to move from the edge of a disaster zone such as a damaged nuclear power plant to its interior, where it could assess and possibly repair critical systems. The Vehicle stage of the challenge called for the robot to drive a golf-cart- like utility vehicle around obstacles to a target location, get out, and walk away (aka egress). In this paper we present techniques for autonomously driv- ing several different utility vehicles using a humanoid robot (the DRC-Hubo robot and specific vehicles are described in Sec. These methods were developed using a robot entered in the 2013 DARPA DRC trials, but this is not a description of our approach to the Vehicle task there."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Vehicle stage of the challenge called for the robot to drive a golf-cart- like utility vehicle around obstacles to a target location, get out, and walk away (aka egress).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0abcac9-682a-4167-9896-b406be59cd76": {"__data__": {"id_": "a0abcac9-682a-4167-9896-b406be59cd76", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "These tasks were meant to represent a set of skills sufficient for a robot to move from the edge of a disaster zone such as a damaged nuclear power plant to its interior, where it could assess and possibly repair critical systems. The Vehicle stage of the challenge called for the robot to drive a golf-cart- like utility vehicle around obstacles to a target location, get out, and walk away (aka egress). In this paper we present techniques for autonomously driv- ing several different utility vehicles using a humanoid robot (the DRC-Hubo robot and specific vehicles are described in Sec. These methods were developed using a robot entered in the 2013 DARPA DRC trials, but this is not a description of our approach to the Vehicle task there. At the competition, we used a pure tele-operation approach tuned specifically for the course and low-bandwidth conditions described in the rules ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this paper we present techniques for autonomously driv- ing several different utility vehicles using a humanoid robot (the DRC-Hubo robot and specific vehicles are described in Sec.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8dbd3f8-9c91-43f5-ad11-711ea18b490e": {"__data__": {"id_": "f8dbd3f8-9c91-43f5-ad11-711ea18b490e", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The Vehicle stage of the challenge called for the robot to drive a golf-cart- like utility vehicle around obstacles to a target location, get out, and walk away (aka egress). In this paper we present techniques for autonomously driv- ing several different utility vehicles using a humanoid robot (the DRC-Hubo robot and specific vehicles are described in Sec. These methods were developed using a robot entered in the 2013 DARPA DRC trials, but this is not a description of our approach to the Vehicle task there. At the competition, we used a pure tele-operation approach tuned specifically for the course and low-bandwidth conditions described in the rules . Here we describe a more general set of autonomous skills for driving such vehicles in a variety of static environments."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These methods were developed using a robot entered in the 2013 DARPA DRC trials, but this is not a description of our approach to the Vehicle task there.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3166d3b2-2e64-49a6-99a1-c66364fedb2e": {"__data__": {"id_": "3166d3b2-2e64-49a6-99a1-c66364fedb2e", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "In this paper we present techniques for autonomously driv- ing several different utility vehicles using a humanoid robot (the DRC-Hubo robot and specific vehicles are described in Sec. These methods were developed using a robot entered in the 2013 DARPA DRC trials, but this is not a description of our approach to the Vehicle task there. At the competition, we used a pure tele-operation approach tuned specifically for the course and low-bandwidth conditions described in the rules . Here we describe a more general set of autonomous skills for driving such vehicles in a variety of static environments. Autonomously-driven vehicles of course have a long his- tory , , , , with prominent milestones at the 2005 DARPA Grand Challenge (DGC) and 2007 DARPA Urban Challenge (DUC) , ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the competition, we used a pure tele-operation approach tuned specifically for the course and low-bandwidth conditions described in the rules .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a5dd044-c79e-4340-8805-149de4931cc6": {"__data__": {"id_": "2a5dd044-c79e-4340-8805-149de4931cc6", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "These methods were developed using a robot entered in the 2013 DARPA DRC trials, but this is not a description of our approach to the Vehicle task there. At the competition, we used a pure tele-operation approach tuned specifically for the course and low-bandwidth conditions described in the rules . Here we describe a more general set of autonomous skills for driving such vehicles in a variety of static environments. Autonomously-driven vehicles of course have a long his- tory , , , , with prominent milestones at the 2005 DARPA Grand Challenge (DGC) and 2007 DARPA Urban Challenge (DUC) , . Since the DUC, much progress in the field has come in the industrial sector as au- tomobile manufacturers and Google have extensively refined and tested driverless car technologies and in some cases begun to offer them as safety options on production vehicles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here we describe a more general set of autonomous skills for driving such vehicles in a variety of static environments.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2bcc3fb-509c-4974-b649-0318ea6f4f1c": {"__data__": {"id_": "c2bcc3fb-509c-4974-b649-0318ea6f4f1c", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "At the competition, we used a pure tele-operation approach tuned specifically for the course and low-bandwidth conditions described in the rules . Here we describe a more general set of autonomous skills for driving such vehicles in a variety of static environments. Autonomously-driven vehicles of course have a long his- tory , , , , with prominent milestones at the 2005 DARPA Grand Challenge (DGC) and 2007 DARPA Urban Challenge (DUC) , . Since the DUC, much progress in the field has come in the industrial sector as au- tomobile manufacturers and Google have extensively refined and tested driverless car technologies and in some cases begun to offer them as safety options on production vehicles. These vehicles are effectively robots, but there are a number of significant differences between them and humanoid robots with respect to the structure and difficulty of the driving task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Autonomously-driven vehicles of course have a long his- tory , , , , with prominent milestones at the 2005 DARPA Grand Challenge (DGC) and 2007 DARPA Urban Challenge (DUC) , .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dde99584-3064-400c-8441-a1107e95deaf": {"__data__": {"id_": "dde99584-3064-400c-8441-a1107e95deaf", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Here we describe a more general set of autonomous skills for driving such vehicles in a variety of static environments. Autonomously-driven vehicles of course have a long his- tory , , , , with prominent milestones at the 2005 DARPA Grand Challenge (DGC) and 2007 DARPA Urban Challenge (DUC) , . Since the DUC, much progress in the field has come in the industrial sector as au- tomobile manufacturers and Google have extensively refined and tested driverless car technologies and in some cases begun to offer them as safety options on production vehicles. These vehicles are effectively robots, but there are a number of significant differences between them and humanoid robots with respect to the structure and difficulty of the driving task. *This work was supported by DARPA award #N65236-12-1-1005 1Dept."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the DUC, much progress in the field has come in the industrial sector as au- tomobile manufacturers and Google have extensively refined and tested driverless car technologies and in some cases begun to offer them as safety options on production vehicles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "907af906-f5c6-49b7-94f7-587098942720": {"__data__": {"id_": "907af906-f5c6-49b7-94f7-587098942720", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Autonomously-driven vehicles of course have a long his- tory , , , , with prominent milestones at the 2005 DARPA Grand Challenge (DGC) and 2007 DARPA Urban Challenge (DUC) , . Since the DUC, much progress in the field has come in the industrial sector as au- tomobile manufacturers and Google have extensively refined and tested driverless car technologies and in some cases begun to offer them as safety options on production vehicles. These vehicles are effectively robots, but there are a number of significant differences between them and humanoid robots with respect to the structure and difficulty of the driving task. *This work was supported by DARPA award #N65236-12-1-1005 1Dept. Computer & Information Sciences, University of Delaware, USA."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These vehicles are effectively robots, but there are a number of significant differences between them and humanoid robots with respect to the structure and difficulty of the driving task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10d49a32-245a-4f73-8089-bd15fabbbf17": {"__data__": {"id_": "10d49a32-245a-4f73-8089-bd15fabbbf17", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Since the DUC, much progress in the field has come in the industrial sector as au- tomobile manufacturers and Google have extensively refined and tested driverless car technologies and in some cases begun to offer them as safety options on production vehicles. These vehicles are effectively robots, but there are a number of significant differences between them and humanoid robots with respect to the structure and difficulty of the driving task. *This work was supported by DARPA award #N65236-12-1-1005 1Dept. Computer & Information Sciences, University of Delaware, USA. stair/ladder climbing, and power tool and door handle ma- nipulation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*This work was supported by DARPA award #N65236-12-1-1005 1Dept.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "342dee3e-4641-4d14-a6b4-c5180a1267b4": {"__data__": {"id_": "342dee3e-4641-4d14-a6b4-c5180a1267b4", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "These vehicles are effectively robots, but there are a number of significant differences between them and humanoid robots with respect to the structure and difficulty of the driving task. *This work was supported by DARPA award #N65236-12-1-1005 1Dept. Computer & Information Sciences, University of Delaware, USA. stair/ladder climbing, and power tool and door handle ma- nipulation. achieving adequate sensor coverage (i.e., \u201cblind spot\u201d elimination) on integrated driverless car systems is generally just a matter of arraying enough fixed cam- era/sonar/ladar/radar units around the vehicle periphery and on its roof."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Computer & Information Sciences, University of Delaware, USA.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6917501-72f7-4fc2-a11c-0b435414ab5b": {"__data__": {"id_": "a6917501-72f7-4fc2-a11c-0b435414ab5b", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "*This work was supported by DARPA award #N65236-12-1-1005 1Dept. Computer & Information Sciences, University of Delaware, USA. stair/ladder climbing, and power tool and door handle ma- nipulation. achieving adequate sensor coverage (i.e., \u201cblind spot\u201d elimination) on integrated driverless car systems is generally just a matter of arraying enough fixed cam- era/sonar/ladar/radar units around the vehicle periphery and on its roof. Conversely, many parts of the road scene are inherently occluded from a humanoid robot inside a vehicle even with omnidirectional sensors on its head."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "stair/ladder climbing, and power tool and door handle ma- nipulation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2be34c1a-c24c-4c51-96a2-32682e75ace8": {"__data__": {"id_": "2be34c1a-c24c-4c51-96a2-32682e75ace8", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Computer & Information Sciences, University of Delaware, USA. stair/ladder climbing, and power tool and door handle ma- nipulation. achieving adequate sensor coverage (i.e., \u201cblind spot\u201d elimination) on integrated driverless car systems is generally just a matter of arraying enough fixed cam- era/sonar/ladar/radar units around the vehicle periphery and on its roof. Conversely, many parts of the road scene are inherently occluded from a humanoid robot inside a vehicle even with omnidirectional sensors on its head. And since the weight/space budget mentioned above makes fewer sen- sors more desirable and therefore less complete coverage"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "achieving adequate sensor coverage (i.e., \u201cblind spot\u201d elimination) on integrated driverless car systems is generally just a matter of arraying enough fixed cam- era/sonar/ladar/radar units around the vehicle periphery and on its roof.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10c4c332-2fc0-4ef6-b6e5-6e86468746c0": {"__data__": {"id_": "10c4c332-2fc0-4ef6-b6e5-6e86468746c0", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "stair/ladder climbing, and power tool and door handle ma- nipulation. achieving adequate sensor coverage (i.e., \u201cblind spot\u201d elimination) on integrated driverless car systems is generally just a matter of arraying enough fixed cam- era/sonar/ladar/radar units around the vehicle periphery and on its roof. Conversely, many parts of the road scene are inherently occluded from a humanoid robot inside a vehicle even with omnidirectional sensors on its head. And since the weight/space budget mentioned above makes fewer sen- sors more desirable and therefore less complete coverage Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conversely, many parts of the road scene are inherently occluded from a humanoid robot inside a vehicle even with omnidirectional sensors on its head.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88ea6a80-89b8-42fe-b2b9-87d0f92a6535": {"__data__": {"id_": "88ea6a80-89b8-42fe-b2b9-87d0f92a6535", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "achieving adequate sensor coverage (i.e., \u201cblind spot\u201d elimination) on integrated driverless car systems is generally just a matter of arraying enough fixed cam- era/sonar/ladar/radar units around the vehicle periphery and on its roof. Conversely, many parts of the road scene are inherently occluded from a humanoid robot inside a vehicle even with omnidirectional sensors on its head. And since the weight/space budget mentioned above makes fewer sen- sors more desirable and therefore less complete coverage Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the robot must carry out view planning to decide where to \u201clook\u201d depending on what it is doing from moment to moment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And since the weight/space budget mentioned above makes fewer sen- sors more desirable and therefore less complete coverage", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8be564f9-d6c6-4bec-8408-4c7246a0b61a": {"__data__": {"id_": "8be564f9-d6c6-4bec-8408-4c7246a0b61a", "embedding": null, "metadata": {"page_number": 1, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Conversely, many parts of the road scene are inherently occluded from a humanoid robot inside a vehicle even with omnidirectional sensors on its head. And since the weight/space budget mentioned above makes fewer sen- sors more desirable and therefore less complete coverage Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the robot must carry out view planning to decide where to \u201clook\u201d depending on what it is doing from moment to moment. making motion control (steering, ac- celeration/braking, and shifting) and vehicle state feedback (speed, steering angle, engine temperature, and so on) trivial to implement and basically error-proof."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa617d7e-fb23-44db-8244-291e878e6ab4": {"__data__": {"id_": "fa617d7e-fb23-44db-8244-291e878e6ab4", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "And since the weight/space budget mentioned above makes fewer sen- sors more desirable and therefore less complete coverage Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the robot must carry out view planning to decide where to \u201clook\u201d depending on what it is doing from moment to moment. making motion control (steering, ac- celeration/braking, and shifting) and vehicle state feedback (speed, steering angle, engine temperature, and so on) trivial to implement and basically error-proof. Although there are specialized machines for actuating steering, acceleration, braking, and gear shifting from the driver\u2019s seat with no permanent vehicle modifications , , , these take considerable time to set up and calibrate and have no other mobility, manipulation, or perception abilities."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot must carry out view planning to decide where to \u201clook\u201d depending on what it is doing from moment to moment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a7cec06-b46c-45f7-a71f-d49169ef346b": {"__data__": {"id_": "3a7cec06-b46c-45f7-a71f-d49169ef346b", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the robot must carry out view planning to decide where to \u201clook\u201d depending on what it is doing from moment to moment. making motion control (steering, ac- celeration/braking, and shifting) and vehicle state feedback (speed, steering angle, engine temperature, and so on) trivial to implement and basically error-proof. Although there are specialized machines for actuating steering, acceleration, braking, and gear shifting from the driver\u2019s seat with no permanent vehicle modifications , , , these take considerable time to set up and calibrate and have no other mobility, manipulation, or perception abilities. On the other hand, a humanoid robot manipulating the steering wheel and pedals is a mechanical system that must self-align, self- stabilize, and monitor for slips and other mishaps."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "making motion control (steering, ac- celeration/braking, and shifting) and vehicle state feedback (speed, steering angle, engine temperature, and so on) trivial to implement and basically error-proof.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a475c164-42c8-46d7-976a-053ddbe1c0d7": {"__data__": {"id_": "a475c164-42c8-46d7-976a-053ddbe1c0d7", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "the robot must carry out view planning to decide where to \u201clook\u201d depending on what it is doing from moment to moment. making motion control (steering, ac- celeration/braking, and shifting) and vehicle state feedback (speed, steering angle, engine temperature, and so on) trivial to implement and basically error-proof. Although there are specialized machines for actuating steering, acceleration, braking, and gear shifting from the driver\u2019s seat with no permanent vehicle modifications , , , these take considerable time to set up and calibrate and have no other mobility, manipulation, or perception abilities. On the other hand, a humanoid robot manipulating the steering wheel and pedals is a mechanical system that must self-align, self- stabilize, and monitor for slips and other mishaps. Further- more, vehicle state variables are not accessible by the robot through simple function calls; rather, they must either be visually read from the dashboard display or inferred from the robot\u2019s own sensors and transformed into the vehicle frame."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although there are specialized machines for actuating steering, acceleration, braking, and gear shifting from the driver\u2019s seat with no permanent vehicle modifications , , , these take considerable time to set up and calibrate and have no other mobility, manipulation, or perception abilities.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7638b625-7381-49b9-9cdb-18dd09efc0ab": {"__data__": {"id_": "7638b625-7381-49b9-9cdb-18dd09efc0ab", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "making motion control (steering, ac- celeration/braking, and shifting) and vehicle state feedback (speed, steering angle, engine temperature, and so on) trivial to implement and basically error-proof. Although there are specialized machines for actuating steering, acceleration, braking, and gear shifting from the driver\u2019s seat with no permanent vehicle modifications , , , these take considerable time to set up and calibrate and have no other mobility, manipulation, or perception abilities. On the other hand, a humanoid robot manipulating the steering wheel and pedals is a mechanical system that must self-align, self- stabilize, and monitor for slips and other mishaps. Further- more, vehicle state variables are not accessible by the robot through simple function calls; rather, they must either be visually read from the dashboard display or inferred from the robot\u2019s own sensors and transformed into the vehicle frame. The main contribution of this paper is a demonstration of the feasibility of a general-purpose humanoid robot driving an unmodified vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, a humanoid robot manipulating the steering wheel and pedals is a mechanical system that must self-align, self- stabilize, and monitor for slips and other mishaps.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2c459da-eec2-45a1-9e7a-26d670512ad9": {"__data__": {"id_": "c2c459da-eec2-45a1-9e7a-26d670512ad9", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Although there are specialized machines for actuating steering, acceleration, braking, and gear shifting from the driver\u2019s seat with no permanent vehicle modifications , , , these take considerable time to set up and calibrate and have no other mobility, manipulation, or perception abilities. On the other hand, a humanoid robot manipulating the steering wheel and pedals is a mechanical system that must self-align, self- stabilize, and monitor for slips and other mishaps. Further- more, vehicle state variables are not accessible by the robot through simple function calls; rather, they must either be visually read from the dashboard display or inferred from the robot\u2019s own sensors and transformed into the vehicle frame. The main contribution of this paper is a demonstration of the feasibility of a general-purpose humanoid robot driving an unmodified vehicle. The only previous work we can find on humanoid robot vehicle handling is , , in which an HRP-1 drove a modified forklift and a backhoe, but all control was via tele-operation using a video feed."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Further- more, vehicle state variables are not accessible by the robot through simple function calls; rather, they must either be visually read from the dashboard display or inferred from the robot\u2019s own sensors and transformed into the vehicle frame.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d7bd5c4-a0de-4fc3-adb5-92401e7b19ff": {"__data__": {"id_": "8d7bd5c4-a0de-4fc3-adb5-92401e7b19ff", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "On the other hand, a humanoid robot manipulating the steering wheel and pedals is a mechanical system that must self-align, self- stabilize, and monitor for slips and other mishaps. Further- more, vehicle state variables are not accessible by the robot through simple function calls; rather, they must either be visually read from the dashboard display or inferred from the robot\u2019s own sensors and transformed into the vehicle frame. The main contribution of this paper is a demonstration of the feasibility of a general-purpose humanoid robot driving an unmodified vehicle. The only previous work we can find on humanoid robot vehicle handling is , , in which an HRP-1 drove a modified forklift and a backhoe, but all control was via tele-operation using a video feed. Here we present a set of perceptual and physical methods which are sufficient to (1) interface the robot with different vehicles such that it can reliably accelerate, stop, and steer them as commanded; and (2) perform simple sensing and motion planning while driving given its limited and often occluded views."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main contribution of this paper is a demonstration of the feasibility of a general-purpose humanoid robot driving an unmodified vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e3ced3c-a7ee-4b99-96ef-af77f1dbffa1": {"__data__": {"id_": "8e3ced3c-a7ee-4b99-96ef-af77f1dbffa1", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Further- more, vehicle state variables are not accessible by the robot through simple function calls; rather, they must either be visually read from the dashboard display or inferred from the robot\u2019s own sensors and transformed into the vehicle frame. The main contribution of this paper is a demonstration of the feasibility of a general-purpose humanoid robot driving an unmodified vehicle. The only previous work we can find on humanoid robot vehicle handling is , , in which an HRP-1 drove a modified forklift and a backhoe, but all control was via tele-operation using a video feed. Here we present a set of perceptual and physical methods which are sufficient to (1) interface the robot with different vehicles such that it can reliably accelerate, stop, and steer them as commanded; and (2) perform simple sensing and motion planning while driving given its limited and often occluded views. wingspan of 2.04 m, weighs 60 kg, and has N = 33 degrees of freedom (DoF): 1 in the waist, 6 per leg, 7 per arm, 1 in the left hand fingers, 2 in the right hand fingers, and 3 in the neck/sensor head."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The only previous work we can find on humanoid robot vehicle handling is , , in which an HRP-1 drove a modified forklift and a backhoe, but all control was via tele-operation using a video feed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8528c256-20ff-41bb-922e-c7e9140c7f39": {"__data__": {"id_": "8528c256-20ff-41bb-922e-c7e9140c7f39", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The main contribution of this paper is a demonstration of the feasibility of a general-purpose humanoid robot driving an unmodified vehicle. The only previous work we can find on humanoid robot vehicle handling is , , in which an HRP-1 drove a modified forklift and a backhoe, but all control was via tele-operation using a video feed. Here we present a set of perceptual and physical methods which are sufficient to (1) interface the robot with different vehicles such that it can reliably accelerate, stop, and steer them as commanded; and (2) perform simple sensing and motion planning while driving given its limited and often occluded views. wingspan of 2.04 m, weighs 60 kg, and has N = 33 degrees of freedom (DoF): 1 in the waist, 6 per leg, 7 per arm, 1 in the left hand fingers, 2 in the right hand fingers, and 3 in the neck/sensor head. Three fingers on each hand close together via one motor for power grasps, and on the right hand there is an additional \u201ctrigger\u201d finger which moves independently."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here we present a set of perceptual and physical methods which are sufficient to (1) interface the robot with different vehicles such that it can reliably accelerate, stop, and steer them as commanded; and (2) perform simple sensing and motion planning while driving given its limited and often occluded views.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50cb3538-642f-4348-8472-fd50d271ce7d": {"__data__": {"id_": "50cb3538-642f-4348-8472-fd50d271ce7d", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The only previous work we can find on humanoid robot vehicle handling is , , in which an HRP-1 drove a modified forklift and a backhoe, but all control was via tele-operation using a video feed. Here we present a set of perceptual and physical methods which are sufficient to (1) interface the robot with different vehicles such that it can reliably accelerate, stop, and steer them as commanded; and (2) perform simple sensing and motion planning while driving given its limited and often occluded views. wingspan of 2.04 m, weighs 60 kg, and has N = 33 degrees of freedom (DoF): 1 in the waist, 6 per leg, 7 per arm, 1 in the left hand fingers, 2 in the right hand fingers, and 3 in the neck/sensor head. Three fingers on each hand close together via one motor for power grasps, and on the right hand there is an additional \u201ctrigger\u201d finger which moves independently. Each hand also has a peg opposite the palm/fingers side (short versions are shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "wingspan of 2.04 m, weighs 60 kg, and has N = 33 degrees of freedom (DoF): 1 in the waist, 6 per leg, 7 per arm, 1 in the left hand fingers, 2 in the right hand fingers, and 3 in the neck/sensor head.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fe58327-f295-4fe4-8d2a-cc73d8fae2f1": {"__data__": {"id_": "7fe58327-f295-4fe4-8d2a-cc73d8fae2f1", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Here we present a set of perceptual and physical methods which are sufficient to (1) interface the robot with different vehicles such that it can reliably accelerate, stop, and steer them as commanded; and (2) perform simple sensing and motion planning while driving given its limited and often occluded views. wingspan of 2.04 m, weighs 60 kg, and has N = 33 degrees of freedom (DoF): 1 in the waist, 6 per leg, 7 per arm, 1 in the left hand fingers, 2 in the right hand fingers, and 3 in the neck/sensor head. Three fingers on each hand close together via one motor for power grasps, and on the right hand there is an additional \u201ctrigger\u201d finger which moves independently. Each hand also has a peg opposite the palm/fingers side (short versions are shown in Fig. 5) which can be used as a point contact when the robot is in a quadrupedal walking mode."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Three fingers on each hand close together via one motor for power grasps, and on the right hand there is an additional \u201ctrigger\u201d finger which moves independently.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d082cfee-086c-4f44-9f06-54da45f7b972": {"__data__": {"id_": "d082cfee-086c-4f44-9f06-54da45f7b972", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "wingspan of 2.04 m, weighs 60 kg, and has N = 33 degrees of freedom (DoF): 1 in the waist, 6 per leg, 7 per arm, 1 in the left hand fingers, 2 in the right hand fingers, and 3 in the neck/sensor head. Three fingers on each hand close together via one motor for power grasps, and on the right hand there is an additional \u201ctrigger\u201d finger which moves independently. Each hand also has a peg opposite the palm/fingers side (short versions are shown in Fig. 5) which can be used as a point contact when the robot is in a quadrupedal walking mode. We have also found them useful as essentially rigid fingers for gross manipulation tasks such as turning the steering wheel, explained in Sec."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each hand also has a peg opposite the palm/fingers side (short versions are shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25bd4cb6-7097-4f25-abb2-add10d1989c8": {"__data__": {"id_": "25bd4cb6-7097-4f25-abb2-add10d1989c8", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Three fingers on each hand close together via one motor for power grasps, and on the right hand there is an additional \u201ctrigger\u201d finger which moves independently. Each hand also has a peg opposite the palm/fingers side (short versions are shown in Fig. 5) which can be used as a point contact when the robot is in a quadrupedal walking mode. We have also found them useful as essentially rigid fingers for gross manipulation tasks such as turning the steering wheel, explained in Sec. 2(a) and (b), was designed and built by us."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5) which can be used as a point contact when the robot is in a quadrupedal walking mode.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "517b1098-0f21-4ff1-b485-e1964a456608": {"__data__": {"id_": "517b1098-0f21-4ff1-b485-e1964a456608", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Each hand also has a peg opposite the palm/fingers side (short versions are shown in Fig. 5) which can be used as a point contact when the robot is in a quadrupedal walking mode. We have also found them useful as essentially rigid fingers for gross manipulation tasks such as turning the steering wheel, explained in Sec. 2(a) and (b), was designed and built by us. It pans +180\u00b0 and tilts +60\u00b0 without self-collision, and has the following sensors which are relevant to this work:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have also found them useful as essentially rigid fingers for gross manipulation tasks such as turning the steering wheel, explained in Sec.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57f88eb9-19b6-4860-a651-4917a74eda25": {"__data__": {"id_": "57f88eb9-19b6-4860-a651-4917a74eda25", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "5) which can be used as a point contact when the robot is in a quadrupedal walking mode. We have also found them useful as essentially rigid fingers for gross manipulation tasks such as turning the steering wheel, explained in Sec. 2(a) and (b), was designed and built by us. It pans +180\u00b0 and tilts +60\u00b0 without self-collision, and has the following sensors which are relevant to this work: Grey Flea3 cameras, each with about a 90\u00b0 x 70\u00b0 field of view (FOV), forming a synchronized stereo rig with baselines of 6 cm, 12 cm, and 18 cm."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2(a) and (b), was designed and built by us.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d072507-1f97-4778-b0cd-c866d4a89786": {"__data__": {"id_": "5d072507-1f97-4778-b0cd-c866d4a89786", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "We have also found them useful as essentially rigid fingers for gross manipulation tasks such as turning the steering wheel, explained in Sec. 2(a) and (b), was designed and built by us. It pans +180\u00b0 and tilts +60\u00b0 without self-collision, and has the following sensors which are relevant to this work: Grey Flea3 cameras, each with about a 90\u00b0 x 70\u00b0 field of view (FOV), forming a synchronized stereo rig with baselines of 6 cm, 12 cm, and 18 cm. Hokuyo UTM-30LX-EW laser range-finder which scans at 40 Hz over a 270\u00b0 FOV at an angular resolution of 0.25\u00b0."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It pans +180\u00b0 and tilts +60\u00b0 without self-collision, and has the following sensors which are relevant to this work:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8b07c68-10ea-4a7b-9f66-088f0da238a0": {"__data__": {"id_": "f8b07c68-10ea-4a7b-9f66-088f0da238a0", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "2(a) and (b), was designed and built by us. It pans +180\u00b0 and tilts +60\u00b0 without self-collision, and has the following sensors which are relevant to this work: Grey Flea3 cameras, each with about a 90\u00b0 x 70\u00b0 field of view (FOV), forming a synchronized stereo rig with baselines of 6 cm, 12 cm, and 18 cm. Hokuyo UTM-30LX-EW laser range-finder which scans at 40 Hz over a 270\u00b0 FOV at an angular resolution of 0.25\u00b0. The minimum detectable depth is 0.1 m and the maximum is 30 m, and intensity-like reflectance information is provided for each point."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Grey Flea3 cameras, each with about a 90\u00b0 x 70\u00b0 field of view (FOV), forming a synchronized stereo rig with baselines of 6 cm, 12 cm, and 18 cm.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6aac6622-fa0a-4638-b6fe-1d1ab1074411": {"__data__": {"id_": "6aac6622-fa0a-4638-b6fe-1d1ab1074411", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "It pans +180\u00b0 and tilts +60\u00b0 without self-collision, and has the following sensors which are relevant to this work: Grey Flea3 cameras, each with about a 90\u00b0 x 70\u00b0 field of view (FOV), forming a synchronized stereo rig with baselines of 6 cm, 12 cm, and 18 cm. Hokuyo UTM-30LX-EW laser range-finder which scans at 40 Hz over a 270\u00b0 FOV at an angular resolution of 0.25\u00b0. The minimum detectable depth is 0.1 m and the maximum is 30 m, and intensity-like reflectance information is provided for each point. The Hokuyo is mounted on a dedicated tilting servo which has a range of +60\u00b0 for point cloud capture"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hokuyo UTM-30LX-EW laser range-finder which scans at 40 Hz over a 270\u00b0 FOV at an angular resolution of 0.25\u00b0.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27e94a23-744a-4c84-baca-41087f7f3bc4": {"__data__": {"id_": "27e94a23-744a-4c84-baca-41087f7f3bc4", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Grey Flea3 cameras, each with about a 90\u00b0 x 70\u00b0 field of view (FOV), forming a synchronized stereo rig with baselines of 6 cm, 12 cm, and 18 cm. Hokuyo UTM-30LX-EW laser range-finder which scans at 40 Hz over a 270\u00b0 FOV at an angular resolution of 0.25\u00b0. The minimum detectable depth is 0.1 m and the maximum is 30 m, and intensity-like reflectance information is provided for each point. The Hokuyo is mounted on a dedicated tilting servo which has a range of +60\u00b0 for point cloud capture Microstrain 3DM-GX3-45 IMU with 3-axis ac- celerometer, 3-axis gyro, and GPS receiver/antenna"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The minimum detectable depth is 0.1 m and the maximum is 30 m, and intensity-like reflectance information is provided for each point.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b174ecfc-7311-43c7-856c-6fd9ffe328bd": {"__data__": {"id_": "b174ecfc-7311-43c7-856c-6fd9ffe328bd", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Hokuyo UTM-30LX-EW laser range-finder which scans at 40 Hz over a 270\u00b0 FOV at an angular resolution of 0.25\u00b0. The minimum detectable depth is 0.1 m and the maximum is 30 m, and intensity-like reflectance information is provided for each point. The Hokuyo is mounted on a dedicated tilting servo which has a range of +60\u00b0 for point cloud capture Microstrain 3DM-GX3-45 IMU with 3-axis ac- celerometer, 3-axis gyro, and GPS receiver/antenna based on the earlier-generation KAIST Hubo 2+ , is pictured in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Hokuyo is mounted on a dedicated tilting servo which has a range of +60\u00b0 for point cloud capture", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ea12de1-743e-4b48-8176-f0095af07924": {"__data__": {"id_": "8ea12de1-743e-4b48-8176-f0095af07924", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The minimum detectable depth is 0.1 m and the maximum is 30 m, and intensity-like reflectance information is provided for each point. The Hokuyo is mounted on a dedicated tilting servo which has a range of +60\u00b0 for point cloud capture Microstrain 3DM-GX3-45 IMU with 3-axis ac- celerometer, 3-axis gyro, and GPS receiver/antenna based on the earlier-generation KAIST Hubo 2+ , is pictured in Fig. 3, com- prise the set of known vehicles V used for this work: an electric Club Car DS and a gas-powered Polaris Ranger XP900."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Microstrain 3DM-GX3-45 IMU with 3-axis ac- celerometer, 3-axis gyro, and GPS receiver/antenna", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb56df6c-51fe-4ef4-a4a5-77731ee9f7d1": {"__data__": {"id_": "eb56df6c-51fe-4ef4-a4a5-77731ee9f7d1", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The Hokuyo is mounted on a dedicated tilting servo which has a range of +60\u00b0 for point cloud capture Microstrain 3DM-GX3-45 IMU with 3-axis ac- celerometer, 3-axis gyro, and GPS receiver/antenna based on the earlier-generation KAIST Hubo 2+ , is pictured in Fig. 3, com- prise the set of known vehicles V used for this work: an electric Club Car DS and a gas-powered Polaris Ranger XP900. Common features of these vehicles which distinguish them from passenger cars are an open cabin with the roof"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "based on the earlier-generation KAIST Hubo 2+ , is pictured in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ee3ef64-1f84-4368-82a8-a888ca3a2e97": {"__data__": {"id_": "4ee3ef64-1f84-4368-82a8-a888ca3a2e97", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Microstrain 3DM-GX3-45 IMU with 3-axis ac- celerometer, 3-axis gyro, and GPS receiver/antenna based on the earlier-generation KAIST Hubo 2+ , is pictured in Fig. 3, com- prise the set of known vehicles V used for this work: an electric Club Car DS and a gas-powered Polaris Ranger XP900. Common features of these vehicles which distinguish them from passenger cars are an open cabin with the roof Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3, com- prise the set of known vehicles V used for this work: an electric Club Car DS and a gas-powered Polaris Ranger XP900.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54b8bbcf-9eae-4cb0-976d-015dc6878057": {"__data__": {"id_": "54b8bbcf-9eae-4cb0-976d-015dc6878057", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "based on the earlier-generation KAIST Hubo 2+ , is pictured in Fig. 3, com- prise the set of known vehicles V used for this work: an electric Club Car DS and a gas-powered Polaris Ranger XP900. Common features of these vehicles which distinguish them from passenger cars are an open cabin with the roof Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. supported by relatively thin pillars; and a bench-like front seat and no center console, making movement between the passenger\u2019s and driver\u2019s side during ingress/egress possible."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Common features of these vehicles which distinguish them from passenger cars are an open cabin with the roof", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61bf5286-7530-433e-a692-176b74cdda00": {"__data__": {"id_": "61bf5286-7530-433e-a692-176b74cdda00", "embedding": null, "metadata": {"page_number": 2, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "3, com- prise the set of known vehicles V used for this work: an electric Club Car DS and a gas-powered Polaris Ranger XP900. Common features of these vehicles which distinguish them from passenger cars are an open cabin with the roof Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. supported by relatively thin pillars; and a bench-like front seat and no center console, making movement between the passenger\u2019s and driver\u2019s side during ingress/egress possible. The Polaris is larger overall, and it has a bump on the floor cov- ering the drive shaft, visible in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81f4e997-2885-482d-8a4b-5b12b6943b5e": {"__data__": {"id_": "81f4e997-2885-482d-8a4b-5b12b6943b5e", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Common features of these vehicles which distinguish them from passenger cars are an open cabin with the roof Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. supported by relatively thin pillars; and a bench-like front seat and no center console, making movement between the passenger\u2019s and driver\u2019s side during ingress/egress possible. The Polaris is larger overall, and it has a bump on the floor cov- ering the drive shaft, visible in Fig. Driving disparities (different steering ratios, implications of electric motor vs. gas engine, etc.)"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "supported by relatively thin pillars; and a bench-like front seat and no center console, making movement between the passenger\u2019s and driver\u2019s side during ingress/egress possible.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c396f98-3064-42c9-9db0-8b3749ade0d5": {"__data__": {"id_": "4c396f98-3064-42c9-9db0-8b3749ade0d5", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. supported by relatively thin pillars; and a bench-like front seat and no center console, making movement between the passenger\u2019s and driver\u2019s side during ingress/egress possible. The Polaris is larger overall, and it has a bump on the floor cov- ering the drive shaft, visible in Fig. Driving disparities (different steering ratios, implications of electric motor vs. gas engine, etc.) In the 2013 DRC trials Vehicle task , humans were allowed to set the robot up inside a vehicle in a drive-ready posture \u5165 uriue rather than having it attempt to approach the vehicle, step up, and seat itself (aka ingress)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Polaris is larger overall, and it has a bump on the floor cov- ering the drive shaft, visible in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7aea38d-e548-4b5f-b80f-1723bd840021": {"__data__": {"id_": "c7aea38d-e548-4b5f-b80f-1723bd840021", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "supported by relatively thin pillars; and a bench-like front seat and no center console, making movement between the passenger\u2019s and driver\u2019s side during ingress/egress possible. The Polaris is larger overall, and it has a bump on the floor cov- ering the drive shaft, visible in Fig. Driving disparities (different steering ratios, implications of electric motor vs. gas engine, etc.) In the 2013 DRC trials Vehicle task , humans were allowed to set the robot up inside a vehicle in a drive-ready posture \u5165 uriue rather than having it attempt to approach the vehicle, step up, and seat itself (aka ingress). By posture we mean the complete robot state which combines its joint state \u00a9 = (41, 6,..., On] and its pose P = [x, y, z, a, 3, 7] in the vehicle_interior frame.!"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Driving disparities (different steering ratios, implications of electric motor vs. gas engine, etc.)", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f2d0bd2-b916-4222-ac4e-6397a68f3ad9": {"__data__": {"id_": "2f2d0bd2-b916-4222-ac4e-6397a68f3ad9", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The Polaris is larger overall, and it has a bump on the floor cov- ering the drive shaft, visible in Fig. Driving disparities (different steering ratios, implications of electric motor vs. gas engine, etc.) In the 2013 DRC trials Vehicle task , humans were allowed to set the robot up inside a vehicle in a drive-ready posture \u5165 uriue rather than having it attempt to approach the vehicle, step up, and seat itself (aka ingress). By posture we mean the complete robot state which combines its joint state \u00a9 = (41, 6,..., On] and its pose P = [x, y, z, a, 3, 7] in the vehicle_interior frame.! For DRC-Hubo this posture, an example of which is pictured in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the 2013 DRC trials Vehicle task , humans were allowed to set the robot up inside a vehicle in a drive-ready posture \u5165 uriue rather than having it attempt to approach the vehicle, step up, and seat itself (aka ingress).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9643de6-e07c-4a03-8fbb-6642b8ce0c95": {"__data__": {"id_": "e9643de6-e07c-4a03-8fbb-6642b8ce0c95", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Driving disparities (different steering ratios, implications of electric motor vs. gas engine, etc.) In the 2013 DRC trials Vehicle task , humans were allowed to set the robot up inside a vehicle in a drive-ready posture \u5165 uriue rather than having it attempt to approach the vehicle, step up, and seat itself (aka ingress). By posture we mean the complete robot state which combines its joint state \u00a9 = (41, 6,..., On] and its pose P = [x, y, z, a, 3, 7] in the vehicle_interior frame.! For DRC-Hubo this posture, an example of which is pictured in Fig. 5(a), consists of: (a) the robot\u2019s torso offset to the right of the steering wheel; (b) its left hand peg inserted between the steering wheel \u201cspokes\u201d; (c) its right hand resting on its lap or grasping a vehicle support structure; and (d) its left foot near the accelerator pedal with its right foot flat on the floor."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By posture we mean the complete robot state which combines its joint state \u00a9 = (41, 6,..., On] and its pose P = [x, y, z, a, 3, 7] in the vehicle_interior frame.!", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "106c24d5-d466-4657-8e43-2cf29526aeb5": {"__data__": {"id_": "106c24d5-d466-4657-8e43-2cf29526aeb5", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "In the 2013 DRC trials Vehicle task , humans were allowed to set the robot up inside a vehicle in a drive-ready posture \u5165 uriue rather than having it attempt to approach the vehicle, step up, and seat itself (aka ingress). By posture we mean the complete robot state which combines its joint state \u00a9 = (41, 6,..., On] and its pose P = [x, y, z, a, 3, 7] in the vehicle_interior frame.! For DRC-Hubo this posture, an example of which is pictured in Fig. 5(a), consists of: (a) the robot\u2019s torso offset to the right of the steering wheel; (b) its left hand peg inserted between the steering wheel \u201cspokes\u201d; (c) its right hand resting on its lap or grasping a vehicle support structure; and (d) its left foot near the accelerator pedal with its right foot flat on the floor. and spoke arrangement; roof pillar spacing, angle, and cross-sectional shape/thickness; lateral location of accelerator; and so on."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For DRC-Hubo this posture, an example of which is pictured in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e02bd80e-d9ce-4757-810e-73bb19175399": {"__data__": {"id_": "e02bd80e-d9ce-4757-810e-73bb19175399", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "By posture we mean the complete robot state which combines its joint state \u00a9 = (41, 6,..., On] and its pose P = [x, y, z, a, 3, 7] in the vehicle_interior frame.! For DRC-Hubo this posture, an example of which is pictured in Fig. 5(a), consists of: (a) the robot\u2019s torso offset to the right of the steering wheel; (b) its left hand peg inserted between the steering wheel \u201cspokes\u201d; (c) its right hand resting on its lap or grasping a vehicle support structure; and (d) its left foot near the accelerator pedal with its right foot flat on the floor. and spoke arrangement; roof pillar spacing, angle, and cross-sectional shape/thickness; lateral location of accelerator; and so on. Thus, each drive-ready posture is specific to a vehicle v: Xue = {OYnivesPopive}> and renderings of these are shown for each vehicle in V in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5(a), consists of: (a) the robot\u2019s torso offset to the right of the steering wheel; (b) its left hand peg inserted between the steering wheel \u201cspokes\u201d; (c) its right hand resting on its lap or grasping a vehicle support structure; and (d) its left foot near the accelerator pedal with its right foot flat on the floor.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f0e32de-7f5e-42ab-9b0d-80c691163f4b": {"__data__": {"id_": "1f0e32de-7f5e-42ab-9b0d-80c691163f4b", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "For DRC-Hubo this posture, an example of which is pictured in Fig. 5(a), consists of: (a) the robot\u2019s torso offset to the right of the steering wheel; (b) its left hand peg inserted between the steering wheel \u201cspokes\u201d; (c) its right hand resting on its lap or grasping a vehicle support structure; and (d) its left foot near the accelerator pedal with its right foot flat on the floor. and spoke arrangement; roof pillar spacing, angle, and cross-sectional shape/thickness; lateral location of accelerator; and so on. Thus, each drive-ready posture is specific to a vehicle v: Xue = {OYnivesPopive}> and renderings of these are shown for each vehicle in V in Fig. ;,,, with adequate pose precision is a time-consuming and strenuous task, and of course it requires a human to specify which vehicle the robot is in."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and spoke arrangement; roof pillar spacing, angle, and cross-sectional shape/thickness; lateral location of accelerator; and so on.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84f3be36-4936-4ff6-893b-69a6bc648cf1": {"__data__": {"id_": "84f3be36-4936-4ff6-893b-69a6bc648cf1", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "5(a), consists of: (a) the robot\u2019s torso offset to the right of the steering wheel; (b) its left hand peg inserted between the steering wheel \u201cspokes\u201d; (c) its right hand resting on its lap or grasping a vehicle support structure; and (d) its left foot near the accelerator pedal with its right foot flat on the floor. and spoke arrangement; roof pillar spacing, angle, and cross-sectional shape/thickness; lateral location of accelerator; and so on. Thus, each drive-ready posture is specific to a vehicle v: Xue = {OYnivesPopive}> and renderings of these are shown for each vehicle in V in Fig. ;,,, with adequate pose precision is a time-consuming and strenuous task, and of course it requires a human to specify which vehicle the robot is in. Therefore, we modify the robot insertion process to make it both faster and vehicle-independent."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, each drive-ready posture is specific to a vehicle v: Xue = {OYnivesPopive}> and renderings of these are shown for each vehicle in V in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35b7fb4d-e33a-4b26-aa80-c8800d2c43b9": {"__data__": {"id_": "35b7fb4d-e33a-4b26-aa80-c8800d2c43b9", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "and spoke arrangement; roof pillar spacing, angle, and cross-sectional shape/thickness; lateral location of accelerator; and so on. Thus, each drive-ready posture is specific to a vehicle v: Xue = {OYnivesPopive}> and renderings of these are shown for each vehicle in V in Fig. ;,,, with adequate pose precision is a time-consuming and strenuous task, and of course it requires a human to specify which vehicle the robot is in. Therefore, we modify the robot insertion process to make it both faster and vehicle-independent. we assume that a high-resolution 3-D point cloud Ci,,,,, of the dashboard?"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ";,,, with adequate pose precision is a time-consuming and strenuous task, and of course it requires a human to specify which vehicle the robot is in.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e162de4-4807-421a-bae4-15123a2b4352": {"__data__": {"id_": "1e162de4-4807-421a-bae4-15123a2b4352", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Thus, each drive-ready posture is specific to a vehicle v: Xue = {OYnivesPopive}> and renderings of these are shown for each vehicle in V in Fig. ;,,, with adequate pose precision is a time-consuming and strenuous task, and of course it requires a human to specify which vehicle the robot is in. Therefore, we modify the robot insertion process to make it both faster and vehicle-independent. we assume that a high-resolution 3-D point cloud Ci,,,,, of the dashboard? of each vehicle v \u20ac V, aligned with the vehicle_interior frame, is available as a reference for the robot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, we modify the robot insertion process to make it both faster and vehicle-independent.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcb446b2-c4a3-4492-b569-524a87e949fd": {"__data__": {"id_": "fcb446b2-c4a3-4492-b569-524a87e949fd", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": ";,,, with adequate pose precision is a time-consuming and strenuous task, and of course it requires a human to specify which vehicle the robot is in. Therefore, we modify the robot insertion process to make it both faster and vehicle-independent. we assume that a high-resolution 3-D point cloud Ci,,,,, of the dashboard? of each vehicle v \u20ac V, aligned with the vehicle_interior frame, is available as a reference for the robot. The dashboard reference clouds used here (shown from different views in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we assume that a high-resolution 3-D point cloud Ci,,,,, of the dashboard?", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46c250ae-ea27-4e5b-8a29-77585af21cf3": {"__data__": {"id_": "46c250ae-ea27-4e5b-8a29-77585af21cf3", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Therefore, we modify the robot insertion process to make it both faster and vehicle-independent. we assume that a high-resolution 3-D point cloud Ci,,,,, of the dashboard? of each vehicle v \u20ac V, aligned with the vehicle_interior frame, is available as a reference for the robot. The dashboard reference clouds used here (shown from different views in Fig. 9) were acquired by the ladar on the sensor head tilting at 1\u00b0/s, voxelized to 0.025 m resolution, trimmed of all background features, and are XYZ only."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of each vehicle v \u20ac V, aligned with the vehicle_interior frame, is available as a reference for the robot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a72ace99-cfd9-4184-900e-5b1d40d829b7": {"__data__": {"id_": "a72ace99-cfd9-4184-900e-5b1d40d829b7", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "we assume that a high-resolution 3-D point cloud Ci,,,,, of the dashboard? of each vehicle v \u20ac V, aligned with the vehicle_interior frame, is available as a reference for the robot. The dashboard reference clouds used here (shown from different views in Fig. 9) were acquired by the ladar on the sensor head tilting at 1\u00b0/s, voxelized to 0.025 m resolution, trimmed of all background features, and are XYZ only. (1) The robot is placed (or arrives on its own) in the pas- senger seat in a neutral/vehicle-agnostic sitting position Xneutral, illustrated in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dashboard reference clouds used here (shown from different views in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09636e0f-a68f-416f-92e7-f0d60ba3fe73": {"__data__": {"id_": "09636e0f-a68f-416f-92e7-f0d60ba3fe73", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "of each vehicle v \u20ac V, aligned with the vehicle_interior frame, is available as a reference for the robot. The dashboard reference clouds used here (shown from different views in Fig. 9) were acquired by the ladar on the sensor head tilting at 1\u00b0/s, voxelized to 0.025 m resolution, trimmed of all background features, and are XYZ only. (1) The robot is placed (or arrives on its own) in the pas- senger seat in a neutral/vehicle-agnostic sitting position Xneutral, illustrated in Fig. The passenger side is advantageous kinematically for steering and entering on that side avoids collision issues with the steering wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9) were acquired by the ladar on the sensor head tilting at 1\u00b0/s, voxelized to 0.025 m resolution, trimmed of all background features, and are XYZ only.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5a677c4-31e5-4029-b071-5c26cb51ea95": {"__data__": {"id_": "d5a677c4-31e5-4029-b071-5c26cb51ea95", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The dashboard reference clouds used here (shown from different views in Fig. 9) were acquired by the ladar on the sensor head tilting at 1\u00b0/s, voxelized to 0.025 m resolution, trimmed of all background features, and are XYZ only. (1) The robot is placed (or arrives on its own) in the pas- senger seat in a neutral/vehicle-agnostic sitting position Xneutral, illustrated in Fig. The passenger side is advantageous kinematically for steering and entering on that side avoids collision issues with the steering wheel. (2) The robot obtains a 3-D point cloud/image capture"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(1) The robot is placed (or arrives on its own) in the pas- senger seat in a neutral/vehicle-agnostic sitting position Xneutral, illustrated in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffe6cecf-9077-4691-81ca-e69434661773": {"__data__": {"id_": "ffe6cecf-9077-4691-81ca-e69434661773", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "9) were acquired by the ladar on the sensor head tilting at 1\u00b0/s, voxelized to 0.025 m resolution, trimmed of all background features, and are XYZ only. (1) The robot is placed (or arrives on its own) in the pas- senger seat in a neutral/vehicle-agnostic sitting position Xneutral, illustrated in Fig. The passenger side is advantageous kinematically for steering and entering on that side avoids collision issues with the steering wheel. (2) The robot obtains a 3-D point cloud/image capture 4, is convenient for robot motion planning inside the vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The passenger side is advantageous kinematically for steering and entering on that side avoids collision issues with the steering wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62572afb-f057-42c4-82a3-a3a894d1e6dd": {"__data__": {"id_": "62572afb-f057-42c4-82a3-a3a894d1e6dd", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "(1) The robot is placed (or arrives on its own) in the pas- senger seat in a neutral/vehicle-agnostic sitting position Xneutral, illustrated in Fig. The passenger side is advantageous kinematically for steering and entering on that side avoids collision issues with the steering wheel. (2) The robot obtains a 3-D point cloud/image capture 4, is convenient for robot motion planning inside the vehicle. Its origin is the intersection between the front of the seats, the floor, and vehicle centerline."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2) The robot obtains a 3-D point cloud/image capture", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e77d5652-e7f3-4714-a80f-d0a6b3e289d5": {"__data__": {"id_": "e77d5652-e7f3-4714-a80f-d0a6b3e289d5", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The passenger side is advantageous kinematically for steering and entering on that side avoids collision issues with the steering wheel. (2) The robot obtains a 3-D point cloud/image capture 4, is convenient for robot motion planning inside the vehicle. Its origin is the intersection between the front of the seats, the floor, and vehicle centerline. 2We define the dashboard loosely as the portion of the vehicle interior vertically above the floor and below head height; and longitudinally between the front of the seat and the beginning of the hood."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4, is convenient for robot motion planning inside the vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7a585f4-fddd-4bc0-a378-d1e0b708076a": {"__data__": {"id_": "a7a585f4-fddd-4bc0-a378-d1e0b708076a", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "(2) The robot obtains a 3-D point cloud/image capture 4, is convenient for robot motion planning inside the vehicle. Its origin is the intersection between the front of the seats, the floor, and vehicle centerline. 2We define the dashboard loosely as the portion of the vehicle interior vertically above the floor and below head height; and longitudinally between the front of the seat and the beginning of the hood. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Its origin is the intersection between the front of the seats, the floor, and vehicle centerline.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b7ec44e-89f8-48db-a85a-eea397103770": {"__data__": {"id_": "0b7ec44e-89f8-48db-a85a-eea397103770", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "4, is convenient for robot motion planning inside the vehicle. Its origin is the intersection between the front of the seats, the floor, and vehicle centerline. 2We define the dashboard loosely as the portion of the vehicle interior vertically above the floor and below head height; and longitudinally between the front of the seat and the beginning of the hood. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Cneutral Of the rough dashboard area in front of it in order to infer the vehicle @ it is in."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2We define the dashboard loosely as the portion of the vehicle interior vertically above the floor and below head height; and longitudinally between the front of the seat and the beginning of the hood.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cdafc79b-f932-4c64-8467-9286fc9f7d1a": {"__data__": {"id_": "cdafc79b-f932-4c64-8467-9286fc9f7d1a", "embedding": null, "metadata": {"page_number": 3, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Its origin is the intersection between the front of the seats, the floor, and vehicle centerline. 2We define the dashboard loosely as the portion of the vehicle interior vertically above the floor and below head height; and longitudinally between the front of the seat and the beginning of the hood. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Cneutral Of the rough dashboard area in front of it in order to infer the vehicle @ it is in. It precisely estimates its initial pose within the vehicle Pe viral by registering Creutrat to the reference CT (3)"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "277b97fe-66c1-46c9-b801-86248e74f8bd": {"__data__": {"id_": "277b97fe-66c1-46c9-b801-86248e74f8bd", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "2We define the dashboard loosely as the portion of the vehicle interior vertically above the floor and below head height; and longitudinally between the front of the seat and the beginning of the hood. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Cneutral Of the rough dashboard area in front of it in order to infer the vehicle @ it is in. It precisely estimates its initial pose within the vehicle Pe viral by registering Creutrat to the reference CT (3) It formulates and executes an interfacing plan to move to rive: Lhe leftward movement on the seat to reach the drive-ready posture is a stereotyped quadrupedal motion which we call \u201cscooting\u201d."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cneutral Of the rough dashboard area in front of it in order to infer the vehicle @ it is in.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1da8e5c3-1cd6-4b97-a8bb-125e775be3c0": {"__data__": {"id_": "1da8e5c3-1cd6-4b97-a8bb-125e775be3c0", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Cneutral Of the rough dashboard area in front of it in order to infer the vehicle @ it is in. It precisely estimates its initial pose within the vehicle Pe viral by registering Creutrat to the reference CT (3) It formulates and executes an interfacing plan to move to rive: Lhe leftward movement on the seat to reach the drive-ready posture is a stereotyped quadrupedal motion which we call \u201cscooting\u201d. After each motion cycle, one of which is shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It precisely estimates its initial pose within the vehicle Pe viral by registering Creutrat to the reference CT (3)", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f9594e7-7437-47b3-9a14-40e926f96cb4": {"__data__": {"id_": "8f9594e7-7437-47b3-9a14-40e926f96cb4", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Cneutral Of the rough dashboard area in front of it in order to infer the vehicle @ it is in. It precisely estimates its initial pose within the vehicle Pe viral by registering Creutrat to the reference CT (3) It formulates and executes an interfacing plan to move to rive: Lhe leftward movement on the seat to reach the drive-ready posture is a stereotyped quadrupedal motion which we call \u201cscooting\u201d. After each motion cycle, one of which is shown in Fig. 6, a new point cloud is scanned and step (3) is repeated to decide how far to move or to stop."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It formulates and executes an interfacing plan to move to rive: Lhe leftward movement on the seat to reach the drive-ready posture is a stereotyped quadrupedal motion which we call \u201cscooting\u201d.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfaa9501-85dd-443c-88c0-768f67ee16c6": {"__data__": {"id_": "dfaa9501-85dd-443c-88c0-768f67ee16c6", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "It precisely estimates its initial pose within the vehicle Pe viral by registering Creutrat to the reference CT (3) It formulates and executes an interfacing plan to move to rive: Lhe leftward movement on the seat to reach the drive-ready posture is a stereotyped quadrupedal motion which we call \u201cscooting\u201d. After each motion cycle, one of which is shown in Fig. 6, a new point cloud is scanned and step (3) is repeated to decide how far to move or to stop. the robot obtains a point cloud of the dashboard region Cneutral by scanning the sensor head ladar as detailed above for the creation of the reference clouds."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After each motion cycle, one of which is shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21d39b5f-e045-49e6-a01f-8b5c1569f201": {"__data__": {"id_": "21d39b5f-e045-49e6-a01f-8b5c1569f201", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "It formulates and executes an interfacing plan to move to rive: Lhe leftward movement on the seat to reach the drive-ready posture is a stereotyped quadrupedal motion which we call \u201cscooting\u201d. After each motion cycle, one of which is shown in Fig. 6, a new point cloud is scanned and step (3) is repeated to decide how far to move or to stop. the robot obtains a point cloud of the dashboard region Cneutral by scanning the sensor head ladar as detailed above for the creation of the reference clouds. Examples of such clouds taken from different locations and angles in both vehicles are shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6, a new point cloud is scanned and step (3) is repeated to decide how far to move or to stop.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5514903f-b12d-4e31-9812-bd224ee1d5ff": {"__data__": {"id_": "5514903f-b12d-4e31-9812-bd224ee1d5ff", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "After each motion cycle, one of which is shown in Fig. 6, a new point cloud is scanned and step (3) is repeated to decide how far to move or to stop. the robot obtains a point cloud of the dashboard region Cneutral by scanning the sensor head ladar as detailed above for the creation of the reference clouds. Examples of such clouds taken from different locations and angles in both vehicles are shown in Fig. Recognizing the current vehicle by matching Cneutral to one of the \u201ctemplates\u201d {Chasns-- Cl} is a shape re- trieval/classification problem , , , ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot obtains a point cloud of the dashboard region Cneutral by scanning the sensor head ladar as detailed above for the creation of the reference clouds.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37833400-2515-490c-9ae7-387a331571a7": {"__data__": {"id_": "37833400-2515-490c-9ae7-387a331571a7", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "6, a new point cloud is scanned and step (3) is repeated to decide how far to move or to stop. the robot obtains a point cloud of the dashboard region Cneutral by scanning the sensor head ladar as detailed above for the creation of the reference clouds. Examples of such clouds taken from different locations and angles in both vehicles are shown in Fig. Recognizing the current vehicle by matching Cneutral to one of the \u201ctemplates\u201d {Chasns-- Cl} is a shape re- trieval/classification problem , , , . However, the 3-D relief of the two dashboards is similar enough that the error after performing a fit to each reference cloud is not a reliable match indicator."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Examples of such clouds taken from different locations and angles in both vehicles are shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f9e7e34-e0cb-43a0-b863-a8bd2a70acd6": {"__data__": {"id_": "9f9e7e34-e0cb-43a0-b863-a8bd2a70acd6", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "the robot obtains a point cloud of the dashboard region Cneutral by scanning the sensor head ladar as detailed above for the creation of the reference clouds. Examples of such clouds taken from different locations and angles in both vehicles are shown in Fig. Recognizing the current vehicle by matching Cneutral to one of the \u201ctemplates\u201d {Chasns-- Cl} is a shape re- trieval/classification problem , , , . However, the 3-D relief of the two dashboards is similar enough that the error after performing a fit to each reference cloud is not a reliable match indicator. Instead, we exploit the size discrepancy between the vehicles to discriminate them."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recognizing the current vehicle by matching Cneutral to one of the \u201ctemplates\u201d {Chasns-- Cl} is a shape re- trieval/classification problem , , , .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "743e0625-9e34-4bf4-9a2d-87416caf41a5": {"__data__": {"id_": "743e0625-9e34-4bf4-9a2d-87416caf41a5", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Examples of such clouds taken from different locations and angles in both vehicles are shown in Fig. Recognizing the current vehicle by matching Cneutral to one of the \u201ctemplates\u201d {Chasns-- Cl} is a shape re- trieval/classification problem , , , . However, the 3-D relief of the two dashboards is similar enough that the error after performing a fit to each reference cloud is not a reliable match indicator. Instead, we exploit the size discrepancy between the vehicles to discriminate them. In particular, the Polaris dashboard is about 0.35 m wider, so after doing a robust vertical plane fit to Creutrat, the maximum lateral distance between inliers can be thresholded, following techniques we described in , to infer v. An alternative method to estimate the width is to first detect the roof pillars, which form parallel, nearly vertical cylinders in Cneutral."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the 3-D relief of the two dashboards is similar enough that the error after performing a fit to each reference cloud is not a reliable match indicator.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab7e35f0-8034-4a8c-8410-31d435c7e7e4": {"__data__": {"id_": "ab7e35f0-8034-4a8c-8410-31d435c7e7e4", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Recognizing the current vehicle by matching Cneutral to one of the \u201ctemplates\u201d {Chasns-- Cl} is a shape re- trieval/classification problem , , , . However, the 3-D relief of the two dashboards is similar enough that the error after performing a fit to each reference cloud is not a reliable match indicator. Instead, we exploit the size discrepancy between the vehicles to discriminate them. In particular, the Polaris dashboard is about 0.35 m wider, so after doing a robust vertical plane fit to Creutrat, the maximum lateral distance between inliers can be thresholded, following techniques we described in , to infer v. An alternative method to estimate the width is to first detect the roof pillars, which form parallel, nearly vertical cylinders in Cneutral. II-B we do the equivalent with a single level ladar scan."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead, we exploit the size discrepancy between the vehicles to discriminate them.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ae7c766-9c40-4db8-a5a3-1d98f626bbb2": {"__data__": {"id_": "0ae7c766-9c40-4db8-a5a3-1d98f626bbb2", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "However, the 3-D relief of the two dashboards is similar enough that the error after performing a fit to each reference cloud is not a reliable match indicator. Instead, we exploit the size discrepancy between the vehicles to discriminate them. In particular, the Polaris dashboard is about 0.35 m wider, so after doing a robust vertical plane fit to Creutrat, the maximum lateral distance between inliers can be thresholded, following techniques we described in , to infer v. An alternative method to estimate the width is to first detect the roof pillars, which form parallel, nearly vertical cylinders in Cneutral. II-B we do the equivalent with a single level ladar scan. the robot currently has two ways of estimating its pose P\u201d relative to vehicle_interior."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, the Polaris dashboard is about 0.35 m wider, so after doing a robust vertical plane fit to Creutrat, the maximum lateral distance between inliers can be thresholded, following techniques we described in , to infer v. An alternative method to estimate the width is to first detect the roof pillars, which form parallel, nearly vertical cylinders in Cneutral.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4951d43-33c8-4790-88b7-47c2afba07a9": {"__data__": {"id_": "b4951d43-33c8-4790-88b7-47c2afba07a9", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Instead, we exploit the size discrepancy between the vehicles to discriminate them. In particular, the Polaris dashboard is about 0.35 m wider, so after doing a robust vertical plane fit to Creutrat, the maximum lateral distance between inliers can be thresholded, following techniques we described in , to infer v. An alternative method to estimate the width is to first detect the roof pillars, which form parallel, nearly vertical cylinders in Cneutral. II-B we do the equivalent with a single level ladar scan. the robot currently has two ways of estimating its pose P\u201d relative to vehicle_interior. The first way, which takes several seconds, is to perform a RANSAC-style robust registration of Cneutral and Vash: Briefly, a minimum set of correspondences to compute a rigid transform [R|t] between the two point clouds are repeatedly chosen at random, and the number of inliers cal- culated for each sample."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "II-B we do the equivalent with a single level ladar scan.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9027d42d-9546-4a13-9286-8abb936ff526": {"__data__": {"id_": "9027d42d-9546-4a13-9286-8abb936ff526", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "In particular, the Polaris dashboard is about 0.35 m wider, so after doing a robust vertical plane fit to Creutrat, the maximum lateral distance between inliers can be thresholded, following techniques we described in , to infer v. An alternative method to estimate the width is to first detect the roof pillars, which form parallel, nearly vertical cylinders in Cneutral. II-B we do the equivalent with a single level ladar scan. the robot currently has two ways of estimating its pose P\u201d relative to vehicle_interior. The first way, which takes several seconds, is to perform a RANSAC-style robust registration of Cneutral and Vash: Briefly, a minimum set of correspondences to compute a rigid transform [R|t] between the two point clouds are repeatedly chosen at random, and the number of inliers cal- culated for each sample. The transform with the most inliers after a maximum number of iterations is then estimated using a least-squares fit."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot currently has two ways of estimating its pose P\u201d relative to vehicle_interior.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f886487-719f-4070-8d64-7d1eb61a8f38": {"__data__": {"id_": "5f886487-719f-4070-8d64-7d1eb61a8f38", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "II-B we do the equivalent with a single level ladar scan. the robot currently has two ways of estimating its pose P\u201d relative to vehicle_interior. The first way, which takes several seconds, is to perform a RANSAC-style robust registration of Cneutral and Vash: Briefly, a minimum set of correspondences to compute a rigid transform [R|t] between the two point clouds are repeatedly chosen at random, and the number of inliers cal- culated for each sample. The transform with the most inliers after a maximum number of iterations is then estimated using a least-squares fit. We use the Point Cloud Library\u2019s (PCL) Sample Consensus Initial Alignment with Fast Point Feature Histograms (FPFH) to perform this step."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first way, which takes several seconds, is to perform a RANSAC-style robust registration of Cneutral and Vash: Briefly, a minimum set of correspondences to compute a rigid transform [R|t] between the two point clouds are repeatedly chosen at random, and the number of inliers cal- culated for each sample.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa96f28e-b170-4afd-8d05-fda959122ef5": {"__data__": {"id_": "fa96f28e-b170-4afd-8d05-fda959122ef5", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "the robot currently has two ways of estimating its pose P\u201d relative to vehicle_interior. The first way, which takes several seconds, is to perform a RANSAC-style robust registration of Cneutral and Vash: Briefly, a minimum set of correspondences to compute a rigid transform [R|t] between the two point clouds are repeatedly chosen at random, and the number of inliers cal- culated for each sample. The transform with the most inliers after a maximum number of iterations is then estimated using a least-squares fit. We use the Point Cloud Library\u2019s (PCL) Sample Consensus Initial Alignment with Fast Point Feature Histograms (FPFH) to perform this step. The initial aligning transform is then refined using Iterative Closest Points (ICP) ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The transform with the most inliers after a maximum number of iterations is then estimated using a least-squares fit.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb0a2ade-e53f-485c-9de7-2884df038302": {"__data__": {"id_": "eb0a2ade-e53f-485c-9de7-2884df038302", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The first way, which takes several seconds, is to perform a RANSAC-style robust registration of Cneutral and Vash: Briefly, a minimum set of correspondences to compute a rigid transform [R|t] between the two point clouds are repeatedly chosen at random, and the number of inliers cal- culated for each sample. The transform with the most inliers after a maximum number of iterations is then estimated using a least-squares fit. We use the Point Cloud Library\u2019s (PCL) Sample Consensus Initial Alignment with Fast Point Feature Histograms (FPFH) to perform this step. The initial aligning transform is then refined using Iterative Closest Points (ICP) . there may still be some objects inside the vehicle (such as the steering wheel or the robot\u2019s own hands), so the robot does Euclidean cluster extraction with a small maximum cluster size."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use the Point Cloud Library\u2019s (PCL) Sample Consensus Initial Alignment with Fast Point Feature Histograms (FPFH) to perform this step.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19212ae0-b8f9-45ac-b06f-a528562d85ba": {"__data__": {"id_": "19212ae0-b8f9-45ac-b06f-a528562d85ba", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The transform with the most inliers after a maximum number of iterations is then estimated using a least-squares fit. We use the Point Cloud Library\u2019s (PCL) Sample Consensus Initial Alignment with Fast Point Feature Histograms (FPFH) to perform this step. The initial aligning transform is then refined using Iterative Closest Points (ICP) . there may still be some objects inside the vehicle (such as the steering wheel or the robot\u2019s own hands), so the robot does Euclidean cluster extraction with a small maximum cluster size. All cluster pairs are then checked for 2-D geometric feasibility (distance, angle, etc."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The initial aligning transform is then refined using Iterative Closest Points (ICP) .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5799a40-9301-409c-9769-fd68c5ea3aea": {"__data__": {"id_": "f5799a40-9301-409c-9769-fd68c5ea3aea", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "We use the Point Cloud Library\u2019s (PCL) Sample Consensus Initial Alignment with Fast Point Feature Histograms (FPFH) to perform this step. The initial aligning transform is then refined using Iterative Closest Points (ICP) . there may still be some objects inside the vehicle (such as the steering wheel or the robot\u2019s own hands), so the robot does Euclidean cluster extraction with a small maximum cluster size. All cluster pairs are then checked for 2-D geometric feasibility (distance, angle, etc. ), and the most likely feasible pair is used to extract the sensor head yaw and t,, ty."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "there may still be some objects inside the vehicle (such as the steering wheel or the robot\u2019s own hands), so the robot does Euclidean cluster extraction with a small maximum cluster size.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "252beae0-e031-4073-be3f-31897ef7a51d": {"__data__": {"id_": "252beae0-e031-4073-be3f-31897ef7a51d", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The initial aligning transform is then refined using Iterative Closest Points (ICP) . there may still be some objects inside the vehicle (such as the steering wheel or the robot\u2019s own hands), so the robot does Euclidean cluster extraction with a small maximum cluster size. All cluster pairs are then checked for 2-D geometric feasibility (distance, angle, etc. ), and the most likely feasible pair is used to extract the sensor head yaw and t,, ty. We make several important assumptions about the state of the vehicle when the robot begins interfacing: (1) It is powered on or the engine is running; (2) It is in forward mode (Club Car) or drive gear (Polaris); (3) The tires are straight and therefore the steering wheel\u2019s orientation is known."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All cluster pairs are then checked for 2-D geometric feasibility (distance, angle, etc.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55799fe2-c798-4fb0-933f-6d6d992d4292": {"__data__": {"id_": "55799fe2-c798-4fb0-933f-6d6d992d4292", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "there may still be some objects inside the vehicle (such as the steering wheel or the robot\u2019s own hands), so the robot does Euclidean cluster extraction with a small maximum cluster size. All cluster pairs are then checked for 2-D geometric feasibility (distance, angle, etc. ), and the most likely feasible pair is used to extract the sensor head yaw and t,, ty. We make several important assumptions about the state of the vehicle when the robot begins interfacing: (1) It is powered on or the engine is running; (2) It is in forward mode (Club Car) or drive gear (Polaris); (3) The tires are straight and therefore the steering wheel\u2019s orientation is known. 5 for the Club Car and Polaris show the robot\u2019s right hand resting on its lap and grasping a roof pillar, respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "), and the most likely feasible pair is used to extract the sensor head yaw and t,, ty.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04bee71c-d5fd-4d15-9019-9df7fde7bcda": {"__data__": {"id_": "04bee71c-d5fd-4d15-9019-9df7fde7bcda", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "All cluster pairs are then checked for 2-D geometric feasibility (distance, angle, etc. ), and the most likely feasible pair is used to extract the sensor head yaw and t,, ty. We make several important assumptions about the state of the vehicle when the robot begins interfacing: (1) It is powered on or the engine is running; (2) It is in forward mode (Club Car) or drive gear (Polaris); (3) The tires are straight and therefore the steering wheel\u2019s orientation is known. 5 for the Club Car and Polaris show the robot\u2019s right hand resting on its lap and grasping a roof pillar, respectively. At the very low speeds we have currently tested, forces on the robot due to vehicle dynamics are small and therefore active bracing is not required in either vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We make several important assumptions about the state of the vehicle when the robot begins interfacing: (1) It is powered on or the engine is running; (2) It is in forward mode (Club Car) or drive gear (Polaris); (3) The tires are straight and therefore the steering wheel\u2019s orientation is known.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac669a46-09bf-466f-95a7-dbb5cd9e7975": {"__data__": {"id_": "ac669a46-09bf-466f-95a7-dbb5cd9e7975", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "), and the most likely feasible pair is used to extract the sensor head yaw and t,, ty. We make several important assumptions about the state of the vehicle when the robot begins interfacing: (1) It is powered on or the engine is running; (2) It is in forward mode (Club Car) or drive gear (Polaris); (3) The tires are straight and therefore the steering wheel\u2019s orientation is known. 5 for the Club Car and Polaris show the robot\u2019s right hand resting on its lap and grasping a roof pillar, respectively. At the very low speeds we have currently tested, forces on the robot due to vehicle dynamics are small and therefore active bracing is not required in either vehicle. Grasping is essential for the stages of ingress/egress in which the robot transitions between the vehicle exterior and interior, but they are outside this paper\u2019s scope (more details can be found in )."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 for the Club Car and Polaris show the robot\u2019s right hand resting on its lap and grasping a roof pillar, respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16eceee8-6f7d-4c2e-9d0d-d0761d744722": {"__data__": {"id_": "16eceee8-6f7d-4c2e-9d0d-d0761d744722", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "We make several important assumptions about the state of the vehicle when the robot begins interfacing: (1) It is powered on or the engine is running; (2) It is in forward mode (Club Car) or drive gear (Polaris); (3) The tires are straight and therefore the steering wheel\u2019s orientation is known. 5 for the Club Car and Polaris show the robot\u2019s right hand resting on its lap and grasping a roof pillar, respectively. At the very low speeds we have currently tested, forces on the robot due to vehicle dynamics are small and therefore active bracing is not required in either vehicle. Grasping is essential for the stages of ingress/egress in which the robot transitions between the vehicle exterior and interior, but they are outside this paper\u2019s scope (more details can be found in ). The robot turns the steering wheel by dialing: it moves its left hand in a circular trajectory with the peg inserted between the steering wheel spokes as shown in Fig 7(a)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the very low speeds we have currently tested, forces on the robot due to vehicle dynamics are small and therefore active bracing is not required in either vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1fee925-bf34-4834-b356-bf0bea9f4aa4": {"__data__": {"id_": "b1fee925-bf34-4834-b356-bf0bea9f4aa4", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "5 for the Club Car and Polaris show the robot\u2019s right hand resting on its lap and grasping a roof pillar, respectively. At the very low speeds we have currently tested, forces on the robot due to vehicle dynamics are small and therefore active bracing is not required in either vehicle. Grasping is essential for the stages of ingress/egress in which the robot transitions between the vehicle exterior and interior, but they are outside this paper\u2019s scope (more details can be found in ). The robot turns the steering wheel by dialing: it moves its left hand in a circular trajectory with the peg inserted between the steering wheel spokes as shown in Fig 7(a). This motion was chosen over holding the wheel in the traditional manner because it has no singularities, avoiding the necessity of regrasping when turning through large angles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Grasping is essential for the stages of ingress/egress in which the robot transitions between the vehicle exterior and interior, but they are outside this paper\u2019s scope (more details can be found in ).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfdc197e-4e28-4577-b534-af3696543e18": {"__data__": {"id_": "bfdc197e-4e28-4577-b534-af3696543e18", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "At the very low speeds we have currently tested, forces on the robot due to vehicle dynamics are small and therefore active bracing is not required in either vehicle. Grasping is essential for the stages of ingress/egress in which the robot transitions between the vehicle exterior and interior, but they are outside this paper\u2019s scope (more details can be found in ). The robot turns the steering wheel by dialing: it moves its left hand in a circular trajectory with the peg inserted between the steering wheel spokes as shown in Fig 7(a). This motion was chosen over holding the wheel in the traditional manner because it has no singularities, avoiding the necessity of regrasping when turning through large angles. The lack of a slip ring in DRC-Hubo\u2019s wrist roll joint precludes grasping"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot turns the steering wheel by dialing: it moves its left hand in a circular trajectory with the peg inserted between the steering wheel spokes as shown in Fig 7(a).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82edaa0d-6009-418b-a99f-3c85d9df9c65": {"__data__": {"id_": "82edaa0d-6009-418b-a99f-3c85d9df9c65", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Grasping is essential for the stages of ingress/egress in which the robot transitions between the vehicle exterior and interior, but they are outside this paper\u2019s scope (more details can be found in ). The robot turns the steering wheel by dialing: it moves its left hand in a circular trajectory with the peg inserted between the steering wheel spokes as shown in Fig 7(a). This motion was chosen over holding the wheel in the traditional manner because it has no singularities, avoiding the necessity of regrasping when turning through large angles. The lack of a slip ring in DRC-Hubo\u2019s wrist roll joint precludes grasping Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This motion was chosen over holding the wheel in the traditional manner because it has no singularities, avoiding the necessity of regrasping when turning through large angles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf7d2f04-9c65-48ae-8e2d-0d479a80ea25": {"__data__": {"id_": "bf7d2f04-9c65-48ae-8e2d-0d479a80ea25", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The robot turns the steering wheel by dialing: it moves its left hand in a circular trajectory with the peg inserted between the steering wheel spokes as shown in Fig 7(a). This motion was chosen over holding the wheel in the traditional manner because it has no singularities, avoiding the necessity of regrasping when turning through large angles. The lack of a slip ring in DRC-Hubo\u2019s wrist roll joint precludes grasping Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the center of the wheel and turning it directly (the method of autopilots such as , and several 2013 DARPA DRC trials teams)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The lack of a slip ring in DRC-Hubo\u2019s wrist roll joint precludes grasping", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ab936fd-0670-4b21-b3e8-9f82edb1250d": {"__data__": {"id_": "1ab936fd-0670-4b21-b3e8-9f82edb1250d", "embedding": null, "metadata": {"page_number": 4, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "This motion was chosen over holding the wheel in the traditional manner because it has no singularities, avoiding the necessity of regrasping when turning through large angles. The lack of a slip ring in DRC-Hubo\u2019s wrist roll joint precludes grasping Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the center of the wheel and turning it directly (the method of autopilots such as , and several 2013 DARPA DRC trials teams). (b) not as good kinematically for steering, and (c) more difficult to get into and out of without colliding during the scooting phase described in Sec."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27ff4e30-59db-4d48-8cb8-905ecc536cd7": {"__data__": {"id_": "27ff4e30-59db-4d48-8cb8-905ecc536cd7", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The lack of a slip ring in DRC-Hubo\u2019s wrist roll joint precludes grasping Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the center of the wheel and turning it directly (the method of autopilots such as , and several 2013 DARPA DRC trials teams). (b) not as good kinematically for steering, and (c) more difficult to get into and out of without colliding during the scooting phase described in Sec. It is predetermined for each drive-ready position but can be reparametrized from sensor data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the center of the wheel and turning it directly (the method of autopilots such as , and several 2013 DARPA DRC trials teams).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce67012c-58a1-4b0a-bcd7-97cd2bd66044": {"__data__": {"id_": "ce67012c-58a1-4b0a-bcd7-97cd2bd66044", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. the center of the wheel and turning it directly (the method of autopilots such as , and several 2013 DARPA DRC trials teams). (b) not as good kinematically for steering, and (c) more difficult to get into and out of without colliding during the scooting phase described in Sec. It is predetermined for each drive-ready position but can be reparametrized from sensor data. The arm joint trajectories for the peg to trace the circle are generated by approximating it with a polygon (the circle is discretized at 30\u00b0 intervals here), solving the inverse kinematics offline for each vertex to make a look-up table, and linearly interpolating arm joint angles online for steering hand angles between the vertices."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) not as good kinematically for steering, and (c) more difficult to get into and out of without colliding during the scooting phase described in Sec.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4c8f9c6-8130-4f8d-96df-119ae5b7a550": {"__data__": {"id_": "f4c8f9c6-8130-4f8d-96df-119ae5b7a550", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "the center of the wheel and turning it directly (the method of autopilots such as , and several 2013 DARPA DRC trials teams). (b) not as good kinematically for steering, and (c) more difficult to get into and out of without colliding during the scooting phase described in Sec. It is predetermined for each drive-ready position but can be reparametrized from sensor data. The arm joint trajectories for the peg to trace the circle are generated by approximating it with a polygon (the circle is discretized at 30\u00b0 intervals here), solving the inverse kinematics offline for each vertex to make a look-up table, and linearly interpolating arm joint angles online for steering hand angles between the vertices. Using this method the robot can move its hand and therefore the wheel in a circle at up to \\dranal\u2122* = 120\u00b0/s."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is predetermined for each drive-ready position but can be reparametrized from sensor data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "459cd465-f5d3-40d5-b68c-c6996dea8c9e": {"__data__": {"id_": "459cd465-f5d3-40d5-b68c-c6996dea8c9e", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "(b) not as good kinematically for steering, and (c) more difficult to get into and out of without colliding during the scooting phase described in Sec. It is predetermined for each drive-ready position but can be reparametrized from sensor data. The arm joint trajectories for the peg to trace the circle are generated by approximating it with a polygon (the circle is discretized at 30\u00b0 intervals here), solving the inverse kinematics offline for each vertex to make a look-up table, and linearly interpolating arm joint angles online for steering hand angles between the vertices. Using this method the robot can move its hand and therefore the wheel in a circle at up to \\dranal\u2122* = 120\u00b0/s. A key limitation of this method is backlash: the robot hand motion only causes steering wheel motion when it is \u201cpushing\u201d one of the spokes\u2014when the steering direction is changed, there is a gap that the peg must cross before engaging another spoke."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The arm joint trajectories for the peg to trace the circle are generated by approximating it with a polygon (the circle is discretized at 30\u00b0 intervals here), solving the inverse kinematics offline for each vertex to make a look-up table, and linearly interpolating arm joint angles online for steering hand angles between the vertices.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c06b267-333b-4551-bb5a-8cde9ee67f9e": {"__data__": {"id_": "2c06b267-333b-4551-bb5a-8cde9ee67f9e", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "It is predetermined for each drive-ready position but can be reparametrized from sensor data. The arm joint trajectories for the peg to trace the circle are generated by approximating it with a polygon (the circle is discretized at 30\u00b0 intervals here), solving the inverse kinematics offline for each vertex to make a look-up table, and linearly interpolating arm joint angles online for steering hand angles between the vertices. Using this method the robot can move its hand and therefore the wheel in a circle at up to \\dranal\u2122* = 120\u00b0/s. A key limitation of this method is backlash: the robot hand motion only causes steering wheel motion when it is \u201cpushing\u201d one of the spokes\u2014when the steering direction is changed, there is a gap that the peg must cross before engaging another spoke. 3, the smallest (depending on which pair of spokes the peg is inserted between) backlash angle {,, for vehicle v is about 90\u00b0 for the Polaris and 60\u00b0 for the Club Car."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using this method the robot can move its hand and therefore the wheel in a circle at up to \\dranal\u2122* = 120\u00b0/s.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb70b768-f63f-4b78-9d08-ab445e61aa19": {"__data__": {"id_": "bb70b768-f63f-4b78-9d08-ab445e61aa19", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The arm joint trajectories for the peg to trace the circle are generated by approximating it with a polygon (the circle is discretized at 30\u00b0 intervals here), solving the inverse kinematics offline for each vertex to make a look-up table, and linearly interpolating arm joint angles online for steering hand angles between the vertices. Using this method the robot can move its hand and therefore the wheel in a circle at up to \\dranal\u2122* = 120\u00b0/s. A key limitation of this method is backlash: the robot hand motion only causes steering wheel motion when it is \u201cpushing\u201d one of the spokes\u2014when the steering direction is changed, there is a gap that the peg must cross before engaging another spoke. 3, the smallest (depending on which pair of spokes the peg is inserted between) backlash angle {,, for vehicle v is about 90\u00b0 for the Polaris and 60\u00b0 for the Club Car. Consequences of the backlash are: (1) Additional bookkeeping in the steering wheel controller, as the hand angle is not necessarily the same as the steering wheel angle (aka dpand \u5929 Psteer)s (2) A delay to change the sign of the steering rate dsteer of at least Tg, = Gnana/Bv s; and (3) Inability to resist external forces on the tires due to slope, bumps, etc."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A key limitation of this method is backlash: the robot hand motion only causes steering wheel motion when it is \u201cpushing\u201d one of the spokes\u2014when the steering direction is changed, there is a gap that the peg must cross before engaging another spoke.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25e61af9-90ad-40d9-86e8-72d1d884ed30": {"__data__": {"id_": "25e61af9-90ad-40d9-86e8-72d1d884ed30", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Using this method the robot can move its hand and therefore the wheel in a circle at up to \\dranal\u2122* = 120\u00b0/s. A key limitation of this method is backlash: the robot hand motion only causes steering wheel motion when it is \u201cpushing\u201d one of the spokes\u2014when the steering direction is changed, there is a gap that the peg must cross before engaging another spoke. 3, the smallest (depending on which pair of spokes the peg is inserted between) backlash angle {,, for vehicle v is about 90\u00b0 for the Polaris and 60\u00b0 for the Club Car. Consequences of the backlash are: (1) Additional bookkeeping in the steering wheel controller, as the hand angle is not necessarily the same as the steering wheel angle (aka dpand \u5929 Psteer)s (2) A delay to change the sign of the steering rate dsteer of at least Tg, = Gnana/Bv s; and (3) Inability to resist external forces on the tires due to slope, bumps, etc. which might tend to amplify the turning rate."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3, the smallest (depending on which pair of spokes the peg is inserted between) backlash angle {,, for vehicle v is about 90\u00b0 for the Polaris and 60\u00b0 for the Club Car.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2232d54b-fe21-409d-b988-884df8cd7a3f": {"__data__": {"id_": "2232d54b-fe21-409d-b988-884df8cd7a3f", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "A key limitation of this method is backlash: the robot hand motion only causes steering wheel motion when it is \u201cpushing\u201d one of the spokes\u2014when the steering direction is changed, there is a gap that the peg must cross before engaging another spoke. 3, the smallest (depending on which pair of spokes the peg is inserted between) backlash angle {,, for vehicle v is about 90\u00b0 for the Polaris and 60\u00b0 for the Club Car. Consequences of the backlash are: (1) Additional bookkeeping in the steering wheel controller, as the hand angle is not necessarily the same as the steering wheel angle (aka dpand \u5929 Psteer)s (2) A delay to change the sign of the steering rate dsteer of at least Tg, = Gnana/Bv s; and (3) Inability to resist external forces on the tires due to slope, bumps, etc. which might tend to amplify the turning rate. DRC-Hubo\u2019s left arm joint encoder values and forward kinematics are used to derive @panad, Which with a known initial steering wheel angle yields msieer."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Consequences of the backlash are: (1) Additional bookkeeping in the steering wheel controller, as the hand angle is not necessarily the same as the steering wheel angle (aka dpand \u5929 Psteer)s (2) A delay to change the sign of the steering rate dsteer of at least Tg, = Gnana/Bv s; and (3) Inability to resist external forces on the tires due to slope, bumps, etc.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6c57d80-c96a-41d4-a91b-b76179187713": {"__data__": {"id_": "b6c57d80-c96a-41d4-a91b-b76179187713", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "3, the smallest (depending on which pair of spokes the peg is inserted between) backlash angle {,, for vehicle v is about 90\u00b0 for the Polaris and 60\u00b0 for the Club Car. Consequences of the backlash are: (1) Additional bookkeeping in the steering wheel controller, as the hand angle is not necessarily the same as the steering wheel angle (aka dpand \u5929 Psteer)s (2) A delay to change the sign of the steering rate dsteer of at least Tg, = Gnana/Bv s; and (3) Inability to resist external forces on the tires due to slope, bumps, etc. which might tend to amplify the turning rate. DRC-Hubo\u2019s left arm joint encoder values and forward kinematics are used to derive @panad, Which with a known initial steering wheel angle yields msieer. This can be refined somewhat by sensing contact between the peg and spoke using a force-torque sensor in the wrist."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which might tend to amplify the turning rate.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2b93a86-5f4e-4e8c-ac6a-ee38c4d97f7c": {"__data__": {"id_": "c2b93a86-5f4e-4e8c-ac6a-ee38c4d97f7c", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Consequences of the backlash are: (1) Additional bookkeeping in the steering wheel controller, as the hand angle is not necessarily the same as the steering wheel angle (aka dpand \u5929 Psteer)s (2) A delay to change the sign of the steering rate dsteer of at least Tg, = Gnana/Bv s; and (3) Inability to resist external forces on the tires due to slope, bumps, etc. which might tend to amplify the turning rate. DRC-Hubo\u2019s left arm joint encoder values and forward kinematics are used to derive @panad, Which with a known initial steering wheel angle yields msieer. This can be refined somewhat by sensing contact between the peg and spoke using a force-torque sensor in the wrist. Even with some misalignment, given the current peg length there is no danger of it slipping out of the steering wheel plane during motion."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo\u2019s left arm joint encoder values and forward kinematics are used to derive @panad, Which with a known initial steering wheel angle yields msieer.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "788dc869-d6aa-4ab6-ace3-c8ba84c86628": {"__data__": {"id_": "788dc869-d6aa-4ab6-ace3-c8ba84c86628", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "which might tend to amplify the turning rate. DRC-Hubo\u2019s left arm joint encoder values and forward kinematics are used to derive @panad, Which with a known initial steering wheel angle yields msieer. This can be refined somewhat by sensing contact between the peg and spoke using a force-torque sensor in the wrist. Even with some misalignment, given the current peg length there is no danger of it slipping out of the steering wheel plane during motion. The robot affects the vehicle speed solely through varying pressure on the accelerator pedal by changing its left ankle pitch joint angle Or4P as shown in Fig 7(b), where 0, 4p = 0 indicates that the foot and lower leg form a 90\u00b0 angle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This can be refined somewhat by sensing contact between the peg and spoke using a force-torque sensor in the wrist.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4894eab3-11c3-47be-a904-d8d576752c2c": {"__data__": {"id_": "4894eab3-11c3-47be-a904-d8d576752c2c", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "DRC-Hubo\u2019s left arm joint encoder values and forward kinematics are used to derive @panad, Which with a known initial steering wheel angle yields msieer. This can be refined somewhat by sensing contact between the peg and spoke using a force-torque sensor in the wrist. Even with some misalignment, given the current peg length there is no danger of it slipping out of the steering wheel plane during motion. The robot affects the vehicle speed solely through varying pressure on the accelerator pedal by changing its left ankle pitch joint angle Or4P as shown in Fig 7(b), where 0, 4p = 0 indicates that the foot and lower leg form a 90\u00b0 angle. At the low speeds we have driven (< 2.5 m/s), a range of Or ap \u20ac [\u201435\u00b0, 5\u00b0] suffices (|6r4P|max = 400\u00b0/s)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even with some misalignment, given the current peg length there is no danger of it slipping out of the steering wheel plane during motion.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf06a922-5712-4070-93c8-01929b0ac542": {"__data__": {"id_": "cf06a922-5712-4070-93c8-01929b0ac542", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "This can be refined somewhat by sensing contact between the peg and spoke using a force-torque sensor in the wrist. Even with some misalignment, given the current peg length there is no danger of it slipping out of the steering wheel plane during motion. The robot affects the vehicle speed solely through varying pressure on the accelerator pedal by changing its left ankle pitch joint angle Or4P as shown in Fig 7(b), where 0, 4p = 0 indicates that the foot and lower leg form a 90\u00b0 angle. At the low speeds we have driven (< 2.5 m/s), a range of Or ap \u20ac [\u201435\u00b0, 5\u00b0] suffices (|6r4P|max = 400\u00b0/s). At Onin, the foot is not contacting the accelerator pedal and thus is designated the stop angle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot affects the vehicle speed solely through varying pressure on the accelerator pedal by changing its left ankle pitch joint angle Or4P as shown in Fig 7(b), where 0, 4p = 0 indicates that the foot and lower leg form a 90\u00b0 angle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b45d38aa-bb2c-496c-99ef-f8457871658f": {"__data__": {"id_": "b45d38aa-bb2c-496c-99ef-f8457871658f", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Even with some misalignment, given the current peg length there is no danger of it slipping out of the steering wheel plane during motion. The robot affects the vehicle speed solely through varying pressure on the accelerator pedal by changing its left ankle pitch joint angle Or4P as shown in Fig 7(b), where 0, 4p = 0 indicates that the foot and lower leg form a 90\u00b0 angle. At the low speeds we have driven (< 2.5 m/s), a range of Or ap \u20ac [\u201435\u00b0, 5\u00b0] suffices (|6r4P|max = 400\u00b0/s). At Onin, the foot is not contacting the accelerator pedal and thus is designated the stop angle. This \u201cstopping by deceleration\u201d strategy is adequate because all vehicles in V coast to a stop in a very short distance at such low speeds, even while still in drive in the case of the Polaris."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the low speeds we have driven (< 2.5 m/s), a range of Or ap \u20ac [\u201435\u00b0, 5\u00b0] suffices (|6r4P|max = 400\u00b0/s).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9bd22418-6b1c-47f4-8d08-28ef7c53fc93": {"__data__": {"id_": "9bd22418-6b1c-47f4-8d08-28ef7c53fc93", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The robot affects the vehicle speed solely through varying pressure on the accelerator pedal by changing its left ankle pitch joint angle Or4P as shown in Fig 7(b), where 0, 4p = 0 indicates that the foot and lower leg form a 90\u00b0 angle. At the low speeds we have driven (< 2.5 m/s), a range of Or ap \u20ac [\u201435\u00b0, 5\u00b0] suffices (|6r4P|max = 400\u00b0/s). At Onin, the foot is not contacting the accelerator pedal and thus is designated the stop angle. This \u201cstopping by deceleration\u201d strategy is adequate because all vehicles in V coast to a stop in a very short distance at such low speeds, even while still in drive in the case of the Polaris. It is only a problem if the vehicle tries to stop on a slope, which we have avoided in testing."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At Onin, the foot is not contacting the accelerator pedal and thus is designated the stop angle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6ab48ef-d664-4c32-9e95-765106d49757": {"__data__": {"id_": "e6ab48ef-d664-4c32-9e95-765106d49757", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "At the low speeds we have driven (< 2.5 m/s), a range of Or ap \u20ac [\u201435\u00b0, 5\u00b0] suffices (|6r4P|max = 400\u00b0/s). At Onin, the foot is not contacting the accelerator pedal and thus is designated the stop angle. This \u201cstopping by deceleration\u201d strategy is adequate because all vehicles in V coast to a stop in a very short distance at such low speeds, even while still in drive in the case of the Polaris. It is only a problem if the vehicle tries to stop on a slope, which we have avoided in testing. the hip yaw required to translate the left foot over to the brake pedal would violate joint limits on DRC-Hubo."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This \u201cstopping by deceleration\u201d strategy is adequate because all vehicles in V coast to a stop in a very short distance at such low speeds, even while still in drive in the case of the Polaris.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ca78c9d-1da0-452c-988f-e7e66b316c43": {"__data__": {"id_": "4ca78c9d-1da0-452c-988f-e7e66b316c43", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "At Onin, the foot is not contacting the accelerator pedal and thus is designated the stop angle. This \u201cstopping by deceleration\u201d strategy is adequate because all vehicles in V coast to a stop in a very short distance at such low speeds, even while still in drive in the case of the Polaris. It is only a problem if the vehicle tries to stop on a slope, which we have avoided in testing. the hip yaw required to translate the left foot over to the brake pedal would violate joint limits on DRC-Hubo. A two-footed approach (left on brake, right on accelerator) is possible, but would require a sitting position that is (a) less stable because of the narrower"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is only a problem if the vehicle tries to stop on a slope, which we have avoided in testing.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "870976d9-1def-4482-83b9-a22f0aaa212e": {"__data__": {"id_": "870976d9-1def-4482-83b9-a22f0aaa212e", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "This \u201cstopping by deceleration\u201d strategy is adequate because all vehicles in V coast to a stop in a very short distance at such low speeds, even while still in drive in the case of the Polaris. It is only a problem if the vehicle tries to stop on a slope, which we have avoided in testing. the hip yaw required to translate the left foot over to the brake pedal would violate joint limits on DRC-Hubo. A two-footed approach (left on brake, right on accelerator) is possible, but would require a sitting position that is (a) less stable because of the narrower The estimated current vehicle velocity v(t) is obtained through stereo visual odometry , ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the hip yaw required to translate the left foot over to the brake pedal would violate joint limits on DRC-Hubo.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "910fe6ff-0263-4c65-ba79-8d21492f4d4f": {"__data__": {"id_": "910fe6ff-0263-4c65-ba79-8d21492f4d4f", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "It is only a problem if the vehicle tries to stop on a slope, which we have avoided in testing. the hip yaw required to translate the left foot over to the brake pedal would violate joint limits on DRC-Hubo. A two-footed approach (left on brake, right on accelerator) is possible, but would require a sitting position that is (a) less stable because of the narrower The estimated current vehicle velocity v(t) is obtained through stereo visual odometry , . Specifically, we use synchronized images from the sensor head\u2019s 18-cm baseline stereo camera pair taken at 15 Hz to compute a frame-to-frame rigid transform with the libviso2 library , constrain it to planar motion, and transform it into the vehicle_driving frame."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A two-footed approach (left on brake, right on accelerator) is possible, but would require a sitting position that is (a) less stable because of the narrower", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ab728cd-d07c-4ef0-80ea-a47c7abc688b": {"__data__": {"id_": "9ab728cd-d07c-4ef0-80ea-a47c7abc688b", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "the hip yaw required to translate the left foot over to the brake pedal would violate joint limits on DRC-Hubo. A two-footed approach (left on brake, right on accelerator) is possible, but would require a sitting position that is (a) less stable because of the narrower The estimated current vehicle velocity v(t) is obtained through stereo visual odometry , . Specifically, we use synchronized images from the sensor head\u2019s 18-cm baseline stereo camera pair taken at 15 Hz to compute a frame-to-frame rigid transform with the libviso2 library , constrain it to planar motion, and transform it into the vehicle_driving frame. Given a target speed v\u2019\"9\u00b0t, the difference between it and the current forward velocity component v!7\"9*!"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The estimated current vehicle velocity v(t) is obtained through stereo visual odometry , .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0f24310-b527-428e-9b5e-8fea01aaf08d": {"__data__": {"id_": "e0f24310-b527-428e-9b5e-8fea01aaf08d", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "A two-footed approach (left on brake, right on accelerator) is possible, but would require a sitting position that is (a) less stable because of the narrower The estimated current vehicle velocity v(t) is obtained through stereo visual odometry , . Specifically, we use synchronized images from the sensor head\u2019s 18-cm baseline stereo camera pair taken at 15 Hz to compute a frame-to-frame rigid transform with the libviso2 library , constrain it to planar motion, and transform it into the vehicle_driving frame. Given a target speed v\u2019\"9\u00b0t, the difference between it and the current forward velocity component v!7\"9*! \u2014\u00a5,,(t) is computed and low-pass filtered as \u20ac(t) to get a new pedal command:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Specifically, we use synchronized images from the sensor head\u2019s 18-cm baseline stereo camera pair taken at 15 Hz to compute a frame-to-frame rigid transform with the libviso2 library , constrain it to planar motion, and transform it into the vehicle_driving frame.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9b30c28-545f-4a11-a698-207e0fcc3ef7": {"__data__": {"id_": "f9b30c28-545f-4a11-a698-207e0fcc3ef7", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The estimated current vehicle velocity v(t) is obtained through stereo visual odometry , . Specifically, we use synchronized images from the sensor head\u2019s 18-cm baseline stereo camera pair taken at 15 Hz to compute a frame-to-frame rigid transform with the libviso2 library , constrain it to planar motion, and transform it into the vehicle_driving frame. Given a target speed v\u2019\"9\u00b0t, the difference between it and the current forward velocity component v!7\"9*! \u2014\u00a5,,(t) is computed and low-pass filtered as \u20ac(t) to get a new pedal command: a) Stop-to-steer: To remove the concerns from Sec."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given a target speed v\u2019\"9\u00b0t, the difference between it and the current forward velocity component v!7\"9*!", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b427bfcb-7e64-4592-9e42-cd8605e9f5ea": {"__data__": {"id_": "b427bfcb-7e64-4592-9e42-cd8605e9f5ea", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Specifically, we use synchronized images from the sensor head\u2019s 18-cm baseline stereo camera pair taken at 15 Hz to compute a frame-to-frame rigid transform with the libviso2 library , constrain it to planar motion, and transform it into the vehicle_driving frame. Given a target speed v\u2019\"9\u00b0t, the difference between it and the current forward velocity component v!7\"9*! \u2014\u00a5,,(t) is computed and low-pass filtered as \u20ac(t) to get a new pedal command: a) Stop-to-steer: To remove the concerns from Sec. IV- A about the limited turning rate achievable with \\bhanal\u2122\u2122* and the backlash lag 7g, on direction changes, the robot is constrained to only turn the steering wheel while stopped."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2014\u00a5,,(t) is computed and low-pass filtered as \u20ac(t) to get a new pedal command:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63b7e32e-3e71-4f5b-ae8d-6099fb19c4be": {"__data__": {"id_": "63b7e32e-3e71-4f5b-ae8d-6099fb19c4be", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Given a target speed v\u2019\"9\u00b0t, the difference between it and the current forward velocity component v!7\"9*! \u2014\u00a5,,(t) is computed and low-pass filtered as \u20ac(t) to get a new pedal command: a) Stop-to-steer: To remove the concerns from Sec. IV- A about the limited turning rate achievable with \\bhanal\u2122\u2122* and the backlash lag 7g, on direction changes, the robot is constrained to only turn the steering wheel while stopped. This results in the vehicle following a Dubins path consisting of circular arc and straight line segments, which may be the original form of the motion plan or simply an approximation of a more complicated trajectory, subject to a full stop between each segment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a) Stop-to-steer: To remove the concerns from Sec.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7af9eae-680b-4350-b5cd-0ac616752a4e": {"__data__": {"id_": "a7af9eae-680b-4350-b5cd-0ac616752a4e", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "\u2014\u00a5,,(t) is computed and low-pass filtered as \u20ac(t) to get a new pedal command: a) Stop-to-steer: To remove the concerns from Sec. IV- A about the limited turning rate achievable with \\bhanal\u2122\u2122* and the backlash lag 7g, on direction changes, the robot is constrained to only turn the steering wheel while stopped. This results in the vehicle following a Dubins path consisting of circular arc and straight line segments, which may be the original form of the motion plan or simply an approximation of a more complicated trajectory, subject to a full stop between each segment. Suppose the current step 7 of an /-step plan (see Sec."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IV- A about the limited turning rate achievable with \\bhanal\u2122\u2122* and the backlash lag 7g, on direction changes, the robot is constrained to only turn the steering wheel while stopped.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ae38392-e74c-4f7d-8d91-31e8ccc7d517": {"__data__": {"id_": "3ae38392-e74c-4f7d-8d91-31e8ccc7d517", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "a) Stop-to-steer: To remove the concerns from Sec. IV- A about the limited turning rate achievable with \\bhanal\u2122\u2122* and the backlash lag 7g, on direction changes, the robot is constrained to only turn the steering wheel while stopped. This results in the vehicle following a Dubins path consisting of circular arc and straight line segments, which may be the original form of the motion plan or simply an approximation of a more complicated trajectory, subject to a full stop between each segment. Suppose the current step 7 of an /-step plan (see Sec. IV- D) calls for the stopped vehicle to traverse a segment with curvature Ai and arc length d;."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This results in the vehicle following a Dubins path consisting of circular arc and straight line segments, which may be the original form of the motion plan or simply an approximation of a more complicated trajectory, subject to a full stop between each segment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2cc5d03c-7358-4b2b-98a9-604513671303": {"__data__": {"id_": "2cc5d03c-7358-4b2b-98a9-604513671303", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "IV- A about the limited turning rate achievable with \\bhanal\u2122\u2122* and the backlash lag 7g, on direction changes, the robot is constrained to only turn the steering wheel while stopped. This results in the vehicle following a Dubins path consisting of circular arc and straight line segments, which may be the original form of the motion plan or simply an approximation of a more complicated trajectory, subject to a full stop between each segment. Suppose the current step 7 of an /-step plan (see Sec. IV- D) calls for the stopped vehicle to traverse a segment with curvature Ai and arc length d;. With Ackermann steering, the steering ratio r,, wheelbase w,, and steering wheel backlash 3, of the vehicle imply a steering hand target angle brat = f (Ki, Ty, Wy, By), which is then executed."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Suppose the current step 7 of an /-step plan (see Sec.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b9222ee-558c-42d2-8143-d3ea59607c98": {"__data__": {"id_": "3b9222ee-558c-42d2-8143-d3ea59607c98", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "This results in the vehicle following a Dubins path consisting of circular arc and straight line segments, which may be the original form of the motion plan or simply an approximation of a more complicated trajectory, subject to a full stop between each segment. Suppose the current step 7 of an /-step plan (see Sec. IV- D) calls for the stopped vehicle to traverse a segment with curvature Ai and arc length d;. With Ackermann steering, the steering ratio r,, wheelbase w,, and steering wheel backlash 3, of the vehicle imply a steering hand target angle brat = f (Ki, Ty, Wy, By), which is then executed. For high \u00ab, this may take several seconds to complete."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IV- D) calls for the stopped vehicle to traverse a segment with curvature Ai and arc length d;.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fed68388-0d03-4158-b3b9-9266133e9da3": {"__data__": {"id_": "fed68388-0d03-4158-b3b9-9266133e9da3", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Suppose the current step 7 of an /-step plan (see Sec. IV- D) calls for the stopped vehicle to traverse a segment with curvature Ai and arc length d;. With Ackermann steering, the steering ratio r,, wheelbase w,, and steering wheel backlash 3, of the vehicle imply a steering hand target angle brat = f (Ki, Ty, Wy, By), which is then executed. For high \u00ab, this may take several seconds to complete. viarget is then set (1 m/s is the current default), and the vehicle speed controller will slowly push down the pedal until the vehicle begins to move and reaches its target speed."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With Ackermann steering, the steering ratio r,, wheelbase w,, and steering wheel backlash 3, of the vehicle imply a steering hand target angle brat = f (Ki, Ty, Wy, By), which is then executed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "048449a5-ed5a-4028-8250-10d77b9354bd": {"__data__": {"id_": "048449a5-ed5a-4028-8250-10d77b9354bd", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "IV- D) calls for the stopped vehicle to traverse a segment with curvature Ai and arc length d;. With Ackermann steering, the steering ratio r,, wheelbase w,, and steering wheel backlash 3, of the vehicle imply a steering hand target angle brat = f (Ki, Ty, Wy, By), which is then executed. For high \u00ab, this may take several seconds to complete. viarget is then set (1 m/s is the current default), and the vehicle speed controller will slowly push down the pedal until the vehicle begins to move and reaches its target speed. As the vehicle moves, the forward motion component v,,(t) is integrated until di is exceeded, at which point vi\u00a2\"9\u00ae!"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For high \u00ab, this may take several seconds to complete.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "604ccbee-f179-47b5-ac0c-c9e8f5311933": {"__data__": {"id_": "604ccbee-f179-47b5-ac0c-c9e8f5311933", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "With Ackermann steering, the steering ratio r,, wheelbase w,, and steering wheel backlash 3, of the vehicle imply a steering hand target angle brat = f (Ki, Ty, Wy, By), which is then executed. For high \u00ab, this may take several seconds to complete. viarget is then set (1 m/s is the current default), and the vehicle speed controller will slowly push down the pedal until the vehicle begins to move and reaches its target speed. As the vehicle moves, the forward motion component v,,(t) is integrated until di is exceeded, at which point vi\u00a2\"9\u00ae! is set to 0 and the next plan step +1 is obtained."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "viarget is then set (1 m/s is the current default), and the vehicle speed controller will slowly push down the pedal until the vehicle begins to move and reaches its target speed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "453475c1-22ab-4079-9fd7-698e6aa1f40b": {"__data__": {"id_": "453475c1-22ab-4079-9fd7-698e6aa1f40b", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "For high \u00ab, this may take several seconds to complete. viarget is then set (1 m/s is the current default), and the vehicle speed controller will slowly push down the pedal until the vehicle begins to move and reaches its target speed. As the vehicle moves, the forward motion component v,,(t) is integrated until di is exceeded, at which point vi\u00a2\"9\u00ae! is set to 0 and the next plan step +1 is obtained. If at any time during motion an imminent collision is detected because of dynamic obstacles or deviation from the planned trajectory, motion is also halted."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the vehicle moves, the forward motion component v,,(t) is integrated until di is exceeded, at which point vi\u00a2\"9\u00ae!", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d26a983-7037-4c0f-a42f-9f7419a3419d": {"__data__": {"id_": "9d26a983-7037-4c0f-a42f-9f7419a3419d", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "viarget is then set (1 m/s is the current default), and the vehicle speed controller will slowly push down the pedal until the vehicle begins to move and reaches its target speed. As the vehicle moves, the forward motion component v,,(t) is integrated until di is exceeded, at which point vi\u00a2\"9\u00ae! is set to 0 and the next plan step +1 is obtained. If at any time during motion an imminent collision is detected because of dynamic obstacles or deviation from the planned trajectory, motion is also halted. b) Continuous trajectory following: In this mode the robot attempts to follow an arbitrary continuous trajectory without stopping completely."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "is set to 0 and the next plan step +1 is obtained.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f8b43b7-d4ba-4216-b925-ec9c5e18cdd8": {"__data__": {"id_": "7f8b43b7-d4ba-4216-b925-ec9c5e18cdd8", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "As the vehicle moves, the forward motion component v,,(t) is integrated until di is exceeded, at which point vi\u00a2\"9\u00ae! is set to 0 and the next plan step +1 is obtained. If at any time during motion an imminent collision is detected because of dynamic obstacles or deviation from the planned trajectory, motion is also halted. b) Continuous trajectory following: In this mode the robot attempts to follow an arbitrary continuous trajectory without stopping completely. We use a version of the cross- track error steering controller from ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If at any time during motion an imminent collision is detected because of dynamic obstacles or deviation from the planned trajectory, motion is also halted.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cca1a4c7-5523-4cbf-b7f4-10c30536e55f": {"__data__": {"id_": "cca1a4c7-5523-4cbf-b7f4-10c30536e55f", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "is set to 0 and the next plan step +1 is obtained. If at any time during motion an imminent collision is detected because of dynamic obstacles or deviation from the planned trajectory, motion is also halted. b) Continuous trajectory following: In this mode the robot attempts to follow an arbitrary continuous trajectory without stopping completely. We use a version of the cross- track error steering controller from . Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "b) Continuous trajectory following: In this mode the robot attempts to follow an arbitrary continuous trajectory without stopping completely.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f51afaff-8205-4a60-8242-e7f60ecb32a1": {"__data__": {"id_": "f51afaff-8205-4a60-8242-e7f60ecb32a1", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "If at any time during motion an imminent collision is detected because of dynamic obstacles or deviation from the planned trajectory, motion is also halted. b) Continuous trajectory following: In this mode the robot attempts to follow an arbitrary continuous trajectory without stopping completely. We use a version of the cross- track error steering controller from . Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. where W(t) is the difference between the tangent to the trajectory and the current vehicle heading, y(t) is the lateral error between the vehicle centerline and the trajectory itself, and k is a gain factor that governs how sharply the vehicle attempts to return to the trajectory curve."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use a version of the cross- track error steering controller from .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6c3e174-7c16-4d87-8e5f-fcd8243a3214": {"__data__": {"id_": "c6c3e174-7c16-4d87-8e5f-fcd8243a3214", "embedding": null, "metadata": {"page_number": 5, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "b) Continuous trajectory following: In this mode the robot attempts to follow an arbitrary continuous trajectory without stopping completely. We use a version of the cross- track error steering controller from . Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. where W(t) is the difference between the tangent to the trajectory and the current vehicle heading, y(t) is the lateral error between the vehicle centerline and the trajectory itself, and k is a gain factor that governs how sharply the vehicle attempts to return to the trajectory curve. The steering command that is generated based on the difference between the actual @sieer and the target one is damped to account for the delay while the robot\u2019s hand moves, and the target vehicle speed is reduced in proportion to the turn magnitude."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc30f599-d922-4d87-81c1-2b2ebbbe6b10": {"__data__": {"id_": "cc30f599-d922-4d87-81c1-2b2ebbbe6b10", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "We use a version of the cross- track error steering controller from . Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. where W(t) is the difference between the tangent to the trajectory and the current vehicle heading, y(t) is the lateral error between the vehicle centerline and the trajectory itself, and k is a gain factor that governs how sharply the vehicle attempts to return to the trajectory curve. The steering command that is generated based on the difference between the actual @sieer and the target one is damped to account for the delay while the robot\u2019s hand moves, and the target vehicle speed is reduced in proportion to the turn magnitude. but for this prototype we follow and other early DGC participants and simply use the robot sensor head\u2019s ladar in a sweep configuration as its chief obstacle detector."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where W(t) is the difference between the tangent to the trajectory and the current vehicle heading, y(t) is the lateral error between the vehicle centerline and the trajectory itself, and k is a gain factor that governs how sharply the vehicle attempts to return to the trajectory curve.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4a02a88-3804-44d6-a57e-bfe35fb02f38": {"__data__": {"id_": "a4a02a88-3804-44d6-a57e-bfe35fb02f38", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. where W(t) is the difference between the tangent to the trajectory and the current vehicle heading, y(t) is the lateral error between the vehicle centerline and the trajectory itself, and k is a gain factor that governs how sharply the vehicle attempts to return to the trajectory curve. The steering command that is generated based on the difference between the actual @sieer and the target one is damped to account for the delay while the robot\u2019s hand moves, and the target vehicle speed is reduced in proportion to the turn magnitude. but for this prototype we follow and other early DGC participants and simply use the robot sensor head\u2019s ladar in a sweep configuration as its chief obstacle detector. When not moving, the robot can obtain a point cloud of the scene outside the vehicle with a single ladar sweep and detect obstacles as outliers to a planar ground fit."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The steering command that is generated based on the difference between the actual @sieer and the target one is damped to account for the delay while the robot\u2019s hand moves, and the target vehicle speed is reduced in proportion to the turn magnitude.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c9629ff-7849-47d8-b660-2e2d8dc78fff": {"__data__": {"id_": "2c9629ff-7849-47d8-b660-2e2d8dc78fff", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "where W(t) is the difference between the tangent to the trajectory and the current vehicle heading, y(t) is the lateral error between the vehicle centerline and the trajectory itself, and k is a gain factor that governs how sharply the vehicle attempts to return to the trajectory curve. The steering command that is generated based on the difference between the actual @sieer and the target one is damped to account for the delay while the robot\u2019s hand moves, and the target vehicle speed is reduced in proportion to the turn magnitude. but for this prototype we follow and other early DGC participants and simply use the robot sensor head\u2019s ladar in a sweep configuration as its chief obstacle detector. When not moving, the robot can obtain a point cloud of the scene outside the vehicle with a single ladar sweep and detect obstacles as outliers to a planar ground fit. ladar scan points from the point cloud are inserted into a 3-D occupancy grid\u2014here we use OctoMap with a minimum cell size of 0.2 m. The map is rolling\u2014we are only concerned with what is immediately in front of and around the robot, rather than trying to build a full map of the environment it is traveling through\u2014so the occupancy grid is roughly 10 m on a side, centered forward of the vehicle_driving origin."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "but for this prototype we follow and other early DGC participants and simply use the robot sensor head\u2019s ladar in a sweep configuration as its chief obstacle detector.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b7a60df-ecc6-4e1b-9a1b-8f69fc5afbbd": {"__data__": {"id_": "7b7a60df-ecc6-4e1b-9a1b-8f69fc5afbbd", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The steering command that is generated based on the difference between the actual @sieer and the target one is damped to account for the delay while the robot\u2019s hand moves, and the target vehicle speed is reduced in proportion to the turn magnitude. but for this prototype we follow and other early DGC participants and simply use the robot sensor head\u2019s ladar in a sweep configuration as its chief obstacle detector. When not moving, the robot can obtain a point cloud of the scene outside the vehicle with a single ladar sweep and detect obstacles as outliers to a planar ground fit. ladar scan points from the point cloud are inserted into a 3-D occupancy grid\u2014here we use OctoMap with a minimum cell size of 0.2 m. The map is rolling\u2014we are only concerned with what is immediately in front of and around the robot, rather than trying to build a full map of the environment it is traveling through\u2014so the occupancy grid is roughly 10 m on a side, centered forward of the vehicle_driving origin. Each scan point can be annotated with relevant information such as its intensity or obstacle/free classification before insertion into the occupancy grid."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When not moving, the robot can obtain a point cloud of the scene outside the vehicle with a single ladar sweep and detect obstacles as outliers to a planar ground fit.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f57265bd-f095-4865-b256-6e435c3a1d94": {"__data__": {"id_": "f57265bd-f095-4865-b256-6e435c3a1d94", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "but for this prototype we follow and other early DGC participants and simply use the robot sensor head\u2019s ladar in a sweep configuration as its chief obstacle detector. When not moving, the robot can obtain a point cloud of the scene outside the vehicle with a single ladar sweep and detect obstacles as outliers to a planar ground fit. ladar scan points from the point cloud are inserted into a 3-D occupancy grid\u2014here we use OctoMap with a minimum cell size of 0.2 m. The map is rolling\u2014we are only concerned with what is immediately in front of and around the robot, rather than trying to build a full map of the environment it is traveling through\u2014so the occupancy grid is roughly 10 m on a side, centered forward of the vehicle_driving origin. Each scan point can be annotated with relevant information such as its intensity or obstacle/free classification before insertion into the occupancy grid. In the latter case, this leads to aggregation of multiple observations such that we can get the freespace likelihood for each grid square."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ladar scan points from the point cloud are inserted into a 3-D occupancy grid\u2014here we use OctoMap with a minimum cell size of 0.2 m. The map is rolling\u2014we are only concerned with what is immediately in front of and around the robot, rather than trying to build a full map of the environment it is traveling through\u2014so the occupancy grid is roughly 10 m on a side, centered forward of the vehicle_driving origin.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eef3d8f8-57f7-4d61-bb31-5ded24e2deda": {"__data__": {"id_": "eef3d8f8-57f7-4d61-bb31-5ded24e2deda", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "When not moving, the robot can obtain a point cloud of the scene outside the vehicle with a single ladar sweep and detect obstacles as outliers to a planar ground fit. ladar scan points from the point cloud are inserted into a 3-D occupancy grid\u2014here we use OctoMap with a minimum cell size of 0.2 m. The map is rolling\u2014we are only concerned with what is immediately in front of and around the robot, rather than trying to build a full map of the environment it is traveling through\u2014so the occupancy grid is roughly 10 m on a side, centered forward of the vehicle_driving origin. Each scan point can be annotated with relevant information such as its intensity or obstacle/free classification before insertion into the occupancy grid. In the latter case, this leads to aggregation of multiple observations such that we can get the freespace likelihood for each grid square. The motion estimates furnished by the visual odometry module are crucial in allowing the robot to reason about obstacles it observed previously but can no longer see."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each scan point can be annotated with relevant information such as its intensity or obstacle/free classification before insertion into the occupancy grid.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa478122-32ec-4142-9162-58bc685e1031": {"__data__": {"id_": "fa478122-32ec-4142-9162-58bc685e1031", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "ladar scan points from the point cloud are inserted into a 3-D occupancy grid\u2014here we use OctoMap with a minimum cell size of 0.2 m. The map is rolling\u2014we are only concerned with what is immediately in front of and around the robot, rather than trying to build a full map of the environment it is traveling through\u2014so the occupancy grid is roughly 10 m on a side, centered forward of the vehicle_driving origin. Each scan point can be annotated with relevant information such as its intensity or obstacle/free classification before insertion into the occupancy grid. In the latter case, this leads to aggregation of multiple observations such that we can get the freespace likelihood for each grid square. The motion estimates furnished by the visual odometry module are crucial in allowing the robot to reason about obstacles it observed previously but can no longer see. Projecting the current 3-D occupancy grid down to a 2-D costmap allows the robot to generate a feasible trajectory to a goal pose by calling a path planner such as the Search-Based Planning Library (SBPL) , which is aware of the vehicle\u2019s Ackermann motion constraints."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the latter case, this leads to aggregation of multiple observations such that we can get the freespace likelihood for each grid square.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a97979af-a588-4488-882c-e6842ac8e646": {"__data__": {"id_": "a97979af-a588-4488-882c-e6842ac8e646", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Each scan point can be annotated with relevant information such as its intensity or obstacle/free classification before insertion into the occupancy grid. In the latter case, this leads to aggregation of multiple observations such that we can get the freespace likelihood for each grid square. The motion estimates furnished by the visual odometry module are crucial in allowing the robot to reason about obstacles it observed previously but can no longer see. Projecting the current 3-D occupancy grid down to a 2-D costmap allows the robot to generate a feasible trajectory to a goal pose by calling a path planner such as the Search-Based Planning Library (SBPL) , which is aware of the vehicle\u2019s Ackermann motion constraints. Human operators may also provide high-level plans to the robot by inspecting the current obstacle costmap and supplying Dubins-like segments to be executed via the stop-to-steer method of Sec."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The motion estimates furnished by the visual odometry module are crucial in allowing the robot to reason about obstacles it observed previously but can no longer see.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7cacc65-f415-428f-bd14-9c463d5f2f8f": {"__data__": {"id_": "e7cacc65-f415-428f-bd14-9c463d5f2f8f", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "In the latter case, this leads to aggregation of multiple observations such that we can get the freespace likelihood for each grid square. The motion estimates furnished by the visual odometry module are crucial in allowing the robot to reason about obstacles it observed previously but can no longer see. Projecting the current 3-D occupancy grid down to a 2-D costmap allows the robot to generate a feasible trajectory to a goal pose by calling a path planner such as the Search-Based Planning Library (SBPL) , which is aware of the vehicle\u2019s Ackermann motion constraints. Human operators may also provide high-level plans to the robot by inspecting the current obstacle costmap and supplying Dubins-like segments to be executed via the stop-to-steer method of Sec. A final perception and planning mode we have developed is for path- or road-following."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Projecting the current 3-D occupancy grid down to a 2-D costmap allows the robot to generate a feasible trajectory to a goal pose by calling a path planner such as the Search-Based Planning Library (SBPL) , which is aware of the vehicle\u2019s Ackermann motion constraints.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50f6d20c-f310-4ea7-a8ef-e7e9537c9ced": {"__data__": {"id_": "50f6d20c-f310-4ea7-a8ef-e7e9537c9ced", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The motion estimates furnished by the visual odometry module are crucial in allowing the robot to reason about obstacles it observed previously but can no longer see. Projecting the current 3-D occupancy grid down to a 2-D costmap allows the robot to generate a feasible trajectory to a goal pose by calling a path planner such as the Search-Based Planning Library (SBPL) , which is aware of the vehicle\u2019s Ackermann motion constraints. Human operators may also provide high-level plans to the robot by inspecting the current obstacle costmap and supplying Dubins-like segments to be executed via the stop-to-steer method of Sec. A final perception and planning mode we have developed is for path- or road-following. Following our methods for trail-following detailed in , , we can track a path in front of the vehicle by formulating a path likelihood function which measures how well a low-dimensional path shape hypothesis (width, lateral offset, relative heading, and cur- vature) agrees with sensor measurements and continuously optimizing it via particle filtering."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Human operators may also provide high-level plans to the robot by inspecting the current obstacle costmap and supplying Dubins-like segments to be executed via the stop-to-steer method of Sec.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb95c842-46f5-46d1-82c5-4152f7a84997": {"__data__": {"id_": "bb95c842-46f5-46d1-82c5-4152f7a84997", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Projecting the current 3-D occupancy grid down to a 2-D costmap allows the robot to generate a feasible trajectory to a goal pose by calling a path planner such as the Search-Based Planning Library (SBPL) , which is aware of the vehicle\u2019s Ackermann motion constraints. Human operators may also provide high-level plans to the robot by inspecting the current obstacle costmap and supplying Dubins-like segments to be executed via the stop-to-steer method of Sec. A final perception and planning mode we have developed is for path- or road-following. Following our methods for trail-following detailed in , , we can track a path in front of the vehicle by formulating a path likelihood function which measures how well a low-dimensional path shape hypothesis (width, lateral offset, relative heading, and cur- vature) agrees with sensor measurements and continuously optimizing it via particle filtering. The likelihood function used here simply measures the proportion of ground fit inliers to outliers taken from the costmap introduced above, although more sophisticated appearance cues could easily be incorporated."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A final perception and planning mode we have developed is for path- or road-following.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb3222c7-efdc-47de-ae3f-454b32a89ca9": {"__data__": {"id_": "fb3222c7-efdc-47de-ae3f-454b32a89ca9", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Human operators may also provide high-level plans to the robot by inspecting the current obstacle costmap and supplying Dubins-like segments to be executed via the stop-to-steer method of Sec. A final perception and planning mode we have developed is for path- or road-following. Following our methods for trail-following detailed in , , we can track a path in front of the vehicle by formulating a path likelihood function which measures how well a low-dimensional path shape hypothesis (width, lateral offset, relative heading, and cur- vature) agrees with sensor measurements and continuously optimizing it via particle filtering. The likelihood function used here simply measures the proportion of ground fit inliers to outliers taken from the costmap introduced above, although more sophisticated appearance cues could easily be incorporated. The centerline of the tracked path supplies the cross-track error and path segment orientation needed for the continuous trajectory following method in Sec."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Following our methods for trail-following detailed in , , we can track a path in front of the vehicle by formulating a path likelihood function which measures how well a low-dimensional path shape hypothesis (width, lateral offset, relative heading, and cur- vature) agrees with sensor measurements and continuously optimizing it via particle filtering.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "571cdb73-7846-42d4-a51d-86602250cfe9": {"__data__": {"id_": "571cdb73-7846-42d4-a51d-86602250cfe9", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "A final perception and planning mode we have developed is for path- or road-following. Following our methods for trail-following detailed in , , we can track a path in front of the vehicle by formulating a path likelihood function which measures how well a low-dimensional path shape hypothesis (width, lateral offset, relative heading, and cur- vature) agrees with sensor measurements and continuously optimizing it via particle filtering. The likelihood function used here simply measures the proportion of ground fit inliers to outliers taken from the costmap introduced above, although more sophisticated appearance cues could easily be incorporated. The centerline of the tracked path supplies the cross-track error and path segment orientation needed for the continuous trajectory following method in Sec. Dashboard point cloud registration and sensor head pose estimation results are shown for the Club Car in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The likelihood function used here simply measures the proportion of ground fit inliers to outliers taken from the costmap introduced above, although more sophisticated appearance cues could easily be incorporated.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d192ee94-a50e-4a0a-baa1-089be360b61e": {"__data__": {"id_": "d192ee94-a50e-4a0a-baa1-089be360b61e", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Following our methods for trail-following detailed in , , we can track a path in front of the vehicle by formulating a path likelihood function which measures how well a low-dimensional path shape hypothesis (width, lateral offset, relative heading, and cur- vature) agrees with sensor measurements and continuously optimizing it via particle filtering. The likelihood function used here simply measures the proportion of ground fit inliers to outliers taken from the costmap introduced above, although more sophisticated appearance cues could easily be incorporated. The centerline of the tracked path supplies the cross-track error and path segment orientation needed for the continuous trajectory following method in Sec. Dashboard point cloud registration and sensor head pose estimation results are shown for the Club Car in Fig. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The centerline of the tracked path supplies the cross-track error and path segment orientation needed for the continuous trajectory following method in Sec.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acea8140-f6de-477d-8176-a946e6c14a08": {"__data__": {"id_": "acea8140-f6de-477d-8176-a946e6c14a08", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The likelihood function used here simply measures the proportion of ground fit inliers to outliers taken from the costmap introduced above, although more sophisticated appearance cues could easily be incorporated. The centerline of the tracked path supplies the cross-track error and path segment orientation needed for the continuous trajectory following method in Sec. Dashboard point cloud registration and sensor head pose estimation results are shown for the Club Car in Fig. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Scans were captured from about 10 different locations for each vehicle along the driver-passenger seat, with some slightly outside the envelope of the vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dashboard point cloud registration and sensor head pose estimation results are shown for the Club Car in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f89473e1-eb77-465e-848e-56496b77dbb6": {"__data__": {"id_": "f89473e1-eb77-465e-848e-56496b77dbb6", "embedding": null, "metadata": {"page_number": 6, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The centerline of the tracked path supplies the cross-track error and path segment orientation needed for the continuous trajectory following method in Sec. Dashboard point cloud registration and sensor head pose estimation results are shown for the Club Car in Fig. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Scans were captured from about 10 different locations for each vehicle along the driver-passenger seat, with some slightly outside the envelope of the vehicle. Some poses were quite close together or varied only by orientation, so for clarity we have omitted these."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8203ba5-86df-47f5-9785-620c0c5926b8": {"__data__": {"id_": "f8203ba5-86df-47f5-9785-620c0c5926b8", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Dashboard point cloud registration and sensor head pose estimation results are shown for the Club Car in Fig. Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Scans were captured from about 10 different locations for each vehicle along the driver-passenger seat, with some slightly outside the envelope of the vehicle. Some poses were quite close together or varied only by orientation, so for clarity we have omitted these. On an Intel Core i7-3930K processor at 3.8 GHz with 64 Gb of RAM, the mean combined time of the FPFH and ICP stages of dashboard point cloud registration for the Club Car and Polaris data is about 5 seconds."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Scans were captured from about 10 different locations for each vehicle along the driver-passenger seat, with some slightly outside the envelope of the vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a553f9ca-c533-47ed-aaa5-9eae015a2073": {"__data__": {"id_": "a553f9ca-c533-47ed-aaa5-9eae015a2073", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Downloaded on May 08,2025 at 10:50:42 UTC from IEEE Xplore. Scans were captured from about 10 different locations for each vehicle along the driver-passenger seat, with some slightly outside the envelope of the vehicle. Some poses were quite close together or varied only by orientation, so for clarity we have omitted these. On an Intel Core i7-3930K processor at 3.8 GHz with 64 Gb of RAM, the mean combined time of the FPFH and ICP stages of dashboard point cloud registration for the Club Car and Polaris data is about 5 seconds. This operation only needs to be run a few times during interfacing, so the wait is not unreasonable."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some poses were quite close together or varied only by orientation, so for clarity we have omitted these.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d3b8054-119c-4fb9-804c-66d996f5c156": {"__data__": {"id_": "9d3b8054-119c-4fb9-804c-66d996f5c156", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Scans were captured from about 10 different locations for each vehicle along the driver-passenger seat, with some slightly outside the envelope of the vehicle. Some poses were quite close together or varied only by orientation, so for clarity we have omitted these. On an Intel Core i7-3930K processor at 3.8 GHz with 64 Gb of RAM, the mean combined time of the FPFH and ICP stages of dashboard point cloud registration for the Club Car and Polaris data is about 5 seconds. This operation only needs to be run a few times during interfacing, so the wait is not unreasonable. and for that exception there was a roll error of about 10\u00b0."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On an Intel Core i7-3930K processor at 3.8 GHz with 64 Gb of RAM, the mean combined time of the FPFH and ICP stages of dashboard point cloud registration for the Club Car and Polaris data is about 5 seconds.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a64aba1-b43b-4e69-83ac-2217c2f5390d": {"__data__": {"id_": "8a64aba1-b43b-4e69-83ac-2217c2f5390d", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Some poses were quite close together or varied only by orientation, so for clarity we have omitted these. On an Intel Core i7-3930K processor at 3.8 GHz with 64 Gb of RAM, the mean combined time of the FPFH and ICP stages of dashboard point cloud registration for the Club Car and Polaris data is about 5 seconds. This operation only needs to be run a few times during interfacing, so the wait is not unreasonable. and for that exception there was a roll error of about 10\u00b0. Because of the near-planarity of the Polaris dashboard refer- ence cloud, there were some ambiguities due to symmetries that were eliminated by enforcing some mild constraints on the sensor orientation and position."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This operation only needs to be run a few times during interfacing, so the wait is not unreasonable.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be388ea6-f290-4e47-a484-c3b084d24f56": {"__data__": {"id_": "be388ea6-f290-4e47-a484-c3b084d24f56", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "On an Intel Core i7-3930K processor at 3.8 GHz with 64 Gb of RAM, the mean combined time of the FPFH and ICP stages of dashboard point cloud registration for the Club Car and Polaris data is about 5 seconds. This operation only needs to be run a few times during interfacing, so the wait is not unreasonable. and for that exception there was a roll error of about 10\u00b0. Because of the near-planarity of the Polaris dashboard refer- ence cloud, there were some ambiguities due to symmetries that were eliminated by enforcing some mild constraints on the sensor orientation and position. A sequence from one of many indoor obstacle avoidance tests with DRC-Hubo at the wheel, using human- provided plan segments, is shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and for that exception there was a roll error of about 10\u00b0.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "525316b8-d3a2-4784-999f-20efcad1dffd": {"__data__": {"id_": "525316b8-d3a2-4784-999f-20efcad1dffd", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "This operation only needs to be run a few times during interfacing, so the wait is not unreasonable. and for that exception there was a roll error of about 10\u00b0. Because of the near-planarity of the Polaris dashboard refer- ence cloud, there were some ambiguities due to symmetries that were eliminated by enforcing some mild constraints on the sensor orientation and position. A sequence from one of many indoor obstacle avoidance tests with DRC-Hubo at the wheel, using human- provided plan segments, is shown in Fig. IV- D have been demonstrated in an integrated fashion in simulation (using ground-truth odometry)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because of the near-planarity of the Polaris dashboard refer- ence cloud, there were some ambiguities due to symmetries that were eliminated by enforcing some mild constraints on the sensor orientation and position.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7867b23d-7c43-4a11-a119-3a385e296338": {"__data__": {"id_": "7867b23d-7c43-4a11-a119-3a385e296338", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "and for that exception there was a roll error of about 10\u00b0. Because of the near-planarity of the Polaris dashboard refer- ence cloud, there were some ambiguities due to symmetries that were eliminated by enforcing some mild constraints on the sensor orientation and position. A sequence from one of many indoor obstacle avoidance tests with DRC-Hubo at the wheel, using human- provided plan segments, is shown in Fig. IV- D have been demonstrated in an integrated fashion in simulation (using ground-truth odometry). A snapshot of a Gazebo simulation is shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A sequence from one of many indoor obstacle avoidance tests with DRC-Hubo at the wheel, using human- provided plan segments, is shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c6e20e3-26e5-475b-a5c7-56a3fccff15e": {"__data__": {"id_": "4c6e20e3-26e5-475b-a5c7-56a3fccff15e", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Because of the near-planarity of the Polaris dashboard refer- ence cloud, there were some ambiguities due to symmetries that were eliminated by enforcing some mild constraints on the sensor orientation and position. A sequence from one of many indoor obstacle avoidance tests with DRC-Hubo at the wheel, using human- provided plan segments, is shown in Fig. IV- D have been demonstrated in an integrated fashion in simulation (using ground-truth odometry). A snapshot of a Gazebo simulation is shown in Fig. 11(a), with the generated Octomap and planned trajectory several frames later shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IV- D have been demonstrated in an integrated fashion in simulation (using ground-truth odometry).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f35fc6cb-c645-4a7a-8b48-fa299a8673b5": {"__data__": {"id_": "f35fc6cb-c645-4a7a-8b48-fa299a8673b5", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "A sequence from one of many indoor obstacle avoidance tests with DRC-Hubo at the wheel, using human- provided plan segments, is shown in Fig. IV- D have been demonstrated in an integrated fashion in simulation (using ground-truth odometry). A snapshot of a Gazebo simulation is shown in Fig. 11(a), with the generated Octomap and planned trajectory several frames later shown in Fig. The perception and path tracking algorithms have further been validated offline using data collected with the sensor head while driving manually 5+ km around campus and golf course testing areas (samples pictured in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A snapshot of a Gazebo simulation is shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82b9c6c7-1985-4bd6-81c1-3cccfddadb88": {"__data__": {"id_": "82b9c6c7-1985-4bd6-81c1-3cccfddadb88", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "IV- D have been demonstrated in an integrated fashion in simulation (using ground-truth odometry). A snapshot of a Gazebo simulation is shown in Fig. 11(a), with the generated Octomap and planned trajectory several frames later shown in Fig. The perception and path tracking algorithms have further been validated offline using data collected with the sensor head while driving manually 5+ km around campus and golf course testing areas (samples pictured in Fig. Steps of the pipeline from an area where the vehicle was driven through a 3-point turn are represented in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11(a), with the generated Octomap and planned trajectory several frames later shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92871670-81c2-4ed7-8537-c1199f8dc0a7": {"__data__": {"id_": "92871670-81c2-4ed7-8537-c1199f8dc0a7", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "A snapshot of a Gazebo simulation is shown in Fig. 11(a), with the generated Octomap and planned trajectory several frames later shown in Fig. The perception and path tracking algorithms have further been validated offline using data collected with the sensor head while driving manually 5+ km around campus and golf course testing areas (samples pictured in Fig. Steps of the pipeline from an area where the vehicle was driven through a 3-point turn are represented in Fig. The two left images show the scene and its corresponding point cloud, while the two right images show the ground/obstacle segmentation and height-colored Octomap of the point cloud, with the visual odometry- derived motion estimate of the maneuver overlaid."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The perception and path tracking algorithms have further been validated offline using data collected with the sensor head while driving manually 5+ km around campus and golf course testing areas (samples pictured in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b0cfc57-2ee3-4ac8-9169-0c4a3c7e02ba": {"__data__": {"id_": "3b0cfc57-2ee3-4ac8-9169-0c4a3c7e02ba", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "11(a), with the generated Octomap and planned trajectory several frames later shown in Fig. The perception and path tracking algorithms have further been validated offline using data collected with the sensor head while driving manually 5+ km around campus and golf course testing areas (samples pictured in Fig. Steps of the pipeline from an area where the vehicle was driven through a 3-point turn are represented in Fig. The two left images show the scene and its corresponding point cloud, while the two right images show the ground/obstacle segmentation and height-colored Octomap of the point cloud, with the visual odometry- derived motion estimate of the maneuver overlaid. but part detection and tracking inside the car could help speed pose estimation and allow the reading of visual indicators like the speedometer and gear state."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Steps of the pipeline from an area where the vehicle was driven through a 3-point turn are represented in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "310ac328-33f6-4167-a0a0-acc3e45618cb": {"__data__": {"id_": "310ac328-33f6-4167-a0a0-acc3e45618cb", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The perception and path tracking algorithms have further been validated offline using data collected with the sensor head while driving manually 5+ km around campus and golf course testing areas (samples pictured in Fig. Steps of the pipeline from an area where the vehicle was driven through a 3-point turn are represented in Fig. The two left images show the scene and its corresponding point cloud, while the two right images show the ground/obstacle segmentation and height-colored Octomap of the point cloud, with the visual odometry- derived motion estimate of the maneuver overlaid. but part detection and tracking inside the car could help speed pose estimation and allow the reading of visual indicators like the speedometer and gear state. including turning the vehicle on/off with a key or switch and gear shifting to allow reverse maneuvers in the motion planner."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two left images show the scene and its corresponding point cloud, while the two right images show the ground/obstacle segmentation and height-colored Octomap of the point cloud, with the visual odometry- derived motion estimate of the maneuver overlaid.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54b67631-f5a9-44a4-a73a-e07e2b549190": {"__data__": {"id_": "54b67631-f5a9-44a4-a73a-e07e2b549190", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "Steps of the pipeline from an area where the vehicle was driven through a 3-point turn are represented in Fig. The two left images show the scene and its corresponding point cloud, while the two right images show the ground/obstacle segmentation and height-colored Octomap of the point cloud, with the visual odometry- derived motion estimate of the maneuver overlaid. but part detection and tracking inside the car could help speed pose estimation and allow the reading of visual indicators like the speedometer and gear state. including turning the vehicle on/off with a key or switch and gear shifting to allow reverse maneuvers in the motion planner. Both of these would require more dextrous manipulation, including force feedback, and visual analysis for hand-eye coordination."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "but part detection and tracking inside the car could help speed pose estimation and allow the reading of visual indicators like the speedometer and gear state.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15d2b3cb-1bed-4a17-9e80-8d72d5117695": {"__data__": {"id_": "15d2b3cb-1bed-4a17-9e80-8d72d5117695", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "The two left images show the scene and its corresponding point cloud, while the two right images show the ground/obstacle segmentation and height-colored Octomap of the point cloud, with the visual odometry- derived motion estimate of the maneuver overlaid. but part detection and tracking inside the car could help speed pose estimation and allow the reading of visual indicators like the speedometer and gear state. including turning the vehicle on/off with a key or switch and gear shifting to allow reverse maneuvers in the motion planner. Both of these would require more dextrous manipulation, including force feedback, and visual analysis for hand-eye coordination."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "including turning the vehicle on/off with a key or switch and gear shifting to allow reverse maneuvers in the motion planner.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7348f286-aac9-4e72-bdc9-180bf667e6c5": {"__data__": {"id_": "7348f286-aac9-4e72-bdc9-180bf667e6c5", "embedding": null, "metadata": {"page_number": 7, "source": "Perception_and_control_strategies_for_driving_utility_vehicles_with_a_humanoid_robot.pdf", "window": "but part detection and tracking inside the car could help speed pose estimation and allow the reading of visual indicators like the speedometer and gear state. including turning the vehicle on/off with a key or switch and gear shifting to allow reverse maneuvers in the motion planner. Both of these would require more dextrous manipulation, including force feedback, and visual analysis for hand-eye coordination."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both of these would require more dextrous manipulation, including force feedback, and visual analysis for hand-eye coordination.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d24a2c54-6d7f-4ff7-b61b-868608d64163": {"__data__": {"id_": "d24a2c54-6d7f-4ff7-b61b-868608d64163", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Received: 18 January 2019 / Accepted: 2 December 2019 / Published online: 27 December 2019 \u00a9 Springer Nature B.V. 2019 This paper presents a technical overview of the Team DRC-Hubo@UNLV\u2019s approach to the driving task in DARPA Robotics Challenge Finals (DRC-Finals). First, hardware updates in the main body and the perception head of the contestant robot, DRC-Hubo+, were presented by comparison with the previous platform (DRC-Hubo in DRC-Trials)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Received: 18 January 2019 / Accepted: 2 December 2019 / Published online: 27 December 2019 \u00a9 Springer Nature B.V. 2019", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ab7ea70-e095-4b30-880a-6c6c63d2f041": {"__data__": {"id_": "6ab7ea70-e095-4b30-880a-6c6c63d2f041", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Received: 18 January 2019 / Accepted: 2 December 2019 / Published online: 27 December 2019 \u00a9 Springer Nature B.V. 2019 This paper presents a technical overview of the Team DRC-Hubo@UNLV\u2019s approach to the driving task in DARPA Robotics Challenge Finals (DRC-Finals). First, hardware updates in the main body and the perception head of the contestant robot, DRC-Hubo+, were presented by comparison with the previous platform (DRC-Hubo in DRC-Trials). Then, the control system which enabled the full-sized humanoid to drive the off-the-shelf utility vehicle in the competition was provided."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This paper presents a technical overview of the Team DRC-Hubo@UNLV\u2019s approach to the driving task in DARPA Robotics Challenge Finals (DRC-Finals).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "797252a2-808e-46d5-84d7-4da42069113b": {"__data__": {"id_": "797252a2-808e-46d5-84d7-4da42069113b", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Received: 18 January 2019 / Accepted: 2 December 2019 / Published online: 27 December 2019 \u00a9 Springer Nature B.V. 2019 This paper presents a technical overview of the Team DRC-Hubo@UNLV\u2019s approach to the driving task in DARPA Robotics Challenge Finals (DRC-Finals). First, hardware updates in the main body and the perception head of the contestant robot, DRC-Hubo+, were presented by comparison with the previous platform (DRC-Hubo in DRC-Trials). Then, the control system which enabled the full-sized humanoid to drive the off-the-shelf utility vehicle in the competition was provided. For this, the sensor data fusion process and the advanced driving assistant techniques were emphasized."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, hardware updates in the main body and the perception head of the contestant robot, DRC-Hubo+, were presented by comparison with the previous platform (DRC-Hubo in DRC-Trials).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d86a95d-7932-4aa1-bef4-043f3d100f8b": {"__data__": {"id_": "1d86a95d-7932-4aa1-bef4-043f3d100f8b", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "This paper presents a technical overview of the Team DRC-Hubo@UNLV\u2019s approach to the driving task in DARPA Robotics Challenge Finals (DRC-Finals). First, hardware updates in the main body and the perception head of the contestant robot, DRC-Hubo+, were presented by comparison with the previous platform (DRC-Hubo in DRC-Trials). Then, the control system which enabled the full-sized humanoid to drive the off-the-shelf utility vehicle in the competition was provided. For this, the sensor data fusion process and the advanced driving assistant techniques were emphasized. Next, the driving pattern analysis of other type operators (such as human-drivers or pure tele-operation systems) was provided to demonstrate the performance of the developed system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the control system which enabled the full-sized humanoid to drive the off-the-shelf utility vehicle in the competition was provided.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67e07487-7e89-43fb-ad89-676aa08cf609": {"__data__": {"id_": "67e07487-7e89-43fb-ad89-676aa08cf609", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "First, hardware updates in the main body and the perception head of the contestant robot, DRC-Hubo+, were presented by comparison with the previous platform (DRC-Hubo in DRC-Trials). Then, the control system which enabled the full-sized humanoid to drive the off-the-shelf utility vehicle in the competition was provided. For this, the sensor data fusion process and the advanced driving assistant techniques were emphasized. Next, the driving pattern analysis of other type operators (such as human-drivers or pure tele-operation systems) was provided to demonstrate the performance of the developed system. Lastly, test and evaluation of the built robot and control system was provided via experimentation which DRC-Hubo+ drove the vehicle in a real-world setting."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this, the sensor data fusion process and the advanced driving assistant techniques were emphasized.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49f9aade-bc2b-4a28-8e00-8ab132d641a4": {"__data__": {"id_": "49f9aade-bc2b-4a28-8e00-8ab132d641a4", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Then, the control system which enabled the full-sized humanoid to drive the off-the-shelf utility vehicle in the competition was provided. For this, the sensor data fusion process and the advanced driving assistant techniques were emphasized. Next, the driving pattern analysis of other type operators (such as human-drivers or pure tele-operation systems) was provided to demonstrate the performance of the developed system. Lastly, test and evaluation of the built robot and control system was provided via experimentation which DRC-Hubo+ drove the vehicle in a real-world setting. The presented approach was also verified in DRC-Finals as the author\u2019s team was placed in the top record at the driving task in the competition."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, the driving pattern analysis of other type operators (such as human-drivers or pure tele-operation systems) was provided to demonstrate the performance of the developed system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0af0ab50-4188-48c9-ae0d-cba6495d22fe": {"__data__": {"id_": "0af0ab50-4188-48c9-ae0d-cba6495d22fe", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "For this, the sensor data fusion process and the advanced driving assistant techniques were emphasized. Next, the driving pattern analysis of other type operators (such as human-drivers or pure tele-operation systems) was provided to demonstrate the performance of the developed system. Lastly, test and evaluation of the built robot and control system was provided via experimentation which DRC-Hubo+ drove the vehicle in a real-world setting. The presented approach was also verified in DRC-Finals as the author\u2019s team was placed in the top record at the driving task in the competition. Keywords Humanoids \u00b7 Vehicle driving \u00b7 Darpa robotics challenge \u00b7 DRC finals \u00b7 DRC-Hubo+"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lastly, test and evaluation of the built robot and control system was provided via experimentation which DRC-Hubo+ drove the vehicle in a real-world setting.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ea0e623-a281-4f09-a476-dd3dd05ca5ad": {"__data__": {"id_": "2ea0e623-a281-4f09-a476-dd3dd05ca5ad", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Next, the driving pattern analysis of other type operators (such as human-drivers or pure tele-operation systems) was provided to demonstrate the performance of the developed system. Lastly, test and evaluation of the built robot and control system was provided via experimentation which DRC-Hubo+ drove the vehicle in a real-world setting. The presented approach was also verified in DRC-Finals as the author\u2019s team was placed in the top record at the driving task in the competition. Keywords Humanoids \u00b7 Vehicle driving \u00b7 Darpa robotics challenge \u00b7 DRC finals \u00b7 DRC-Hubo+ After DARPA (Defense Advanced Research Projects Agency) Grand Challenge in 2005 and Urban Challenge in 2007 , the development of driverless cars have been one of main focuses in the robotics research and its related industries."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The presented approach was also verified in DRC-Finals as the author\u2019s team was placed in the top record at the driving task in the competition.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c63038a-9d5d-4808-bd72-f0ef8fef0ed0": {"__data__": {"id_": "8c63038a-9d5d-4808-bd72-f0ef8fef0ed0", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Lastly, test and evaluation of the built robot and control system was provided via experimentation which DRC-Hubo+ drove the vehicle in a real-world setting. The presented approach was also verified in DRC-Finals as the author\u2019s team was placed in the top record at the driving task in the competition. Keywords Humanoids \u00b7 Vehicle driving \u00b7 Darpa robotics challenge \u00b7 DRC finals \u00b7 DRC-Hubo+ After DARPA (Defense Advanced Research Projects Agency) Grand Challenge in 2005 and Urban Challenge in 2007 , the development of driverless cars have been one of main focuses in the robotics research and its related industries. Google\u2019s former robot chief, Andy Rubin, has said that robots will have replaced Google\u2019s factory workers and its delivery drivers within the decade ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Keywords Humanoids \u00b7 Vehicle driving \u00b7 Darpa robotics challenge \u00b7 DRC finals \u00b7 DRC-Hubo+", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39bc86ea-2e13-49dd-8280-334798d72301": {"__data__": {"id_": "39bc86ea-2e13-49dd-8280-334798d72301", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "The presented approach was also verified in DRC-Finals as the author\u2019s team was placed in the top record at the driving task in the competition. Keywords Humanoids \u00b7 Vehicle driving \u00b7 Darpa robotics challenge \u00b7 DRC finals \u00b7 DRC-Hubo+ After DARPA (Defense Advanced Research Projects Agency) Grand Challenge in 2005 and Urban Challenge in 2007 , the development of driverless cars have been one of main focuses in the robotics research and its related industries. Google\u2019s former robot chief, Andy Rubin, has said that robots will have replaced Google\u2019s factory workers and its delivery drivers within the decade . As such, autonomous robot systems which can perform given tasks and deliver products are receiving much attentions from industry now ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After DARPA (Defense Advanced Research Projects Agency) Grand Challenge in 2005 and Urban Challenge in 2007 , the development of driverless cars have been one of main focuses in the robotics research and its related industries.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9440350-11a9-4ae2-81b8-20a22e6264a1": {"__data__": {"id_": "f9440350-11a9-4ae2-81b8-20a22e6264a1", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Keywords Humanoids \u00b7 Vehicle driving \u00b7 Darpa robotics challenge \u00b7 DRC finals \u00b7 DRC-Hubo+ After DARPA (Defense Advanced Research Projects Agency) Grand Challenge in 2005 and Urban Challenge in 2007 , the development of driverless cars have been one of main focuses in the robotics research and its related industries. Google\u2019s former robot chief, Andy Rubin, has said that robots will have replaced Google\u2019s factory workers and its delivery drivers within the decade . As such, autonomous robot systems which can perform given tasks and deliver products are receiving much attentions from industry now . Car is focusing on engineering the vehicle , there are broader impacts and merits if robots can drive off-the- shelf vehicles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Google\u2019s former robot chief, Andy Rubin, has said that robots will have replaced Google\u2019s factory workers and its delivery drivers within the decade .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f98632d-9383-4894-9b61-367ff42b621f": {"__data__": {"id_": "5f98632d-9383-4894-9b61-367ff42b621f", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "After DARPA (Defense Advanced Research Projects Agency) Grand Challenge in 2005 and Urban Challenge in 2007 , the development of driverless cars have been one of main focuses in the robotics research and its related industries. Google\u2019s former robot chief, Andy Rubin, has said that robots will have replaced Google\u2019s factory workers and its delivery drivers within the decade . As such, autonomous robot systems which can perform given tasks and deliver products are receiving much attentions from industry now . Car is focusing on engineering the vehicle , there are broader impacts and merits if robots can drive off-the- shelf vehicles. One big picture is for humanoids to drive unmodified utility vehicles and perform various tasks in human-centered workplaces."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, autonomous robot systems which can perform given tasks and deliver products are receiving much attentions from industry now .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bba819c-7663-42be-9d31-6302e7461906": {"__data__": {"id_": "6bba819c-7663-42be-9d31-6302e7461906", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Google\u2019s former robot chief, Andy Rubin, has said that robots will have replaced Google\u2019s factory workers and its delivery drivers within the decade . As such, autonomous robot systems which can perform given tasks and deliver products are receiving much attentions from industry now . Car is focusing on engineering the vehicle , there are broader impacts and merits if robots can drive off-the- shelf vehicles. One big picture is for humanoids to drive unmodified utility vehicles and perform various tasks in human-centered workplaces. Self-driving humanoids can operate human tools and assist human workers in various stages of supply chains such as logistics, inventory and distribution."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Car is focusing on engineering the vehicle , there are broader impacts and merits if robots can drive off-the- shelf vehicles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1d97c15-e4e1-499f-bcfc-e252ea18fc42": {"__data__": {"id_": "d1d97c15-e4e1-499f-bcfc-e252ea18fc42", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "As such, autonomous robot systems which can perform given tasks and deliver products are receiving much attentions from industry now . Car is focusing on engineering the vehicle , there are broader impacts and merits if robots can drive off-the- shelf vehicles. One big picture is for humanoids to drive unmodified utility vehicles and perform various tasks in human-centered workplaces. Self-driving humanoids can operate human tools and assist human workers in various stages of supply chains such as logistics, inventory and distribution. For example, the robot can load containers to a vehicle and transport them to other storage or distribution centers (DC)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One big picture is for humanoids to drive unmodified utility vehicles and perform various tasks in human-centered workplaces.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bc7d3b1-9cea-452b-9108-0f19b8f9c984": {"__data__": {"id_": "1bc7d3b1-9cea-452b-9108-0f19b8f9c984", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Car is focusing on engineering the vehicle , there are broader impacts and merits if robots can drive off-the- shelf vehicles. One big picture is for humanoids to drive unmodified utility vehicles and perform various tasks in human-centered workplaces. Self-driving humanoids can operate human tools and assist human workers in various stages of supply chains such as logistics, inventory and distribution. For example, the robot can load containers to a vehicle and transport them to other storage or distribution centers (DC). Such robots can assist or replace human labors in warehouse, delivery and even retail areas."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Self-driving humanoids can operate human tools and assist human workers in various stages of supply chains such as logistics, inventory and distribution.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3021c7ad-f786-4f4b-9efd-2ad39a8ad5c8": {"__data__": {"id_": "3021c7ad-f786-4f4b-9efd-2ad39a8ad5c8", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "One big picture is for humanoids to drive unmodified utility vehicles and perform various tasks in human-centered workplaces. Self-driving humanoids can operate human tools and assist human workers in various stages of supply chains such as logistics, inventory and distribution. For example, the robot can load containers to a vehicle and transport them to other storage or distribution centers (DC). Such robots can assist or replace human labors in warehouse, delivery and even retail areas. enabling robots to drive a vehicle is different from the driverless cars."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, the robot can load containers to a vehicle and transport them to other storage or distribution centers (DC).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1f57fc1-b274-4e45-b85c-3fb4c8ab6fed": {"__data__": {"id_": "d1f57fc1-b274-4e45-b85c-3fb4c8ab6fed", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Self-driving humanoids can operate human tools and assist human workers in various stages of supply chains such as logistics, inventory and distribution. For example, the robot can load containers to a vehicle and transport them to other storage or distribution centers (DC). Such robots can assist or replace human labors in warehouse, delivery and even retail areas. enabling robots to drive a vehicle is different from the driverless cars. This project was supported in part by a US DARPA Award #N65236-12-1-1005 for the DARPA Robotics Challenge."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such robots can assist or replace human labors in warehouse, delivery and even retail areas.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e6be556-9731-4511-be0d-2e036e00d46f": {"__data__": {"id_": "2e6be556-9731-4511-be0d-2e036e00d46f", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "For example, the robot can load containers to a vehicle and transport them to other storage or distribution centers (DC). Such robots can assist or replace human labors in warehouse, delivery and even retail areas. enabling robots to drive a vehicle is different from the driverless cars. This project was supported in part by a US DARPA Award #N65236-12-1-1005 for the DARPA Robotics Challenge. endow- ing a humanoid with the ability to drive a vehicle has also intellectual merits."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "enabling robots to drive a vehicle is different from the driverless cars.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b01394e3-d6f6-4907-8acf-1abb4a4f4a97": {"__data__": {"id_": "b01394e3-d6f6-4907-8acf-1abb4a4f4a97", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "Such robots can assist or replace human labors in warehouse, delivery and even retail areas. enabling robots to drive a vehicle is different from the driverless cars. This project was supported in part by a US DARPA Award #N65236-12-1-1005 for the DARPA Robotics Challenge. endow- ing a humanoid with the ability to drive a vehicle has also intellectual merits. 1 Department of Electrical and Computer Engineering, University of Hartford, West Hartford, CT, USA"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This project was supported in part by a US DARPA Award #N65236-12-1-1005 for the DARPA Robotics Challenge.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e42d731-4f6a-4033-b084-ea5ec14e608b": {"__data__": {"id_": "9e42d731-4f6a-4033-b084-ea5ec14e608b", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "enabling robots to drive a vehicle is different from the driverless cars. This project was supported in part by a US DARPA Award #N65236-12-1-1005 for the DARPA Robotics Challenge. endow- ing a humanoid with the ability to drive a vehicle has also intellectual merits. 1 Department of Electrical and Computer Engineering, University of Hartford, West Hartford, CT, USA 2 Department of Mechanical Engineering, University of Nevada, Las Vegas, NV, USA"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "endow- ing a humanoid with the ability to drive a vehicle has also intellectual merits.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7e18ab4-97dd-449c-8cd6-e6d5b1bcf2d0": {"__data__": {"id_": "d7e18ab4-97dd-449c-8cd6-e6d5b1bcf2d0", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "This project was supported in part by a US DARPA Award #N65236-12-1-1005 for the DARPA Robotics Challenge. endow- ing a humanoid with the ability to drive a vehicle has also intellectual merits. 1 Department of Electrical and Computer Engineering, University of Hartford, West Hartford, CT, USA 2 Department of Mechanical Engineering, University of Nevada, Las Vegas, NV, USA Compared to the self-driving vehicle which has been developed for over 50 years , the development of humanoids which can drive off-the-shelf vehicles has not been deeply exploited."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 Department of Electrical and Computer Engineering, University of Hartford, West Hartford, CT, USA", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfa72433-4f4f-4454-a4b2-dc8d1bfd2da3": {"__data__": {"id_": "dfa72433-4f4f-4454-a4b2-dc8d1bfd2da3", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "endow- ing a humanoid with the ability to drive a vehicle has also intellectual merits. 1 Department of Electrical and Computer Engineering, University of Hartford, West Hartford, CT, USA 2 Department of Mechanical Engineering, University of Nevada, Las Vegas, NV, USA Compared to the self-driving vehicle which has been developed for over 50 years , the development of humanoids which can drive off-the-shelf vehicles has not been deeply exploited. of low dimensional solutions to build a motion plan for simulating human-like robot models inside a vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 Department of Mechanical Engineering, University of Nevada, Las Vegas, NV, USA", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dea59d27-5283-4e0e-acd7-011a195c0784": {"__data__": {"id_": "dea59d27-5283-4e0e-acd7-011a195c0784", "embedding": null, "metadata": {"page_number": 1, "source": "s10846-019-01130-x.pdf", "window": "1 Department of Electrical and Computer Engineering, University of Hartford, West Hartford, CT, USA 2 Department of Mechanical Engineering, University of Nevada, Las Vegas, NV, USA Compared to the self-driving vehicle which has been developed for over 50 years , the development of humanoids which can drive off-the-shelf vehicles has not been deeply exploited. of low dimensional solutions to build a motion plan for simulating human-like robot models inside a vehicle. addressed a hierarchical framework for planning and simulating humanoid\u2019s vehicle ingress motions in dynamic environments."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Compared to the self-driving vehicle which has been developed for over 50 years , the development of humanoids which can drive off-the-shelf vehicles has not been deeply exploited.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "391f486f-4bdc-4f78-9d89-7e3ff97ecdfe": {"__data__": {"id_": "391f486f-4bdc-4f78-9d89-7e3ff97ecdfe", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "2 Department of Mechanical Engineering, University of Nevada, Las Vegas, NV, USA Compared to the self-driving vehicle which has been developed for over 50 years , the development of humanoids which can drive off-the-shelf vehicles has not been deeply exploited. of low dimensional solutions to build a motion plan for simulating human-like robot models inside a vehicle. addressed a hierarchical framework for planning and simulating humanoid\u2019s vehicle ingress motions in dynamic environments. However, driving was often limited to the control of cockpit or mock-up vehicles in the experimental set-up , not real platform driving in the previous works."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of low dimensional solutions to build a motion plan for simulating human-like robot models inside a vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1e65a6f-8a8a-42b3-a672-11e866dd044e": {"__data__": {"id_": "b1e65a6f-8a8a-42b3-a672-11e866dd044e", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Compared to the self-driving vehicle which has been developed for over 50 years , the development of humanoids which can drive off-the-shelf vehicles has not been deeply exploited. of low dimensional solutions to build a motion plan for simulating human-like robot models inside a vehicle. addressed a hierarchical framework for planning and simulating humanoid\u2019s vehicle ingress motions in dynamic environments. However, driving was often limited to the control of cockpit or mock-up vehicles in the experimental set-up , not real platform driving in the previous works. In 2012, the DARPA Robotics Challenge (DRC) emphasized the impacts and merits of humanoid\u2019s vehicle driving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "addressed a hierarchical framework for planning and simulating humanoid\u2019s vehicle ingress motions in dynamic environments.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5606388d-5825-4663-bf8f-9053ec9133a0": {"__data__": {"id_": "5606388d-5825-4663-bf8f-9053ec9133a0", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "of low dimensional solutions to build a motion plan for simulating human-like robot models inside a vehicle. addressed a hierarchical framework for planning and simulating humanoid\u2019s vehicle ingress motions in dynamic environments. However, driving was often limited to the control of cockpit or mock-up vehicles in the experimental set-up , not real platform driving in the previous works. In 2012, the DARPA Robotics Challenge (DRC) emphasized the impacts and merits of humanoid\u2019s vehicle driving. Motivated by Japan\u2019s Fukushima Daiichi nuclear disaster on March 2011, DARPA announced that they will kick off competitions which ask all the participant teams to develop robot systems which can replace or assist human labors in disasters."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, driving was often limited to the control of cockpit or mock-up vehicles in the experimental set-up , not real platform driving in the previous works.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6abd5b79-8093-4011-a3b0-55fd79453203": {"__data__": {"id_": "6abd5b79-8093-4011-a3b0-55fd79453203", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "addressed a hierarchical framework for planning and simulating humanoid\u2019s vehicle ingress motions in dynamic environments. However, driving was often limited to the control of cockpit or mock-up vehicles in the experimental set-up , not real platform driving in the previous works. In 2012, the DARPA Robotics Challenge (DRC) emphasized the impacts and merits of humanoid\u2019s vehicle driving. Motivated by Japan\u2019s Fukushima Daiichi nuclear disaster on March 2011, DARPA announced that they will kick off competitions which ask all the participant teams to develop robot systems which can replace or assist human labors in disasters. two competitions which were called DRC-Trials and DRC-Finals were held in 2013 and 2015 respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In 2012, the DARPA Robotics Challenge (DRC) emphasized the impacts and merits of humanoid\u2019s vehicle driving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28dcef94-1e8e-47e7-9f1b-9f3a27e125a7": {"__data__": {"id_": "28dcef94-1e8e-47e7-9f1b-9f3a27e125a7", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "However, driving was often limited to the control of cockpit or mock-up vehicles in the experimental set-up , not real platform driving in the previous works. In 2012, the DARPA Robotics Challenge (DRC) emphasized the impacts and merits of humanoid\u2019s vehicle driving. Motivated by Japan\u2019s Fukushima Daiichi nuclear disaster on March 2011, DARPA announced that they will kick off competitions which ask all the participant teams to develop robot systems which can replace or assist human labors in disasters. two competitions which were called DRC-Trials and DRC-Finals were held in 2013 and 2015 respectively. The robotics challenge demanded the robots to complete eight different tasks in a given hour under disrupted communication environments."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Motivated by Japan\u2019s Fukushima Daiichi nuclear disaster on March 2011, DARPA announced that they will kick off competitions which ask all the participant teams to develop robot systems which can replace or assist human labors in disasters.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87591350-f022-4ac0-81ef-60e14c428448": {"__data__": {"id_": "87591350-f022-4ac0-81ef-60e14c428448", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "In 2012, the DARPA Robotics Challenge (DRC) emphasized the impacts and merits of humanoid\u2019s vehicle driving. Motivated by Japan\u2019s Fukushima Daiichi nuclear disaster on March 2011, DARPA announced that they will kick off competitions which ask all the participant teams to develop robot systems which can replace or assist human labors in disasters. two competitions which were called DRC-Trials and DRC-Finals were held in 2013 and 2015 respectively. The robotics challenge demanded the robots to complete eight different tasks in a given hour under disrupted communication environments. DRC-Trials was the preliminary round match and 16 teams gathered in Florida."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "two competitions which were called DRC-Trials and DRC-Finals were held in 2013 and 2015 respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6dc95d3-f41f-4d28-80cc-13a9f4010cb4": {"__data__": {"id_": "c6dc95d3-f41f-4d28-80cc-13a9f4010cb4", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Motivated by Japan\u2019s Fukushima Daiichi nuclear disaster on March 2011, DARPA announced that they will kick off competitions which ask all the participant teams to develop robot systems which can replace or assist human labors in disasters. two competitions which were called DRC-Trials and DRC-Finals were held in 2013 and 2015 respectively. The robotics challenge demanded the robots to complete eight different tasks in a given hour under disrupted communication environments. DRC-Trials was the preliminary round match and 16 teams gathered in Florida. It allowed the participants to tele-operate robots for the vast majority of tasks."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robotics challenge demanded the robots to complete eight different tasks in a given hour under disrupted communication environments.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45802596-7803-49ab-9917-3972aecce78a": {"__data__": {"id_": "45802596-7803-49ab-9917-3972aecce78a", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "two competitions which were called DRC-Trials and DRC-Finals were held in 2013 and 2015 respectively. The robotics challenge demanded the robots to complete eight different tasks in a given hour under disrupted communication environments. DRC-Trials was the preliminary round match and 16 teams gathered in Florida. It allowed the participants to tele-operate robots for the vast majority of tasks. DRC- Finals was the final round match of the challenge and 25 worldwide robotics organizations competed in Pomona, California."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Trials was the preliminary round match and 16 teams gathered in Florida.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a74ea41a-c46c-4bf2-8cc8-c3b8bb9d801c": {"__data__": {"id_": "a74ea41a-c46c-4bf2-8cc8-c3b8bb9d801c", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "The robotics challenge demanded the robots to complete eight different tasks in a given hour under disrupted communication environments. DRC-Trials was the preliminary round match and 16 teams gathered in Florida. It allowed the participants to tele-operate robots for the vast majority of tasks. DRC- Finals was the final round match of the challenge and 25 worldwide robotics organizations competed in Pomona, California. In Finals, teams could not heavily rely on tele- operation due to the communication blackouts which were controlled by DARPA."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It allowed the participants to tele-operate robots for the vast majority of tasks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e81483a-099d-4bdc-b6fd-7ca72394a087": {"__data__": {"id_": "0e81483a-099d-4bdc-b6fd-7ca72394a087", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "DRC-Trials was the preliminary round match and 16 teams gathered in Florida. It allowed the participants to tele-operate robots for the vast majority of tasks. DRC- Finals was the final round match of the challenge and 25 worldwide robotics organizations competed in Pomona, California. In Finals, teams could not heavily rely on tele- operation due to the communication blackouts which were controlled by DARPA. Vehicle driving was the first mission among eight tasks in the robotics competition (Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC- Finals was the final round match of the challenge and 25 worldwide robotics organizations competed in Pomona, California.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aaf83d6f-9ba2-4f42-8aee-585cc5d3381e": {"__data__": {"id_": "aaf83d6f-9ba2-4f42-8aee-585cc5d3381e", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "It allowed the participants to tele-operate robots for the vast majority of tasks. DRC- Finals was the final round match of the challenge and 25 worldwide robotics organizations competed in Pomona, California. In Finals, teams could not heavily rely on tele- operation due to the communication blackouts which were controlled by DARPA. Vehicle driving was the first mission among eight tasks in the robotics competition (Fig. DARPA explained the reason that robots should be able to drive vehicles by themselves since disasters such as radio-active area are often too toxic for human drivers."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Finals, teams could not heavily rely on tele- operation due to the communication blackouts which were controlled by DARPA.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f9b80c0-fff6-4485-bb1e-9bc366bd326b": {"__data__": {"id_": "6f9b80c0-fff6-4485-bb1e-9bc366bd326b", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "DRC- Finals was the final round match of the challenge and 25 worldwide robotics organizations competed in Pomona, California. In Finals, teams could not heavily rely on tele- operation due to the communication blackouts which were controlled by DARPA. Vehicle driving was the first mission among eight tasks in the robotics competition (Fig. DARPA explained the reason that robots should be able to drive vehicles by themselves since disasters such as radio-active area are often too toxic for human drivers. In DRC-Trials (2013), 16 world-finalists which passed qualification process were competing initially."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Vehicle driving was the first mission among eight tasks in the robotics competition (Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "384d1a5e-209a-4203-a45a-6d8102a80f0c": {"__data__": {"id_": "384d1a5e-209a-4203-a45a-6d8102a80f0c", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "In Finals, teams could not heavily rely on tele- operation due to the communication blackouts which were controlled by DARPA. Vehicle driving was the first mission among eight tasks in the robotics competition (Fig. DARPA explained the reason that robots should be able to drive vehicles by themselves since disasters such as radio-active area are often too toxic for human drivers. In DRC-Trials (2013), 16 world-finalists which passed qualification process were competing initially. In the first challenge, only a few robots were able to drive a given vehicle (Polaris Ranger XP) to"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DARPA explained the reason that robots should be able to drive vehicles by themselves since disasters such as radio-active area are often too toxic for human drivers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0c6389f-8f28-4210-9f91-ebacd98c4073": {"__data__": {"id_": "d0c6389f-8f28-4210-9f91-ebacd98c4073", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Vehicle driving was the first mission among eight tasks in the robotics competition (Fig. DARPA explained the reason that robots should be able to drive vehicles by themselves since disasters such as radio-active area are often too toxic for human drivers. In DRC-Trials (2013), 16 world-finalists which passed qualification process were competing initially. In the first challenge, only a few robots were able to drive a given vehicle (Polaris Ranger XP) to Many of the participating robots even could not start the vehicle ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In DRC-Trials (2013), 16 world-finalists which passed qualification process were competing initially.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "393b5045-8bbb-45fc-bbe7-058ff8097ebf": {"__data__": {"id_": "393b5045-8bbb-45fc-bbe7-058ff8097ebf", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "DARPA explained the reason that robots should be able to drive vehicles by themselves since disasters such as radio-active area are often too toxic for human drivers. In DRC-Trials (2013), 16 world-finalists which passed qualification process were competing initially. In the first challenge, only a few robots were able to drive a given vehicle (Polaris Ranger XP) to Many of the participating robots even could not start the vehicle . Furthermore, the team who even finished as the first place took 5 minutes to finish the just 250 foot distance race ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the first challenge, only a few robots were able to drive a given vehicle (Polaris Ranger XP) to", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce334202-b69d-4c8d-b0cc-c420f56e325f": {"__data__": {"id_": "ce334202-b69d-4c8d-b0cc-c420f56e325f", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "In DRC-Trials (2013), 16 world-finalists which passed qualification process were competing initially. In the first challenge, only a few robots were able to drive a given vehicle (Polaris Ranger XP) to Many of the participating robots even could not start the vehicle . Furthermore, the team who even finished as the first place took 5 minutes to finish the just 250 foot distance race . each team developed their own robot platforms and its tele-control systems for the unified use in all different eight tasks of the challenge [16\u201318]."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Many of the participating robots even could not start the vehicle .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b28642d-651b-4ff8-b654-21df7998ee86": {"__data__": {"id_": "1b28642d-651b-4ff8-b654-21df7998ee86", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "In the first challenge, only a few robots were able to drive a given vehicle (Polaris Ranger XP) to Many of the participating robots even could not start the vehicle . Furthermore, the team who even finished as the first place took 5 minutes to finish the just 250 foot distance race . each team developed their own robot platforms and its tele-control systems for the unified use in all different eight tasks of the challenge [16\u201318]. They were built considering all the factors of driving and non-driving tasks."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, the team who even finished as the first place took 5 minutes to finish the just 250 foot distance race .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e853b76d-e531-4658-9042-3bc62e84c159": {"__data__": {"id_": "e853b76d-e531-4658-9042-3bc62e84c159", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Many of the participating robots even could not start the vehicle . Furthermore, the team who even finished as the first place took 5 minutes to finish the just 250 foot distance race . each team developed their own robot platforms and its tele-control systems for the unified use in all different eight tasks of the challenge [16\u201318]. They were built considering all the factors of driving and non-driving tasks. Accordingly, the developed platforms and systems were not often optimized for driving task ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "each team developed their own robot platforms and its tele-control systems for the unified use in all different eight tasks of the challenge [16\u201318].", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95cdc0cb-6a39-49d2-abb6-39025cffd91f": {"__data__": {"id_": "95cdc0cb-6a39-49d2-abb6-39025cffd91f", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Furthermore, the team who even finished as the first place took 5 minutes to finish the just 250 foot distance race . each team developed their own robot platforms and its tele-control systems for the unified use in all different eight tasks of the challenge [16\u201318]. They were built considering all the factors of driving and non-driving tasks. Accordingly, the developed platforms and systems were not often optimized for driving task . However, unlike other tasks which ask for robots to operate simple hand-sized tools in open-space environments (such as door- opening, valve-turning and wall-drilling), the driving task demands for robots to handle many control-inputs of the vehicle (steering wheel, gas and brake pedals)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They were built considering all the factors of driving and non-driving tasks.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eebeae87-784b-4417-92b2-72fef430691b": {"__data__": {"id_": "eebeae87-784b-4417-92b2-72fef430691b", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "each team developed their own robot platforms and its tele-control systems for the unified use in all different eight tasks of the challenge [16\u201318]. They were built considering all the factors of driving and non-driving tasks. Accordingly, the developed platforms and systems were not often optimized for driving task . However, unlike other tasks which ask for robots to operate simple hand-sized tools in open-space environments (such as door- opening, valve-turning and wall-drilling), the driving task demands for robots to handle many control-inputs of the vehicle (steering wheel, gas and brake pedals). Furthermore, during the task, the robot has limited perception and range- of-motion dues to structures (poles and frames) of the vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Accordingly, the developed platforms and systems were not often optimized for driving task .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "627a2819-f2dd-446e-aae7-2590b6005596": {"__data__": {"id_": "627a2819-f2dd-446e-aae7-2590b6005596", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "They were built considering all the factors of driving and non-driving tasks. Accordingly, the developed platforms and systems were not often optimized for driving task . However, unlike other tasks which ask for robots to operate simple hand-sized tools in open-space environments (such as door- opening, valve-turning and wall-drilling), the driving task demands for robots to handle many control-inputs of the vehicle (steering wheel, gas and brake pedals). Furthermore, during the task, the robot has limited perception and range- of-motion dues to structures (poles and frames) of the vehicle. To enable the robot to finish the race within the given time limits, the human operator should make an optimized decision continuously and quickly based on the collected data from perception process."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, unlike other tasks which ask for robots to operate simple hand-sized tools in open-space environments (such as door- opening, valve-turning and wall-drilling), the driving task demands for robots to handle many control-inputs of the vehicle (steering wheel, gas and brake pedals).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "559cda98-c4b3-4c57-85e7-45b0c63acab3": {"__data__": {"id_": "559cda98-c4b3-4c57-85e7-45b0c63acab3", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Accordingly, the developed platforms and systems were not often optimized for driving task . However, unlike other tasks which ask for robots to operate simple hand-sized tools in open-space environments (such as door- opening, valve-turning and wall-drilling), the driving task demands for robots to handle many control-inputs of the vehicle (steering wheel, gas and brake pedals). Furthermore, during the task, the robot has limited perception and range- of-motion dues to structures (poles and frames) of the vehicle. To enable the robot to finish the race within the given time limits, the human operator should make an optimized decision continuously and quickly based on the collected data from perception process. the hardware design which did not fully consider the characteristics of driving task resulted in breakdown of the system during course-driving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, during the task, the robot has limited perception and range- of-motion dues to structures (poles and frames) of the vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "899df535-f2cc-4bd4-bb96-f800107bd934": {"__data__": {"id_": "899df535-f2cc-4bd4-bb96-f800107bd934", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "However, unlike other tasks which ask for robots to operate simple hand-sized tools in open-space environments (such as door- opening, valve-turning and wall-drilling), the driving task demands for robots to handle many control-inputs of the vehicle (steering wheel, gas and brake pedals). Furthermore, during the task, the robot has limited perception and range- of-motion dues to structures (poles and frames) of the vehicle. To enable the robot to finish the race within the given time limits, the human operator should make an optimized decision continuously and quickly based on the collected data from perception process. the hardware design which did not fully consider the characteristics of driving task resulted in breakdown of the system during course-driving. The control system which were made for multi-task use also did not provide direct and easy control interface to operators."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To enable the robot to finish the race within the given time limits, the human operator should make an optimized decision continuously and quickly based on the collected data from perception process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43b73e6e-bbfe-4172-9226-41d015ffedb6": {"__data__": {"id_": "43b73e6e-bbfe-4172-9226-41d015ffedb6", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Furthermore, during the task, the robot has limited perception and range- of-motion dues to structures (poles and frames) of the vehicle. To enable the robot to finish the race within the given time limits, the human operator should make an optimized decision continuously and quickly based on the collected data from perception process. the hardware design which did not fully consider the characteristics of driving task resulted in breakdown of the system during course-driving. The control system which were made for multi-task use also did not provide direct and easy control interface to operators. The net result is that it took almost 30 minutes to finish the driving course (in DRC-Trials)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the hardware design which did not fully consider the characteristics of driving task resulted in breakdown of the system during course-driving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d317982-2573-4c8c-85ab-cc7193a0af4b": {"__data__": {"id_": "5d317982-2573-4c8c-85ab-cc7193a0af4b", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "To enable the robot to finish the race within the given time limits, the human operator should make an optimized decision continuously and quickly based on the collected data from perception process. the hardware design which did not fully consider the characteristics of driving task resulted in breakdown of the system during course-driving. The control system which were made for multi-task use also did not provide direct and easy control interface to operators. The net result is that it took almost 30 minutes to finish the driving course (in DRC-Trials). the authors developed a new humanoid control system which is optimized (especially in perception data processing) for driving task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The control system which were made for multi-task use also did not provide direct and easy control interface to operators.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22766c9f-4191-4a86-9c01-4e623ffadd0f": {"__data__": {"id_": "22766c9f-4191-4a86-9c01-4e623ffadd0f", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "the hardware design which did not fully consider the characteristics of driving task resulted in breakdown of the system during course-driving. The control system which were made for multi-task use also did not provide direct and easy control interface to operators. The net result is that it took almost 30 minutes to finish the driving course (in DRC-Trials). the authors developed a new humanoid control system which is optimized (especially in perception data processing) for driving task. First, intuitive perception of surrounding environments is emphasized in the system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The net result is that it took almost 30 minutes to finish the driving course (in DRC-Trials).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6cd180a-02ff-49a9-8fd0-ed74b0ae6fac": {"__data__": {"id_": "d6cd180a-02ff-49a9-8fd0-ed74b0ae6fac", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "The control system which were made for multi-task use also did not provide direct and easy control interface to operators. The net result is that it took almost 30 minutes to finish the driving course (in DRC-Trials). the authors developed a new humanoid control system which is optimized (especially in perception data processing) for driving task. First, intuitive perception of surrounding environments is emphasized in the system. For that, related data from different sensors (such as Lidar and camera image) were merged through the fusion process."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the authors developed a new humanoid control system which is optimized (especially in perception data processing) for driving task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba193267-8eee-4536-999f-018b85402fb9": {"__data__": {"id_": "ba193267-8eee-4536-999f-018b85402fb9", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "The net result is that it took almost 30 minutes to finish the driving course (in DRC-Trials). the authors developed a new humanoid control system which is optimized (especially in perception data processing) for driving task. First, intuitive perception of surrounding environments is emphasized in the system. For that, related data from different sensors (such as Lidar and camera image) were merged through the fusion process. This process not only provided integrative understanding of the given driving course to human-operator but also enabled the efficient segmentation process which can distinguish the non-obstacle areas from the course."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, intuitive perception of surrounding environments is emphasized in the system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3237c1f-0c80-4540-be94-ddb060d208b9": {"__data__": {"id_": "a3237c1f-0c80-4540-be94-ddb060d208b9", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "the authors developed a new humanoid control system which is optimized (especially in perception data processing) for driving task. First, intuitive perception of surrounding environments is emphasized in the system. For that, related data from different sensors (such as Lidar and camera image) were merged through the fusion process. This process not only provided integrative understanding of the given driving course to human-operator but also enabled the efficient segmentation process which can distinguish the non-obstacle areas from the course. Second, unlike the previous tele-control system which relied mainly on human operator\u2019s manual control, the newly developed system adopted various advanced driver assistant functionalities."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For that, related data from different sensors (such as Lidar and camera image) were merged through the fusion process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68b36f79-3a04-4d14-8b3b-c2ae6cc43890": {"__data__": {"id_": "68b36f79-3a04-4d14-8b3b-c2ae6cc43890", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "First, intuitive perception of surrounding environments is emphasized in the system. For that, related data from different sensors (such as Lidar and camera image) were merged through the fusion process. This process not only provided integrative understanding of the given driving course to human-operator but also enabled the efficient segmentation process which can distinguish the non-obstacle areas from the course. Second, unlike the previous tele-control system which relied mainly on human operator\u2019s manual control, the newly developed system adopted various advanced driver assistant functionalities. Using the kinematics model of the vehicle and sensor- measurement, the expected vehicle-path was calculated."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This process not only provided integrative understanding of the given driving course to human-operator but also enabled the efficient segmentation process which can distinguish the non-obstacle areas from the course.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d01ff0a-9927-4409-a95a-7008d4c8970d": {"__data__": {"id_": "6d01ff0a-9927-4409-a95a-7008d4c8970d", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "For that, related data from different sensors (such as Lidar and camera image) were merged through the fusion process. This process not only provided integrative understanding of the given driving course to human-operator but also enabled the efficient segmentation process which can distinguish the non-obstacle areas from the course. Second, unlike the previous tele-control system which relied mainly on human operator\u2019s manual control, the newly developed system adopted various advanced driver assistant functionalities. Using the kinematics model of the vehicle and sensor- measurement, the expected vehicle-path was calculated. It made the operator to estimate the vehicle\u2019s position with the control input of the moment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, unlike the previous tele-control system which relied mainly on human operator\u2019s manual control, the newly developed system adopted various advanced driver assistant functionalities.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c96bf38-0baf-480e-84b2-64e5921bf87b": {"__data__": {"id_": "5c96bf38-0baf-480e-84b2-64e5921bf87b", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "This process not only provided integrative understanding of the given driving course to human-operator but also enabled the efficient segmentation process which can distinguish the non-obstacle areas from the course. Second, unlike the previous tele-control system which relied mainly on human operator\u2019s manual control, the newly developed system adopted various advanced driver assistant functionalities. Using the kinematics model of the vehicle and sensor- measurement, the expected vehicle-path was calculated. It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among possible paths, the most optimized path was found in terms of driving distance and collision avoidance."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using the kinematics model of the vehicle and sensor- measurement, the expected vehicle-path was calculated.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be20e1c2-b065-452b-bc38-73337825c9f0": {"__data__": {"id_": "be20e1c2-b065-452b-bc38-73337825c9f0", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Second, unlike the previous tele-control system which relied mainly on human operator\u2019s manual control, the newly developed system adopted various advanced driver assistant functionalities. Using the kinematics model of the vehicle and sensor- measurement, the expected vehicle-path was calculated. It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among possible paths, the most optimized path was found in terms of driving distance and collision avoidance. These newly added features provided the operator with more stable and"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It made the operator to estimate the vehicle\u2019s position with the control input of the moment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b8fdb3e-2131-4c70-bc05-e2ab86b043c6": {"__data__": {"id_": "9b8fdb3e-2131-4c70-bc05-e2ab86b043c6", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "Using the kinematics model of the vehicle and sensor- measurement, the expected vehicle-path was calculated. It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among possible paths, the most optimized path was found in terms of driving distance and collision avoidance. These newly added features provided the operator with more stable and continuous experimentation in real outdoor envi- ronment is implemented with the updated robot platform (DRC-Hubo+)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among possible paths, the most optimized path was found in terms of driving distance and collision avoidance.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "867955a9-d1e4-49aa-8099-a83f4a130e1b": {"__data__": {"id_": "867955a9-d1e4-49aa-8099-a83f4a130e1b", "embedding": null, "metadata": {"page_number": 2, "source": "s10846-019-01130-x.pdf", "window": "It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among possible paths, the most optimized path was found in terms of driving distance and collision avoidance. These newly added features provided the operator with more stable and continuous experimentation in real outdoor envi- ronment is implemented with the updated robot platform (DRC-Hubo+). The system is also tested and verified with its performance in DRC-Finals."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These newly added features provided the operator with more stable and", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ff9db89-cba8-45c7-ae8a-3e5898aa11c3": {"__data__": {"id_": "8ff9db89-cba8-45c7-ae8a-3e5898aa11c3", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Among possible paths, the most optimized path was found in terms of driving distance and collision avoidance. These newly added features provided the operator with more stable and continuous experimentation in real outdoor envi- ronment is implemented with the updated robot platform (DRC-Hubo+). The system is also tested and verified with its performance in DRC-Finals. DRC-Hubo+ successfully completed driving a track within 55 second which is top record in the competition ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "continuous experimentation in real outdoor envi- ronment is implemented with the updated robot platform (DRC-Hubo+).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a17ca399-8663-4ad4-9245-e7ed399e0793": {"__data__": {"id_": "a17ca399-8663-4ad4-9245-e7ed399e0793", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "These newly added features provided the operator with more stable and continuous experimentation in real outdoor envi- ronment is implemented with the updated robot platform (DRC-Hubo+). The system is also tested and verified with its performance in DRC-Finals. DRC-Hubo+ successfully completed driving a track within 55 second which is top record in the competition . The paper is organized as follows: Section 2 demon- strates the hardware design changes of the robot platform."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The system is also tested and verified with its performance in DRC-Finals.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5a497ec-6dbb-4c22-bac6-6892c3cef477": {"__data__": {"id_": "d5a497ec-6dbb-4c22-bac6-6892c3cef477", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "continuous experimentation in real outdoor envi- ronment is implemented with the updated robot platform (DRC-Hubo+). The system is also tested and verified with its performance in DRC-Finals. DRC-Hubo+ successfully completed driving a track within 55 second which is top record in the competition . The paper is organized as follows: Section 2 demon- strates the hardware design changes of the robot platform. For the final design, robustness is more emphasized in both mechanical and electronic sides."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo+ successfully completed driving a track within 55 second which is top record in the competition .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18d5e59f-93fc-4ed8-a2c8-27e5d969c8e5": {"__data__": {"id_": "18d5e59f-93fc-4ed8-a2c8-27e5d969c8e5", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "The system is also tested and verified with its performance in DRC-Finals. DRC-Hubo+ successfully completed driving a track within 55 second which is top record in the competition . The paper is organized as follows: Section 2 demon- strates the hardware design changes of the robot platform. For the final design, robustness is more emphasized in both mechanical and electronic sides. Section 3 provides an overview of the control system architecture to operate DRC- Hubo+ under the controlled network."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The paper is organized as follows: Section 2 demon- strates the hardware design changes of the robot platform.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f91d370-9e7d-4c64-8a09-92cf9ac83588": {"__data__": {"id_": "6f91d370-9e7d-4c64-8a09-92cf9ac83588", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "DRC-Hubo+ successfully completed driving a track within 55 second which is top record in the competition . The paper is organized as follows: Section 2 demon- strates the hardware design changes of the robot platform. For the final design, robustness is more emphasized in both mechanical and electronic sides. Section 3 provides an overview of the control system architecture to operate DRC- Hubo+ under the controlled network. Section 4 lists several issues which were generated by the previous head and presents the design updates of DRC-Hubo+\u2019s new sensor- head."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the final design, robustness is more emphasized in both mechanical and electronic sides.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56b2b81a-e814-492c-8985-3d067685017e": {"__data__": {"id_": "56b2b81a-e814-492c-8985-3d067685017e", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "The paper is organized as follows: Section 2 demon- strates the hardware design changes of the robot platform. For the final design, robustness is more emphasized in both mechanical and electronic sides. Section 3 provides an overview of the control system architecture to operate DRC- Hubo+ under the controlled network. Section 4 lists several issues which were generated by the previous head and presents the design updates of DRC-Hubo+\u2019s new sensor- head. Section 5 describes various perception data processing techniques which enabled the human operator to get more intuitive perception of surrounding environment and safe control for driving task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 3 provides an overview of the control system architecture to operate DRC- Hubo+ under the controlled network.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36dea8ed-8681-4cf0-a1c0-b32acb56589c": {"__data__": {"id_": "36dea8ed-8681-4cf0-a1c0-b32acb56589c", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "For the final design, robustness is more emphasized in both mechanical and electronic sides. Section 3 provides an overview of the control system architecture to operate DRC- Hubo+ under the controlled network. Section 4 lists several issues which were generated by the previous head and presents the design updates of DRC-Hubo+\u2019s new sensor- head. Section 5 describes various perception data processing techniques which enabled the human operator to get more intuitive perception of surrounding environment and safe control for driving task. Section 6 shows how driving motion of DRC-Hubo+ is designed with several assistance tools."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 4 lists several issues which were generated by the previous head and presents the design updates of DRC-Hubo+\u2019s new sensor- head.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f71883b5-f3c0-442d-9196-7873086f9891": {"__data__": {"id_": "f71883b5-f3c0-442d-9196-7873086f9891", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Section 3 provides an overview of the control system architecture to operate DRC- Hubo+ under the controlled network. Section 4 lists several issues which were generated by the previous head and presents the design updates of DRC-Hubo+\u2019s new sensor- head. Section 5 describes various perception data processing techniques which enabled the human operator to get more intuitive perception of surrounding environment and safe control for driving task. Section 6 shows how driving motion of DRC-Hubo+ is designed with several assistance tools. Section 7 describes experimental setup and results using the developed control system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 5 describes various perception data processing techniques which enabled the human operator to get more intuitive perception of surrounding environment and safe control for driving task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9bce3f71-8e78-4ea0-9987-d11591df41d6": {"__data__": {"id_": "9bce3f71-8e78-4ea0-9987-d11591df41d6", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Section 4 lists several issues which were generated by the previous head and presents the design updates of DRC-Hubo+\u2019s new sensor- head. Section 5 describes various perception data processing techniques which enabled the human operator to get more intuitive perception of surrounding environment and safe control for driving task. Section 6 shows how driving motion of DRC-Hubo+ is designed with several assistance tools. Section 7 describes experimental setup and results using the developed control system. Section 8 draws the conclusions of the paper and presents future studies."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 6 shows how driving motion of DRC-Hubo+ is designed with several assistance tools.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6974f616-6ba9-4713-83a9-61de55a33db3": {"__data__": {"id_": "6974f616-6ba9-4713-83a9-61de55a33db3", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Section 5 describes various perception data processing techniques which enabled the human operator to get more intuitive perception of surrounding environment and safe control for driving task. Section 6 shows how driving motion of DRC-Hubo+ is designed with several assistance tools. Section 7 describes experimental setup and results using the developed control system. Section 8 draws the conclusions of the paper and presents future studies. 1 demonstrated several issues in vehicle driving task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 7 describes experimental setup and results using the developed control system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8577e23f-d9ed-4583-9a54-c9f7b40c3a47": {"__data__": {"id_": "8577e23f-d9ed-4583-9a54-c9f7b40c3a47", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Section 6 shows how driving motion of DRC-Hubo+ is designed with several assistance tools. Section 7 describes experimental setup and results using the developed control system. Section 8 draws the conclusions of the paper and presents future studies. 1 demonstrated several issues in vehicle driving task. Initially, DRC-Hubo (which has the heights of 1.55 m and the weights of 60 Kg) was designed mainly for rough-terrain walking and material handling tasks (such as door opening ot valve-turning) in the challenge."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 8 draws the conclusions of the paper and presents future studies.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86212aa9-29a4-4b71-bc92-eb2ed1bdc8a8": {"__data__": {"id_": "86212aa9-29a4-4b71-bc92-eb2ed1bdc8a8", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Section 7 describes experimental setup and results using the developed control system. Section 8 draws the conclusions of the paper and presents future studies. 1 demonstrated several issues in vehicle driving task. Initially, DRC-Hubo (which has the heights of 1.55 m and the weights of 60 Kg) was designed mainly for rough-terrain walking and material handling tasks (such as door opening ot valve-turning) in the challenge. Therefore, the platform was not optimized to fit in the DARPA\u2019s provided vehicle (Polaris Ranger XP)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 demonstrated several issues in vehicle driving task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92a47846-6bbd-4d19-8773-c8a116cb3d56": {"__data__": {"id_": "92a47846-6bbd-4d19-8773-c8a116cb3d56", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Section 8 draws the conclusions of the paper and presents future studies. 1 demonstrated several issues in vehicle driving task. Initially, DRC-Hubo (which has the heights of 1.55 m and the weights of 60 Kg) was designed mainly for rough-terrain walking and material handling tasks (such as door opening ot valve-turning) in the challenge. Therefore, the platform was not optimized to fit in the DARPA\u2019s provided vehicle (Polaris Ranger XP). Length of legs were not long enough to get inside (ingress) the vehicle by itself in safe manner."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Initially, DRC-Hubo (which has the heights of 1.55 m and the weights of 60 Kg) was designed mainly for rough-terrain walking and material handling tasks (such as door opening ot valve-turning) in the challenge.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d755679b-ab89-46de-be5c-71aa2e2c3e4f": {"__data__": {"id_": "d755679b-ab89-46de-be5c-71aa2e2c3e4f", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "1 demonstrated several issues in vehicle driving task. Initially, DRC-Hubo (which has the heights of 1.55 m and the weights of 60 Kg) was designed mainly for rough-terrain walking and material handling tasks (such as door opening ot valve-turning) in the challenge. Therefore, the platform was not optimized to fit in the DARPA\u2019s provided vehicle (Polaris Ranger XP). Length of legs were not long enough to get inside (ingress) the vehicle by itself in safe manner. Furthermore, actuators (in wrist and arm joints) did not provide enough strong power to handle the steering-wheel of Polaris."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the platform was not optimized to fit in the DARPA\u2019s provided vehicle (Polaris Ranger XP).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1aa3d136-7155-431f-90aa-e7ffdf21d1f4": {"__data__": {"id_": "1aa3d136-7155-431f-90aa-e7ffdf21d1f4", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Initially, DRC-Hubo (which has the heights of 1.55 m and the weights of 60 Kg) was designed mainly for rough-terrain walking and material handling tasks (such as door opening ot valve-turning) in the challenge. Therefore, the platform was not optimized to fit in the DARPA\u2019s provided vehicle (Polaris Ranger XP). Length of legs were not long enough to get inside (ingress) the vehicle by itself in safe manner. Furthermore, actuators (in wrist and arm joints) did not provide enough strong power to handle the steering-wheel of Polaris. As such, robustness in both mechanical and electronic sides is emphasized in new hardware design for DRC-Finals."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Length of legs were not long enough to get inside (ingress) the vehicle by itself in safe manner.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0310baf0-f6d8-4e66-95b7-297954d005a4": {"__data__": {"id_": "0310baf0-f6d8-4e66-95b7-297954d005a4", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Therefore, the platform was not optimized to fit in the DARPA\u2019s provided vehicle (Polaris Ranger XP). Length of legs were not long enough to get inside (ingress) the vehicle by itself in safe manner. Furthermore, actuators (in wrist and arm joints) did not provide enough strong power to handle the steering-wheel of Polaris. As such, robustness in both mechanical and electronic sides is emphasized in new hardware design for DRC-Finals. As provided in the figure, the upgraded platform has the heights of 1.75 m and the weights of 80 Kg."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, actuators (in wrist and arm joints) did not provide enough strong power to handle the steering-wheel of Polaris.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e0ec999-3f26-493b-957a-0dbf972623fc": {"__data__": {"id_": "1e0ec999-3f26-493b-957a-0dbf972623fc", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Length of legs were not long enough to get inside (ingress) the vehicle by itself in safe manner. Furthermore, actuators (in wrist and arm joints) did not provide enough strong power to handle the steering-wheel of Polaris. As such, robustness in both mechanical and electronic sides is emphasized in new hardware design for DRC-Finals. As provided in the figure, the upgraded platform has the heights of 1.75 m and the weights of 80 Kg. Compared to DRC-Hubo, the heights and weights of DRC-Hubo+ are increased by 0.2 m and 20 Kg, respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, robustness in both mechanical and electronic sides is emphasized in new hardware design for DRC-Finals.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "752a13d0-504e-4de5-8327-e39b6b75257f": {"__data__": {"id_": "752a13d0-504e-4de5-8327-e39b6b75257f", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Furthermore, actuators (in wrist and arm joints) did not provide enough strong power to handle the steering-wheel of Polaris. As such, robustness in both mechanical and electronic sides is emphasized in new hardware design for DRC-Finals. As provided in the figure, the upgraded platform has the heights of 1.75 m and the weights of 80 Kg. Compared to DRC-Hubo, the heights and weights of DRC-Hubo+ are increased by 0.2 m and 20 Kg, respectively. To increase joint power, torque limits of corresponding motor and harmonic driver also become increased ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As provided in the figure, the upgraded platform has the heights of 1.75 m and the weights of 80 Kg.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd50fa0a-ada7-40a1-9d52-f08f68f4bc97": {"__data__": {"id_": "cd50fa0a-ada7-40a1-9d52-f08f68f4bc97", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "As such, robustness in both mechanical and electronic sides is emphasized in new hardware design for DRC-Finals. As provided in the figure, the upgraded platform has the heights of 1.75 m and the weights of 80 Kg. Compared to DRC-Hubo, the heights and weights of DRC-Hubo+ are increased by 0.2 m and 20 Kg, respectively. To increase joint power, torque limits of corresponding motor and harmonic driver also become increased . 1DRC-Hubo (released in 2013) is the generation following the 2007 KHR-4 Hubo and 2010 Hubo+."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Compared to DRC-Hubo, the heights and weights of DRC-Hubo+ are increased by 0.2 m and 20 Kg, respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd361d49-51da-437e-96e2-1107dd202274": {"__data__": {"id_": "cd361d49-51da-437e-96e2-1107dd202274", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "As provided in the figure, the upgraded platform has the heights of 1.75 m and the weights of 80 Kg. Compared to DRC-Hubo, the heights and weights of DRC-Hubo+ are increased by 0.2 m and 20 Kg, respectively. To increase joint power, torque limits of corresponding motor and harmonic driver also become increased . 1DRC-Hubo (released in 2013) is the generation following the 2007 KHR-4 Hubo and 2010 Hubo+. all external cables (which was exposed outside shells in DRC- Hubo) are removed in DRC-Hubo+."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To increase joint power, torque limits of corresponding motor and harmonic driver also become increased .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86b47fb1-3e79-4c92-89c7-d6b414b67ef2": {"__data__": {"id_": "86b47fb1-3e79-4c92-89c7-d6b414b67ef2", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Compared to DRC-Hubo, the heights and weights of DRC-Hubo+ are increased by 0.2 m and 20 Kg, respectively. To increase joint power, torque limits of corresponding motor and harmonic driver also become increased . 1DRC-Hubo (released in 2013) is the generation following the 2007 KHR-4 Hubo and 2010 Hubo+. all external cables (which was exposed outside shells in DRC- Hubo) are removed in DRC-Hubo+. To prevent overheat of actuators, the finned air cooling and heat dissipation system is also attached to joints that need high power consumption ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1DRC-Hubo (released in 2013) is the generation following the 2007 KHR-4 Hubo and 2010 Hubo+.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db542854-ad29-4776-9aa8-f05555a3d90a": {"__data__": {"id_": "db542854-ad29-4776-9aa8-f05555a3d90a", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "To increase joint power, torque limits of corresponding motor and harmonic driver also become increased . 1DRC-Hubo (released in 2013) is the generation following the 2007 KHR-4 Hubo and 2010 Hubo+. all external cables (which was exposed outside shells in DRC- Hubo) are removed in DRC-Hubo+. To prevent overheat of actuators, the finned air cooling and heat dissipation system is also attached to joints that need high power consumption . The robot has 32 DOF (degree of freedom)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "all external cables (which was exposed outside shells in DRC- Hubo) are removed in DRC-Hubo+.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04d05ca5-142f-4ffc-a750-2602e7894aaf": {"__data__": {"id_": "04d05ca5-142f-4ffc-a750-2602e7894aaf", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "1DRC-Hubo (released in 2013) is the generation following the 2007 KHR-4 Hubo and 2010 Hubo+. all external cables (which was exposed outside shells in DRC- Hubo) are removed in DRC-Hubo+. To prevent overheat of actuators, the finned air cooling and heat dissipation system is also attached to joints that need high power consumption . The robot has 32 DOF (degree of freedom). Each leg has 7 DOF including the rotational joint for the knee-wheel and each arm has 8 DOF including the finger joint."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To prevent overheat of actuators, the finned air cooling and heat dissipation system is also attached to joints that need high power consumption .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb2287dc-3a46-4c02-be47-6e35ab3c5570": {"__data__": {"id_": "eb2287dc-3a46-4c02-be47-6e35ab3c5570", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "all external cables (which was exposed outside shells in DRC- Hubo) are removed in DRC-Hubo+. To prevent overheat of actuators, the finned air cooling and heat dissipation system is also attached to joints that need high power consumption . The robot has 32 DOF (degree of freedom). Each leg has 7 DOF including the rotational joint for the knee-wheel and each arm has 8 DOF including the finger joint. The robot\u2019s head and the body-waist joint also have 1 DOF respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot has 32 DOF (degree of freedom).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "373f9b7b-23bd-4ae7-aa41-ebc411ee2540": {"__data__": {"id_": "373f9b7b-23bd-4ae7-aa41-ebc411ee2540", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "To prevent overheat of actuators, the finned air cooling and heat dissipation system is also attached to joints that need high power consumption . The robot has 32 DOF (degree of freedom). Each leg has 7 DOF including the rotational joint for the knee-wheel and each arm has 8 DOF including the finger joint. The robot\u2019s head and the body-waist joint also have 1 DOF respectively. Compared to the original platform (DRC-Hubo), though DOF of each arm is decreased by 1, the total DOF is kept same due to the wheel joint which is attached to each knee."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each leg has 7 DOF including the rotational joint for the knee-wheel and each arm has 8 DOF including the finger joint.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4791cf46-55a8-48fb-be44-31122bd41c56": {"__data__": {"id_": "4791cf46-55a8-48fb-be44-31122bd41c56", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "The robot has 32 DOF (degree of freedom). Each leg has 7 DOF including the rotational joint for the knee-wheel and each arm has 8 DOF including the finger joint. The robot\u2019s head and the body-waist joint also have 1 DOF respectively. Compared to the original platform (DRC-Hubo), though DOF of each arm is decreased by 1, the total DOF is kept same due to the wheel joint which is attached to each knee. In its head, DRC-Hubo+ is equipped with a long distance range Lidar, multi camera system, four force-torque (F/T) sensors, and a body IMU sensor."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot\u2019s head and the body-waist joint also have 1 DOF respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a5d7777-e453-4cef-a0e1-19287328ca67": {"__data__": {"id_": "3a5d7777-e453-4cef-a0e1-19287328ca67", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Each leg has 7 DOF including the rotational joint for the knee-wheel and each arm has 8 DOF including the finger joint. The robot\u2019s head and the body-waist joint also have 1 DOF respectively. Compared to the original platform (DRC-Hubo), though DOF of each arm is decreased by 1, the total DOF is kept same due to the wheel joint which is attached to each knee. In its head, DRC-Hubo+ is equipped with a long distance range Lidar, multi camera system, four force-torque (F/T) sensors, and a body IMU sensor. Newly, a fiber optic gyro and a head IMU sensor are attached to the hip and neck of DRC-Hubo+ respectively for more robust tracking of the vehicle (which the robot drives)\u2019s position and rotation information."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Compared to the original platform (DRC-Hubo), though DOF of each arm is decreased by 1, the total DOF is kept same due to the wheel joint which is attached to each knee.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afb942ba-57f1-43cb-b23e-9d05f10430d5": {"__data__": {"id_": "afb942ba-57f1-43cb-b23e-9d05f10430d5", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "The robot\u2019s head and the body-waist joint also have 1 DOF respectively. Compared to the original platform (DRC-Hubo), though DOF of each arm is decreased by 1, the total DOF is kept same due to the wheel joint which is attached to each knee. In its head, DRC-Hubo+ is equipped with a long distance range Lidar, multi camera system, four force-torque (F/T) sensors, and a body IMU sensor. Newly, a fiber optic gyro and a head IMU sensor are attached to the hip and neck of DRC-Hubo+ respectively for more robust tracking of the vehicle (which the robot drives)\u2019s position and rotation information. The control system is also updated to reflect mechanical and electronic changes of DRC-Hubo+."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In its head, DRC-Hubo+ is equipped with a long distance range Lidar, multi camera system, four force-torque (F/T) sensors, and a body IMU sensor.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3716d104-98cc-49df-910f-759868f60f9b": {"__data__": {"id_": "3716d104-98cc-49df-910f-759868f60f9b", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Compared to the original platform (DRC-Hubo), though DOF of each arm is decreased by 1, the total DOF is kept same due to the wheel joint which is attached to each knee. In its head, DRC-Hubo+ is equipped with a long distance range Lidar, multi camera system, four force-torque (F/T) sensors, and a body IMU sensor. Newly, a fiber optic gyro and a head IMU sensor are attached to the hip and neck of DRC-Hubo+ respectively for more robust tracking of the vehicle (which the robot drives)\u2019s position and rotation information. The control system is also updated to reflect mechanical and electronic changes of DRC-Hubo+. 3, the control system consists of sensor, robot and operator computers."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Newly, a fiber optic gyro and a head IMU sensor are attached to the hip and neck of DRC-Hubo+ respectively for more robust tracking of the vehicle (which the robot drives)\u2019s position and rotation information.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fdf48f86-0981-460d-931e-96486505d1ad": {"__data__": {"id_": "fdf48f86-0981-460d-931e-96486505d1ad", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "In its head, DRC-Hubo+ is equipped with a long distance range Lidar, multi camera system, four force-torque (F/T) sensors, and a body IMU sensor. Newly, a fiber optic gyro and a head IMU sensor are attached to the hip and neck of DRC-Hubo+ respectively for more robust tracking of the vehicle (which the robot drives)\u2019s position and rotation information. The control system is also updated to reflect mechanical and electronic changes of DRC-Hubo+. 3, the control system consists of sensor, robot and operator computers. The operator computer has a sensor client and a robot client to communicate each with a sensor data server (in the sensor computer) and a robot control server (in the robot computer)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The control system is also updated to reflect mechanical and electronic changes of DRC-Hubo+.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a507592-d012-459e-a9d2-deb1a9ff880d": {"__data__": {"id_": "1a507592-d012-459e-a9d2-deb1a9ff880d", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "Newly, a fiber optic gyro and a head IMU sensor are attached to the hip and neck of DRC-Hubo+ respectively for more robust tracking of the vehicle (which the robot drives)\u2019s position and rotation information. The control system is also updated to reflect mechanical and electronic changes of DRC-Hubo+. 3, the control system consists of sensor, robot and operator computers. The operator computer has a sensor client and a robot client to communicate each with a sensor data server (in the sensor computer) and a robot control server (in the robot computer). The communication between servers and clients is established by a wireless network for remote control (of DRC-Hubo+ for driving task)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3, the control system consists of sensor, robot and operator computers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28434f20-7575-4608-9a9c-ce9ffb12610a": {"__data__": {"id_": "28434f20-7575-4608-9a9c-ce9ffb12610a", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "The control system is also updated to reflect mechanical and electronic changes of DRC-Hubo+. 3, the control system consists of sensor, robot and operator computers. The operator computer has a sensor client and a robot client to communicate each with a sensor data server (in the sensor computer) and a robot control server (in the robot computer). The communication between servers and clients is established by a wireless network for remote control (of DRC-Hubo+ for driving task). For utilizing the wireless network, several limitations were also introduced with consideration of the disaster scenario (which is the motivation of DRC)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The operator computer has a sensor client and a robot client to communicate each with a sensor data server (in the sensor computer) and a robot control server (in the robot computer).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64ddac62-086f-4d12-a8bf-ab6b9b0407a4": {"__data__": {"id_": "64ddac62-086f-4d12-a8bf-ab6b9b0407a4", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "3, the control system consists of sensor, robot and operator computers. The operator computer has a sensor client and a robot client to communicate each with a sensor data server (in the sensor computer) and a robot control server (in the robot computer). The communication between servers and clients is established by a wireless network for remote control (of DRC-Hubo+ for driving task). For utilizing the wireless network, several limitations were also introduced with consideration of the disaster scenario (which is the motivation of DRC). They include the bandwidth (300 Mbit/s) and packet size limits and data loss during communication."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The communication between servers and clients is established by a wireless network for remote control (of DRC-Hubo+ for driving task).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25deed2f-309b-4914-aec6-afd021a1cf23": {"__data__": {"id_": "25deed2f-309b-4914-aec6-afd021a1cf23", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "The operator computer has a sensor client and a robot client to communicate each with a sensor data server (in the sensor computer) and a robot control server (in the robot computer). The communication between servers and clients is established by a wireless network for remote control (of DRC-Hubo+ for driving task). For utilizing the wireless network, several limitations were also introduced with consideration of the disaster scenario (which is the motivation of DRC). They include the bandwidth (300 Mbit/s) and packet size limits and data loss during communication. each data collection module in the sensor computer also converted its input data (such as raw images, Lidar, IMU and raw audio signal) to the size-reduced data (such as low resolution images, 3D points, an acceleration and a yaw angle, and mean value of raw audio) to minimize its size."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For utilizing the wireless network, several limitations were also introduced with consideration of the disaster scenario (which is the motivation of DRC).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "384cf783-64e1-44e3-9508-264637f0671f": {"__data__": {"id_": "384cf783-64e1-44e3-9508-264637f0671f", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "The communication between servers and clients is established by a wireless network for remote control (of DRC-Hubo+ for driving task). For utilizing the wireless network, several limitations were also introduced with consideration of the disaster scenario (which is the motivation of DRC). They include the bandwidth (300 Mbit/s) and packet size limits and data loss during communication. each data collection module in the sensor computer also converted its input data (such as raw images, Lidar, IMU and raw audio signal) to the size-reduced data (such as low resolution images, 3D points, an acceleration and a yaw angle, and mean value of raw audio) to minimize its size. The sensor server then transmitted the processed data to sensor client in the operator computer with 15 Hz of latency time."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They include the bandwidth (300 Mbit/s) and packet size limits and data loss during communication.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e9678d6-3640-4bd5-b0c7-4719813050a3": {"__data__": {"id_": "2e9678d6-3640-4bd5-b0c7-4719813050a3", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "For utilizing the wireless network, several limitations were also introduced with consideration of the disaster scenario (which is the motivation of DRC). They include the bandwidth (300 Mbit/s) and packet size limits and data loss during communication. each data collection module in the sensor computer also converted its input data (such as raw images, Lidar, IMU and raw audio signal) to the size-reduced data (such as low resolution images, 3D points, an acceleration and a yaw angle, and mean value of raw audio) to minimize its size. The sensor server then transmitted the processed data to sensor client in the operator computer with 15 Hz of latency time. 2 a DRC-Hubo+ platform upgrade for DRC-Finals and b its joint coordinate and kinematic structure"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "each data collection module in the sensor computer also converted its input data (such as raw images, Lidar, IMU and raw audio signal) to the size-reduced data (such as low resolution images, 3D points, an acceleration and a yaw angle, and mean value of raw audio) to minimize its size.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2f81fcc-4bec-4b4a-af07-9edbc5162e54": {"__data__": {"id_": "f2f81fcc-4bec-4b4a-af07-9edbc5162e54", "embedding": null, "metadata": {"page_number": 3, "source": "s10846-019-01130-x.pdf", "window": "They include the bandwidth (300 Mbit/s) and packet size limits and data loss during communication. each data collection module in the sensor computer also converted its input data (such as raw images, Lidar, IMU and raw audio signal) to the size-reduced data (such as low resolution images, 3D points, an acceleration and a yaw angle, and mean value of raw audio) to minimize its size. The sensor server then transmitted the processed data to sensor client in the operator computer with 15 Hz of latency time. 2 a DRC-Hubo+ platform upgrade for DRC-Finals and b its joint coordinate and kinematic structure The processed sensor data are visualized by a driving information module in the operator computer."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sensor server then transmitted the processed data to sensor client in the operator computer with 15 Hz of latency time.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "686dd075-9356-46dc-a340-904e3e4daaa5": {"__data__": {"id_": "686dd075-9356-46dc-a340-904e3e4daaa5", "embedding": null, "metadata": {"page_number": 4, "source": "s10846-019-01130-x.pdf", "window": "each data collection module in the sensor computer also converted its input data (such as raw images, Lidar, IMU and raw audio signal) to the size-reduced data (such as low resolution images, 3D points, an acceleration and a yaw angle, and mean value of raw audio) to minimize its size. The sensor server then transmitted the processed data to sensor client in the operator computer with 15 Hz of latency time. 2 a DRC-Hubo+ platform upgrade for DRC-Finals and b its joint coordinate and kinematic structure The processed sensor data are visualized by a driving information module in the operator computer. The informa- tion module also implemented two core functionalities for humanoid\u2019s vehicle driving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 a DRC-Hubo+ platform upgrade for DRC-Finals and b its joint coordinate and kinematic structure", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04ceec14-2962-44d4-a9fa-993a08af7e46": {"__data__": {"id_": "04ceec14-2962-44d4-a9fa-993a08af7e46", "embedding": null, "metadata": {"page_number": 4, "source": "s10846-019-01130-x.pdf", "window": "The sensor server then transmitted the processed data to sensor client in the operator computer with 15 Hz of latency time. 2 a DRC-Hubo+ platform upgrade for DRC-Finals and b its joint coordinate and kinematic structure The processed sensor data are visualized by a driving information module in the operator computer. The informa- tion module also implemented two core functionalities for humanoid\u2019s vehicle driving. They include sensor data fusion and advanced driving assistant function."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The processed sensor data are visualized by a driving information module in the operator computer.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e14179fa-d9cc-4ecd-b7bd-c1637c4cedd4": {"__data__": {"id_": "e14179fa-d9cc-4ecd-b7bd-c1637c4cedd4", "embedding": null, "metadata": {"page_number": 4, "source": "s10846-019-01130-x.pdf", "window": "2 a DRC-Hubo+ platform upgrade for DRC-Finals and b its joint coordinate and kinematic structure The processed sensor data are visualized by a driving information module in the operator computer. The informa- tion module also implemented two core functionalities for humanoid\u2019s vehicle driving. They include sensor data fusion and advanced driving assistant function. features provided the operator with more stable and easier control of the vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The informa- tion module also implemented two core functionalities for humanoid\u2019s vehicle driving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2759085b-82ee-4370-a98b-c94f627a1f23": {"__data__": {"id_": "2759085b-82ee-4370-a98b-c94f627a1f23", "embedding": null, "metadata": {"page_number": 4, "source": "s10846-019-01130-x.pdf", "window": "The processed sensor data are visualized by a driving information module in the operator computer. The informa- tion module also implemented two core functionalities for humanoid\u2019s vehicle driving. They include sensor data fusion and advanced driving assistant function. features provided the operator with more stable and easier control of the vehicle. More details with the driving assistant system is described in Section 5."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They include sensor data fusion and advanced driving assistant function.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50953c0d-08f4-4bf7-887d-0824a3ae4b21": {"__data__": {"id_": "50953c0d-08f4-4bf7-887d-0824a3ae4b21", "embedding": null, "metadata": {"page_number": 4, "source": "s10846-019-01130-x.pdf", "window": "The informa- tion module also implemented two core functionalities for humanoid\u2019s vehicle driving. They include sensor data fusion and advanced driving assistant function. features provided the operator with more stable and easier control of the vehicle. More details with the driving assistant system is described in Section 5. the driving control module in the operator computer received the manipulation input (wheel angle and"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "features provided the operator with more stable and easier control of the vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9ef1ab7-8aa7-4410-bcf4-999baf51a6ad": {"__data__": {"id_": "d9ef1ab7-8aa7-4410-bcf4-999baf51a6ad", "embedding": null, "metadata": {"page_number": 4, "source": "s10846-019-01130-x.pdf", "window": "They include sensor data fusion and advanced driving assistant function. features provided the operator with more stable and easier control of the vehicle. More details with the driving assistant system is described in Section 5. the driving control module in the operator computer received the manipulation input (wheel angle and Combined with the output from the driving assistant system of the information module, the final control inputs (such as hand yaw and ankle pitch positions) are transferred to hand and foot position control modules (in the robot computer) respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More details with the driving assistant system is described in Section 5.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da421034-9367-42e2-9aa9-a3b7fe176bf4": {"__data__": {"id_": "da421034-9367-42e2-9aa9-a3b7fe176bf4", "embedding": null, "metadata": {"page_number": 4, "source": "s10846-019-01130-x.pdf", "window": "features provided the operator with more stable and easier control of the vehicle. More details with the driving assistant system is described in Section 5. the driving control module in the operator computer received the manipulation input (wheel angle and Combined with the output from the driving assistant system of the information module, the final control inputs (such as hand yaw and ankle pitch positions) are transferred to hand and foot position control modules (in the robot computer) respectively. The control message communication speed was also set to 15 Hz in the system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the driving control module in the operator computer received the manipulation input (wheel angle and", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29d4db7e-f76d-4d34-bcf8-79dc57b56f72": {"__data__": {"id_": "29d4db7e-f76d-4d34-bcf8-79dc57b56f72", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "More details with the driving assistant system is described in Section 5. the driving control module in the operator computer received the manipulation input (wheel angle and Combined with the output from the driving assistant system of the information module, the final control inputs (such as hand yaw and ankle pitch positions) are transferred to hand and foot position control modules (in the robot computer) respectively. The control message communication speed was also set to 15 Hz in the system. The sensor data server and the robot control module of the architecture above are built and run in the head computer (Intel NUC i5 processor, 3M Cache 1.8 GHz) and the body computer (same specification with the head computer) respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Combined with the output from the driving assistant system of the information module, the final control inputs (such as hand yaw and ankle pitch positions) are transferred to hand and foot position control modules (in the robot computer) respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02328d9d-b01c-430a-a14c-a556c4f6c998": {"__data__": {"id_": "02328d9d-b01c-430a-a14c-a556c4f6c998", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "the driving control module in the operator computer received the manipulation input (wheel angle and Combined with the output from the driving assistant system of the information module, the final control inputs (such as hand yaw and ankle pitch positions) are transferred to hand and foot position control modules (in the robot computer) respectively. The control message communication speed was also set to 15 Hz in the system. The sensor data server and the robot control module of the architecture above are built and run in the head computer (Intel NUC i5 processor, 3M Cache 1.8 GHz) and the body computer (same specification with the head computer) respectively. The sensor data client and the driving control module are both installed in the operator computer (Dell Alienware i7 processor, 4 GHz)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The control message communication speed was also set to 15 Hz in the system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5be59c0e-a045-4202-923b-d29c551deffb": {"__data__": {"id_": "5be59c0e-a045-4202-923b-d29c551deffb", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Combined with the output from the driving assistant system of the information module, the final control inputs (such as hand yaw and ankle pitch positions) are transferred to hand and foot position control modules (in the robot computer) respectively. The control message communication speed was also set to 15 Hz in the system. The sensor data server and the robot control module of the architecture above are built and run in the head computer (Intel NUC i5 processor, 3M Cache 1.8 GHz) and the body computer (same specification with the head computer) respectively. The sensor data client and the driving control module are both installed in the operator computer (Dell Alienware i7 processor, 4 GHz). rotation of the sensor head caused unexpected distortion of electric cables during the competition and it resulted in serious failure of power-supply system in the head."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sensor data server and the robot control module of the architecture above are built and run in the head computer (Intel NUC i5 processor, 3M Cache 1.8 GHz) and the body computer (same specification with the head computer) respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "989b1dc6-8f5e-429b-a0bd-06f5731c5ef9": {"__data__": {"id_": "989b1dc6-8f5e-429b-a0bd-06f5731c5ef9", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "The control message communication speed was also set to 15 Hz in the system. The sensor data server and the robot control module of the architecture above are built and run in the head computer (Intel NUC i5 processor, 3M Cache 1.8 GHz) and the body computer (same specification with the head computer) respectively. The sensor data client and the driving control module are both installed in the operator computer (Dell Alienware i7 processor, 4 GHz). rotation of the sensor head caused unexpected distortion of electric cables during the competition and it resulted in serious failure of power-supply system in the head. the sensor head on DRC-Hubo+, shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sensor data client and the driving control module are both installed in the operator computer (Dell Alienware i7 processor, 4 GHz).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1cb9fc5-6517-498e-bdaf-54bca61fd0ac": {"__data__": {"id_": "a1cb9fc5-6517-498e-bdaf-54bca61fd0ac", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "The sensor data server and the robot control module of the architecture above are built and run in the head computer (Intel NUC i5 processor, 3M Cache 1.8 GHz) and the body computer (same specification with the head computer) respectively. The sensor data client and the driving control module are both installed in the operator computer (Dell Alienware i7 processor, 4 GHz). rotation of the sensor head caused unexpected distortion of electric cables during the competition and it resulted in serious failure of power-supply system in the head. the sensor head on DRC-Hubo+, shown in Fig. 4, is newly designed and built for DRC-Finals."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "rotation of the sensor head caused unexpected distortion of electric cables during the competition and it resulted in serious failure of power-supply system in the head.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63ae930a-ec97-4d35-b853-5b2a4f506e79": {"__data__": {"id_": "63ae930a-ec97-4d35-b853-5b2a4f506e79", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "The sensor data client and the driving control module are both installed in the operator computer (Dell Alienware i7 processor, 4 GHz). rotation of the sensor head caused unexpected distortion of electric cables during the competition and it resulted in serious failure of power-supply system in the head. the sensor head on DRC-Hubo+, shown in Fig. 4, is newly designed and built for DRC-Finals. It tilts with 180\u25e6 without self-collision and can observe surrounding environment (180\u25e6) without pan rotation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the sensor head on DRC-Hubo+, shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "072ebbf6-076b-4e7a-abcd-5e2dfdc30b6a": {"__data__": {"id_": "072ebbf6-076b-4e7a-abcd-5e2dfdc30b6a", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "rotation of the sensor head caused unexpected distortion of electric cables during the competition and it resulted in serious failure of power-supply system in the head. the sensor head on DRC-Hubo+, shown in Fig. 4, is newly designed and built for DRC-Finals. It tilts with 180\u25e6 without self-collision and can observe surrounding environment (180\u25e6) without pan rotation. The upgraded head has the following sensors which are all necessary for driving task:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4, is newly designed and built for DRC-Finals.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76915387-56f1-424d-9af2-c3b315c210e6": {"__data__": {"id_": "76915387-56f1-424d-9af2-c3b315c210e6", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "the sensor head on DRC-Hubo+, shown in Fig. 4, is newly designed and built for DRC-Finals. It tilts with 180\u25e6 without self-collision and can observe surrounding environment (180\u25e6) without pan rotation. The upgraded head has the following sensors which are all necessary for driving task: Grey Flea3 camera with manual iris c-mount lens (Kowa 1/2 in."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It tilts with 180\u25e6 without self-collision and can observe surrounding environment (180\u25e6) without pan rotation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38a7f4df-1a1b-4561-92e8-8fed6878b77e": {"__data__": {"id_": "38a7f4df-1a1b-4561-92e8-8fed6878b77e", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "4, is newly designed and built for DRC-Finals. It tilts with 180\u25e6 without self-collision and can observe surrounding environment (180\u25e6) without pan rotation. The upgraded head has the following sensors which are all necessary for driving task: Grey Flea3 camera with manual iris c-mount lens (Kowa 1/2 in. 3.5 mm F2.4): 1280 x 1024 resolution with about a 90\u25e6 x 70\u25e6 field of view (FOV)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The upgraded head has the following sensors which are all necessary for driving task:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da994521-422f-4de5-bb21-dd8076b2a926": {"__data__": {"id_": "da994521-422f-4de5-bb21-dd8076b2a926", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "It tilts with 180\u25e6 without self-collision and can observe surrounding environment (180\u25e6) without pan rotation. The upgraded head has the following sensors which are all necessary for driving task: Grey Flea3 camera with manual iris c-mount lens (Kowa 1/2 in. 3.5 mm F2.4): 1280 x 1024 resolution with about a 90\u25e6 x 70\u25e6 field of view (FOV). Used for wide-zoom color image acquisition of front-side view."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Grey Flea3 camera with manual iris c-mount lens (Kowa 1/2 in.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "babeccba-d642-4bbb-91ae-5f7eaebef792": {"__data__": {"id_": "babeccba-d642-4bbb-91ae-5f7eaebef792", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "The upgraded head has the following sensors which are all necessary for driving task: Grey Flea3 camera with manual iris c-mount lens (Kowa 1/2 in. 3.5 mm F2.4): 1280 x 1024 resolution with about a 90\u25e6 x 70\u25e6 field of view (FOV). Used for wide-zoom color image acquisition of front-side view. Hokuyo UTM-30LX-EW laser range-finder: It scans at 40 Hz over a 270\u25e6 FOV at an angular resolution of 0.25\u25e6."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.5 mm F2.4): 1280 x 1024 resolution with about a 90\u25e6 x 70\u25e6 field of view (FOV).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2df34c46-311c-4d79-bd9d-50fe414bc64f": {"__data__": {"id_": "2df34c46-311c-4d79-bd9d-50fe414bc64f", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Grey Flea3 camera with manual iris c-mount lens (Kowa 1/2 in. 3.5 mm F2.4): 1280 x 1024 resolution with about a 90\u25e6 x 70\u25e6 field of view (FOV). Used for wide-zoom color image acquisition of front-side view. Hokuyo UTM-30LX-EW laser range-finder: It scans at 40 Hz over a 270\u25e6 FOV at an angular resolution of 0.25\u25e6. The minimum detectable depth is 0.1 m and the maximum is 30 m. The Lidar is mounted on a dedicated tilting servo which can rotate between -90\u25e6 and +-90\u25e6."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Used for wide-zoom color image acquisition of front-side view.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4bc23479-8a0d-4b3f-b6d1-2fd2091b1bd3": {"__data__": {"id_": "4bc23479-8a0d-4b3f-b6d1-2fd2091b1bd3", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "3.5 mm F2.4): 1280 x 1024 resolution with about a 90\u25e6 x 70\u25e6 field of view (FOV). Used for wide-zoom color image acquisition of front-side view. Hokuyo UTM-30LX-EW laser range-finder: It scans at 40 Hz over a 270\u25e6 FOV at an angular resolution of 0.25\u25e6. The minimum detectable depth is 0.1 m and the maximum is 30 m. The Lidar is mounted on a dedicated tilting servo which can rotate between -90\u25e6 and +-90\u25e6. Aligned with the front-side view camera (in vertical) to combine image pixel values (from the camera) with intensity-like reflectance information (from the Hokyuo) for each point."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hokuyo UTM-30LX-EW laser range-finder: It scans at 40 Hz over a 270\u25e6 FOV at an angular resolution of 0.25\u25e6.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c561fd9-f0a3-4e37-afea-719c3b5e7b2d": {"__data__": {"id_": "4c561fd9-f0a3-4e37-afea-719c3b5e7b2d", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Used for wide-zoom color image acquisition of front-side view. Hokuyo UTM-30LX-EW laser range-finder: It scans at 40 Hz over a 270\u25e6 FOV at an angular resolution of 0.25\u25e6. The minimum detectable depth is 0.1 m and the maximum is 30 m. The Lidar is mounted on a dedicated tilting servo which can rotate between -90\u25e6 and +-90\u25e6. Aligned with the front-side view camera (in vertical) to combine image pixel values (from the camera) with intensity-like reflectance information (from the Hokyuo) for each point. Two Logitech Webcam Pro 9000 cameras: 1600 x 1200 resolution with Carl Zeiss ultra-wide angle lens."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The minimum detectable depth is 0.1 m and the maximum is 30 m. The Lidar is mounted on a dedicated tilting servo which can rotate between -90\u25e6 and +-90\u25e6.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d188a54c-d247-4a5e-9642-c09ddce037e4": {"__data__": {"id_": "d188a54c-d247-4a5e-9642-c09ddce037e4", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Hokuyo UTM-30LX-EW laser range-finder: It scans at 40 Hz over a 270\u25e6 FOV at an angular resolution of 0.25\u25e6. The minimum detectable depth is 0.1 m and the maximum is 30 m. The Lidar is mounted on a dedicated tilting servo which can rotate between -90\u25e6 and +-90\u25e6. Aligned with the front-side view camera (in vertical) to combine image pixel values (from the camera) with intensity-like reflectance information (from the Hokyuo) for each point. Two Logitech Webcam Pro 9000 cameras: 1600 x 1200 resolution with Carl Zeiss ultra-wide angle lens. Aligned with the front-side view camera, they provided DRC-Hubo+ with 180\u25e6 camera-view of driving course."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Aligned with the front-side view camera (in vertical) to combine image pixel values (from the camera) with intensity-like reflectance information (from the Hokyuo) for each point.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7f5bc5a-7aa4-4702-9985-21e79617638b": {"__data__": {"id_": "e7f5bc5a-7aa4-4702-9985-21e79617638b", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "The minimum detectable depth is 0.1 m and the maximum is 30 m. The Lidar is mounted on a dedicated tilting servo which can rotate between -90\u25e6 and +-90\u25e6. Aligned with the front-side view camera (in vertical) to combine image pixel values (from the camera) with intensity-like reflectance information (from the Hokyuo) for each point. Two Logitech Webcam Pro 9000 cameras: 1600 x 1200 resolution with Carl Zeiss ultra-wide angle lens. Aligned with the front-side view camera, they provided DRC-Hubo+ with 180\u25e6 camera-view of driving course. 4. x-IMU with 3-axis accelerometer, 3-axis magnetometer and 3-axis gyro."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Two Logitech Webcam Pro 9000 cameras: 1600 x 1200 resolution with Carl Zeiss ultra-wide angle lens.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c667707d-8b8c-4ee2-a8fb-af8f5c19803e": {"__data__": {"id_": "c667707d-8b8c-4ee2-a8fb-af8f5c19803e", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Aligned with the front-side view camera (in vertical) to combine image pixel values (from the camera) with intensity-like reflectance information (from the Hokyuo) for each point. Two Logitech Webcam Pro 9000 cameras: 1600 x 1200 resolution with Carl Zeiss ultra-wide angle lens. Aligned with the front-side view camera, they provided DRC-Hubo+ with 180\u25e6 camera-view of driving course. 4. x-IMU with 3-axis accelerometer, 3-axis magnetometer and 3-axis gyro. Section 5 presents how the collected data (from the sensor head) be processed to provide more intuitive perception and to enable DRC-Hubo+ to drive the vehicle in a fast and safe manner."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Aligned with the front-side view camera, they provided DRC-Hubo+ with 180\u25e6 camera-view of driving course.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61c6cd23-2f93-4bce-8fa8-96c0518ef2b7": {"__data__": {"id_": "61c6cd23-2f93-4bce-8fa8-96c0518ef2b7", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Two Logitech Webcam Pro 9000 cameras: 1600 x 1200 resolution with Carl Zeiss ultra-wide angle lens. Aligned with the front-side view camera, they provided DRC-Hubo+ with 180\u25e6 camera-view of driving course. 4. x-IMU with 3-axis accelerometer, 3-axis magnetometer and 3-axis gyro. Section 5 presents how the collected data (from the sensor head) be processed to provide more intuitive perception and to enable DRC-Hubo+ to drive the vehicle in a fast and safe manner. 5 Perception Data Processing in Driving Information Module"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. x-IMU with 3-axis accelerometer, 3-axis magnetometer and 3-axis gyro.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3637ca77-dc23-4563-bd3b-b222a3925dad": {"__data__": {"id_": "3637ca77-dc23-4563-bd3b-b222a3925dad", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Aligned with the front-side view camera, they provided DRC-Hubo+ with 180\u25e6 camera-view of driving course. 4. x-IMU with 3-axis accelerometer, 3-axis magnetometer and 3-axis gyro. Section 5 presents how the collected data (from the sensor head) be processed to provide more intuitive perception and to enable DRC-Hubo+ to drive the vehicle in a fast and safe manner. 5 Perception Data Processing in Driving Information Module contestant robots were asked to drive the utility vehicle through the outdoor driving course."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 5 presents how the collected data (from the sensor head) be processed to provide more intuitive perception and to enable DRC-Hubo+ to drive the vehicle in a fast and safe manner.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cab22d28-0866-4950-b4c1-799574fb6dd8": {"__data__": {"id_": "cab22d28-0866-4950-b4c1-799574fb6dd8", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "4. x-IMU with 3-axis accelerometer, 3-axis magnetometer and 3-axis gyro. Section 5 presents how the collected data (from the sensor head) be processed to provide more intuitive perception and to enable DRC-Hubo+ to drive the vehicle in a fast and safe manner. 5 Perception Data Processing in Driving Information Module contestant robots were asked to drive the utility vehicle through the outdoor driving course. For the first task, the robot began in the vehicle, drove through the course, and crossed the finish line."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 Perception Data Processing in Driving Information Module", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b86814a7-bf2a-456c-bea5-d546c735ca86": {"__data__": {"id_": "b86814a7-bf2a-456c-bea5-d546c735ca86", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Section 5 presents how the collected data (from the sensor head) be processed to provide more intuitive perception and to enable DRC-Hubo+ to drive the vehicle in a fast and safe manner. 5 Perception Data Processing in Driving Information Module contestant robots were asked to drive the utility vehicle through the outdoor driving course. For the first task, the robot began in the vehicle, drove through the course, and crossed the finish line. The task was considered complete when both rear wheels of the vehicle have crossed the finish line."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "contestant robots were asked to drive the utility vehicle through the outdoor driving course.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fd94a1b-6627-4722-b12c-7a5f3dad0cba": {"__data__": {"id_": "4fd94a1b-6627-4722-b12c-7a5f3dad0cba", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "5 Perception Data Processing in Driving Information Module contestant robots were asked to drive the utility vehicle through the outdoor driving course. For the first task, the robot began in the vehicle, drove through the course, and crossed the finish line. The task was considered complete when both rear wheels of the vehicle have crossed the finish line. As described in Section 1, the authors developed a new perception data processing system (driving information module of control system architecture in Section 3) which is optimized for driving task above."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the first task, the robot began in the vehicle, drove through the course, and crossed the finish line.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af69ac24-a31c-46a7-bf82-1a0fc27514ff": {"__data__": {"id_": "af69ac24-a31c-46a7-bf82-1a0fc27514ff", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "contestant robots were asked to drive the utility vehicle through the outdoor driving course. For the first task, the robot began in the vehicle, drove through the course, and crossed the finish line. The task was considered complete when both rear wheels of the vehicle have crossed the finish line. As described in Section 1, the authors developed a new perception data processing system (driving information module of control system architecture in Section 3) which is optimized for driving task above. related data from different sensors were merged through the data fusion process."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The task was considered complete when both rear wheels of the vehicle have crossed the finish line.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "802a1399-cd48-4005-9b58-3cd02a5e80bd": {"__data__": {"id_": "802a1399-cd48-4005-9b58-3cd02a5e80bd", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "For the first task, the robot began in the vehicle, drove through the course, and crossed the finish line. The task was considered complete when both rear wheels of the vehicle have crossed the finish line. As described in Section 1, the authors developed a new perception data processing system (driving information module of control system architecture in Section 3) which is optimized for driving task above. related data from different sensors were merged through the data fusion process. This process provided integrative understanding of the surrounding environment to human-operator."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As described in Section 1, the authors developed a new perception data processing system (driving information module of control system architecture in Section 3) which is optimized for driving task above.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19c01d89-797d-4e6b-84a8-b2884c4c8b26": {"__data__": {"id_": "19c01d89-797d-4e6b-84a8-b2884c4c8b26", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "The task was considered complete when both rear wheels of the vehicle have crossed the finish line. As described in Section 1, the authors developed a new perception data processing system (driving information module of control system architecture in Section 3) which is optimized for driving task above. related data from different sensors were merged through the data fusion process. This process provided integrative understanding of the surrounding environment to human-operator. It also enabled the efficient obstacle segmentation process which can determine the non-obstacle areas from the driving course."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "related data from different sensors were merged through the data fusion process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f43f3a7f-984c-41d4-a7b5-a573bb37c17e": {"__data__": {"id_": "f43f3a7f-984c-41d4-a7b5-a573bb37c17e", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "As described in Section 1, the authors developed a new perception data processing system (driving information module of control system architecture in Section 3) which is optimized for driving task above. related data from different sensors were merged through the data fusion process. This process provided integrative understanding of the surrounding environment to human-operator. It also enabled the efficient obstacle segmentation process which can determine the non-obstacle areas from the driving course. More details with sensor data fusion and its application are described in Section 5.1."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This process provided integrative understanding of the surrounding environment to human-operator.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a476059f-5279-4371-83e9-c070d9b608d0": {"__data__": {"id_": "a476059f-5279-4371-83e9-c070d9b608d0", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "related data from different sensors were merged through the data fusion process. This process provided integrative understanding of the surrounding environment to human-operator. It also enabled the efficient obstacle segmentation process which can determine the non-obstacle areas from the driving course. More details with sensor data fusion and its application are described in Section 5.1. unlike the previous tele-control approach (in DRC-Trials) which relied mainly on human operator\u2019s manual operation, the newly developed system adopted various advanced driver assistant functionalities."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It also enabled the efficient obstacle segmentation process which can determine the non-obstacle areas from the driving course.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db6ee8a3-1422-4c72-ae2f-f18f62ebe942": {"__data__": {"id_": "db6ee8a3-1422-4c72-ae2f-f18f62ebe942", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "This process provided integrative understanding of the surrounding environment to human-operator. It also enabled the efficient obstacle segmentation process which can determine the non-obstacle areas from the driving course. More details with sensor data fusion and its application are described in Section 5.1. unlike the previous tele-control approach (in DRC-Trials) which relied mainly on human operator\u2019s manual operation, the newly developed system adopted various advanced driver assistant functionalities. Thorough the kinematics modeling of the vehicle (Polaris Ranger XP 900) and sensor-measurement, the expected vehicle-path was computed."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More details with sensor data fusion and its application are described in Section 5.1.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "675e3e96-5a37-4131-8e3d-33c9a101da9f": {"__data__": {"id_": "675e3e96-5a37-4131-8e3d-33c9a101da9f", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "It also enabled the efficient obstacle segmentation process which can determine the non-obstacle areas from the driving course. More details with sensor data fusion and its application are described in Section 5.1. unlike the previous tele-control approach (in DRC-Trials) which relied mainly on human operator\u2019s manual operation, the newly developed system adopted various advanced driver assistant functionalities. Thorough the kinematics modeling of the vehicle (Polaris Ranger XP 900) and sensor-measurement, the expected vehicle-path was computed. It made the operator to estimate the vehicle\u2019s position with the control input of the moment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "unlike the previous tele-control approach (in DRC-Trials) which relied mainly on human operator\u2019s manual operation, the newly developed system adopted various advanced driver assistant functionalities.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4931d35-509e-4fbb-b2df-a704477dcf21": {"__data__": {"id_": "e4931d35-509e-4fbb-b2df-a704477dcf21", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "More details with sensor data fusion and its application are described in Section 5.1. unlike the previous tele-control approach (in DRC-Trials) which relied mainly on human operator\u2019s manual operation, the newly developed system adopted various advanced driver assistant functionalities. Thorough the kinematics modeling of the vehicle (Polaris Ranger XP 900) and sensor-measurement, the expected vehicle-path was computed. It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among all the possible vehicle-paths, the most optimized path was found in terms of driving time and collision avoidance."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thorough the kinematics modeling of the vehicle (Polaris Ranger XP 900) and sensor-measurement, the expected vehicle-path was computed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1343a991-eddb-4146-b2b5-ab0b94d0d499": {"__data__": {"id_": "1343a991-eddb-4146-b2b5-ab0b94d0d499", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "unlike the previous tele-control approach (in DRC-Trials) which relied mainly on human operator\u2019s manual operation, the newly developed system adopted various advanced driver assistant functionalities. Thorough the kinematics modeling of the vehicle (Polaris Ranger XP 900) and sensor-measurement, the expected vehicle-path was computed. It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among all the possible vehicle-paths, the most optimized path was found in terms of driving time and collision avoidance. Section 5.2 presents how the driver assistant functionalities were developed and implemented in the new system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It made the operator to estimate the vehicle\u2019s position with the control input of the moment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "639d1a44-2690-4d4d-b762-70284f21d9e1": {"__data__": {"id_": "639d1a44-2690-4d4d-b762-70284f21d9e1", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Thorough the kinematics modeling of the vehicle (Polaris Ranger XP 900) and sensor-measurement, the expected vehicle-path was computed. It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among all the possible vehicle-paths, the most optimized path was found in terms of driving time and collision avoidance. Section 5.2 presents how the driver assistant functionalities were developed and implemented in the new system. As presented in Section 4, the sensor head of DRC-Hubo+ has three different cameras (1 frontal view and 2 lateral"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among all the possible vehicle-paths, the most optimized path was found in terms of driving time and collision avoidance.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d8dc40d-1eaf-426e-95aa-f2db4faae8d9": {"__data__": {"id_": "3d8dc40d-1eaf-426e-95aa-f2db4faae8d9", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "It made the operator to estimate the vehicle\u2019s position with the control input of the moment. Among all the possible vehicle-paths, the most optimized path was found in terms of driving time and collision avoidance. Section 5.2 presents how the driver assistant functionalities were developed and implemented in the new system. As presented in Section 4, the sensor head of DRC-Hubo+ has three different cameras (1 frontal view and 2 lateral 2Lewis, T., Darpa Robotics Challenge, June 6, 2015."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 5.2 presents how the driver assistant functionalities were developed and implemented in the new system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85f57f83-c71a-4d96-ac88-454c34ef7dc6": {"__data__": {"id_": "85f57f83-c71a-4d96-ac88-454c34ef7dc6", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Among all the possible vehicle-paths, the most optimized path was found in terms of driving time and collision avoidance. Section 5.2 presents how the driver assistant functionalities were developed and implemented in the new system. As presented in Section 4, the sensor head of DRC-Hubo+ has three different cameras (1 frontal view and 2 lateral 2Lewis, T., Darpa Robotics Challenge, June 6, 2015. side view) and 1 Lidar system (laser range finder)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As presented in Section 4, the sensor head of DRC-Hubo+ has three different cameras (1 frontal view and 2 lateral", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89283646-6de8-4f96-a82f-2955ca8e2097": {"__data__": {"id_": "89283646-6de8-4f96-a82f-2955ca8e2097", "embedding": null, "metadata": {"page_number": 5, "source": "s10846-019-01130-x.pdf", "window": "Section 5.2 presents how the driver assistant functionalities were developed and implemented in the new system. As presented in Section 4, the sensor head of DRC-Hubo+ has three different cameras (1 frontal view and 2 lateral 2Lewis, T., Darpa Robotics Challenge, June 6, 2015. side view) and 1 Lidar system (laser range finder). The frontal view camera is located on top of the Lidar system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2Lewis, T., Darpa Robotics Challenge, June 6, 2015.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84ad1be7-7555-4020-9b45-06060b679153": {"__data__": {"id_": "84ad1be7-7555-4020-9b45-06060b679153", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "As presented in Section 4, the sensor head of DRC-Hubo+ has three different cameras (1 frontal view and 2 lateral 2Lewis, T., Darpa Robotics Challenge, June 6, 2015. side view) and 1 Lidar system (laser range finder). The frontal view camera is located on top of the Lidar system. Based on the kinematics (between the frontal view camera and Lidar system) and intrinsic camera matrix data (from calibration process of the frontal view camera), the 3D point cloud data was converted to range-image."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "side view) and 1 Lidar system (laser range finder).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4474d54-061a-4010-953d-9f752cc5e6fe": {"__data__": {"id_": "c4474d54-061a-4010-953d-9f752cc5e6fe", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "2Lewis, T., Darpa Robotics Challenge, June 6, 2015. side view) and 1 Lidar system (laser range finder). The frontal view camera is located on top of the Lidar system. Based on the kinematics (between the frontal view camera and Lidar system) and intrinsic camera matrix data (from calibration process of the frontal view camera), the 3D point cloud data was converted to range-image. After 2D visualization, pixel matching between the original cam- era image and the range-image was implemented and it pro- vided color info to the original point cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The frontal view camera is located on top of the Lidar system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd60365b-d6fd-43f4-bc38-a152a80b577d": {"__data__": {"id_": "cd60365b-d6fd-43f4-bc38-a152a80b577d", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "side view) and 1 Lidar system (laser range finder). The frontal view camera is located on top of the Lidar system. Based on the kinematics (between the frontal view camera and Lidar system) and intrinsic camera matrix data (from calibration process of the frontal view camera), the 3D point cloud data was converted to range-image. After 2D visualization, pixel matching between the original cam- era image and the range-image was implemented and it pro- vided color info to the original point cloud data. The net result is that the image and point cloud data from the two different sen- sors become merged through the data fusion process (which consists of 2D visualization and pixel matching)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Based on the kinematics (between the frontal view camera and Lidar system) and intrinsic camera matrix data (from calibration process of the frontal view camera), the 3D point cloud data was converted to range-image.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa194ec6-8164-49de-86de-2483a683687b": {"__data__": {"id_": "aa194ec6-8164-49de-86de-2483a683687b", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "The frontal view camera is located on top of the Lidar system. Based on the kinematics (between the frontal view camera and Lidar system) and intrinsic camera matrix data (from calibration process of the frontal view camera), the 3D point cloud data was converted to range-image. After 2D visualization, pixel matching between the original cam- era image and the range-image was implemented and it pro- vided color info to the original point cloud data. The net result is that the image and point cloud data from the two different sen- sors become merged through the data fusion process (which consists of 2D visualization and pixel matching). points with no color info exist in the processed cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After 2D visualization, pixel matching between the original cam- era image and the range-image was implemented and it pro- vided color info to the original point cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d84730f-7130-4a46-b8f0-a6aed480602e": {"__data__": {"id_": "8d84730f-7130-4a46-b8f0-a6aed480602e", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "Based on the kinematics (between the frontal view camera and Lidar system) and intrinsic camera matrix data (from calibration process of the frontal view camera), the 3D point cloud data was converted to range-image. After 2D visualization, pixel matching between the original cam- era image and the range-image was implemented and it pro- vided color info to the original point cloud data. The net result is that the image and point cloud data from the two different sen- sors become merged through the data fusion process (which consists of 2D visualization and pixel matching). points with no color info exist in the processed cloud data. It is because the point is out of scope in the frontal view camera perception."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The net result is that the image and point cloud data from the two different sen- sors become merged through the data fusion process (which consists of 2D visualization and pixel matching).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55b47aad-1197-4205-a5d5-ea41ba7560c4": {"__data__": {"id_": "55b47aad-1197-4205-a5d5-ea41ba7560c4", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "After 2D visualization, pixel matching between the original cam- era image and the range-image was implemented and it pro- vided color info to the original point cloud data. The net result is that the image and point cloud data from the two different sen- sors become merged through the data fusion process (which consists of 2D visualization and pixel matching). points with no color info exist in the processed cloud data. It is because the point is out of scope in the frontal view camera perception. Use of wider- zoom camera will increase the area of points which have color info in the cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "points with no color info exist in the processed cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9bfe696-740a-4b1f-b6d6-27a39a8d7e19": {"__data__": {"id_": "d9bfe696-740a-4b1f-b6d6-27a39a8d7e19", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "The net result is that the image and point cloud data from the two different sen- sors become merged through the data fusion process (which consists of 2D visualization and pixel matching). points with no color info exist in the processed cloud data. It is because the point is out of scope in the frontal view camera perception. Use of wider- zoom camera will increase the area of points which have color info in the cloud data. The fusion process enabled 2D image processing techniques be applicable to the collected point cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is because the point is out of scope in the frontal view camera perception.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39956655-c7e8-41fd-9caa-cd377905b6dd": {"__data__": {"id_": "39956655-c7e8-41fd-9caa-cd377905b6dd", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "points with no color info exist in the processed cloud data. It is because the point is out of scope in the frontal view camera perception. Use of wider- zoom camera will increase the area of points which have color info in the cloud data. The fusion process enabled 2D image processing techniques be applicable to the collected point cloud data. 7 and 8, the driving path is initially segmented from the captured camera image."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Use of wider- zoom camera will increase the area of points which have color info in the cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af6898a8-d8f4-4a46-9de8-832ec55666af": {"__data__": {"id_": "af6898a8-d8f4-4a46-9de8-832ec55666af", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "It is because the point is out of scope in the frontal view camera perception. Use of wider- zoom camera will increase the area of points which have color info in the cloud data. The fusion process enabled 2D image processing techniques be applicable to the collected point cloud data. 7 and 8, the driving path is initially segmented from the captured camera image. For this, EM (expectation-maximization) algorithm with Gaussian mixture model is used."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The fusion process enabled 2D image processing techniques be applicable to the collected point cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41a080ef-1ca3-4ce0-be14-9827cc75f014": {"__data__": {"id_": "41a080ef-1ca3-4ce0-be14-9827cc75f014", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "Use of wider- zoom camera will increase the area of points which have color info in the cloud data. The fusion process enabled 2D image processing techniques be applicable to the collected point cloud data. 7 and 8, the driving path is initially segmented from the captured camera image. For this, EM (expectation-maximization) algorithm with Gaussian mixture model is used. EM algorithm is an iterative method which can find maximum likelihood estimates (MLE) or maximum a posteriori (MAP) estimates of parameters in statistical models."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 and 8, the driving path is initially segmented from the captured camera image.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17d68168-b6fb-4843-a9ba-2108f87e496b": {"__data__": {"id_": "17d68168-b6fb-4843-a9ba-2108f87e496b", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "The fusion process enabled 2D image processing techniques be applicable to the collected point cloud data. 7 and 8, the driving path is initially segmented from the captured camera image. For this, EM (expectation-maximization) algorithm with Gaussian mixture model is used. EM algorithm is an iterative method which can find maximum likelihood estimates (MLE) or maximum a posteriori (MAP) estimates of parameters in statistical models. It alternates between E step (which estimates the model\u2019s parameter and creates the log- likelihood probability distributions) and M step (which computes parameters which can maximize the expected log-likelihood found in the E step) ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this, EM (expectation-maximization) algorithm with Gaussian mixture model is used.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52aa6f40-53a3-47bb-9fd2-c5fcb7dd9701": {"__data__": {"id_": "52aa6f40-53a3-47bb-9fd2-c5fcb7dd9701", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "7 and 8, the driving path is initially segmented from the captured camera image. For this, EM (expectation-maximization) algorithm with Gaussian mixture model is used. EM algorithm is an iterative method which can find maximum likelihood estimates (MLE) or maximum a posteriori (MAP) estimates of parameters in statistical models. It alternates between E step (which estimates the model\u2019s parameter and creates the log- likelihood probability distributions) and M step (which computes parameters which can maximize the expected log-likelihood found in the E step) . Algorithm 1 presents how the method is applied for the segmentation process."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "EM algorithm is an iterative method which can find maximum likelihood estimates (MLE) or maximum a posteriori (MAP) estimates of parameters in statistical models.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95f6045e-f617-400f-a949-40338db36318": {"__data__": {"id_": "95f6045e-f617-400f-a949-40338db36318", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "For this, EM (expectation-maximization) algorithm with Gaussian mixture model is used. EM algorithm is an iterative method which can find maximum likelihood estimates (MLE) or maximum a posteriori (MAP) estimates of parameters in statistical models. It alternates between E step (which estimates the model\u2019s parameter and creates the log- likelihood probability distributions) and M step (which computes parameters which can maximize the expected log-likelihood found in the E step) . Algorithm 1 presents how the method is applied for the segmentation process. There are 2 components (obstacle class when i = 1 and driving path class when i = 2) and the i\u2019th component is wi."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It alternates between E step (which estimates the model\u2019s parameter and creates the log- likelihood probability distributions) and M step (which computes parameters which can maximize the expected log-likelihood found in the E step) .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8142cf5-001f-43b4-bbf0-617b9d5aff65": {"__data__": {"id_": "a8142cf5-001f-43b4-bbf0-617b9d5aff65", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "EM algorithm is an iterative method which can find maximum likelihood estimates (MLE) or maximum a posteriori (MAP) estimates of parameters in statistical models. It alternates between E step (which estimates the model\u2019s parameter and creates the log- likelihood probability distributions) and M step (which computes parameters which can maximize the expected log-likelihood found in the E step) . Algorithm 1 presents how the method is applied for the segmentation process. There are 2 components (obstacle class when i = 1 and driving path class when i = 2) and the i\u2019th component is wi. ui and \u03c32I are an associated mean vector and covariance matrix of the component wi."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 1 presents how the method is applied for the segmentation process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcef26a0-5b5e-447b-8807-880c00097617": {"__data__": {"id_": "fcef26a0-5b5e-447b-8807-880c00097617", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "It alternates between E step (which estimates the model\u2019s parameter and creates the log- likelihood probability distributions) and M step (which computes parameters which can maximize the expected log-likelihood found in the E step) . Algorithm 1 presents how the method is applied for the segmentation process. There are 2 components (obstacle class when i = 1 and driving path class when i = 2) and the i\u2019th component is wi. ui and \u03c32I are an associated mean vector and covariance matrix of the component wi. In the initial loop, E-step first computes the probability of expected classes (using pre-determined mean values of two classes) for each pixel in the image."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are 2 components (obstacle class when i = 1 and driving path class when i = 2) and the i\u2019th component is wi.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "beb5b6d8-5d74-4442-97bd-533acb233990": {"__data__": {"id_": "beb5b6d8-5d74-4442-97bd-533acb233990", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "Algorithm 1 presents how the method is applied for the segmentation process. There are 2 components (obstacle class when i = 1 and driving path class when i = 2) and the i\u2019th component is wi. ui and \u03c32I are an associated mean vector and covariance matrix of the component wi. In the initial loop, E-step first computes the probability of expected classes (using pre-determined mean values of two classes) for each pixel in the image. With the calculated probability, M-step computes the maximum likelihood mean value of each class."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ui and \u03c32I are an associated mean vector and covariance matrix of the component wi.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c791b4b3-aa16-4823-b2fe-b8a57cdf444f": {"__data__": {"id_": "c791b4b3-aa16-4823-b2fe-b8a57cdf444f", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "There are 2 components (obstacle class when i = 1 and driving path class when i = 2) and the i\u2019th component is wi. ui and \u03c32I are an associated mean vector and covariance matrix of the component wi. In the initial loop, E-step first computes the probability of expected classes (using pre-determined mean values of two classes) for each pixel in the image. With the calculated probability, M-step computes the maximum likelihood mean value of each class. The estimated mean values are then used in E-step of the second"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the initial loop, E-step first computes the probability of expected classes (using pre-determined mean values of two classes) for each pixel in the image.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e88032a0-8bc3-47c8-8466-b5774b2fc86c": {"__data__": {"id_": "e88032a0-8bc3-47c8-8466-b5774b2fc86c", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "ui and \u03c32I are an associated mean vector and covariance matrix of the component wi. In the initial loop, E-step first computes the probability of expected classes (using pre-determined mean values of two classes) for each pixel in the image. With the calculated probability, M-step computes the maximum likelihood mean value of each class. The estimated mean values are then used in E-step of the second 5 a Polaris Ranger XP and b driving course"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With the calculated probability, M-step computes the maximum likelihood mean value of each class.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "586b4c0d-f5ba-405f-92e0-29786513c403": {"__data__": {"id_": "586b4c0d-f5ba-405f-92e0-29786513c403", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "In the initial loop, E-step first computes the probability of expected classes (using pre-determined mean values of two classes) for each pixel in the image. With the calculated probability, M-step computes the maximum likelihood mean value of each class. The estimated mean values are then used in E-step of the second 5 a Polaris Ranger XP and b driving course 7 Driving Course Segmentation: a user-aided obstacle candidates assignment and b univariate Gaussian distribution case"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The estimated mean values are then used in E-step of the second", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87aa933d-8ff6-4bf8-babe-379405698d4f": {"__data__": {"id_": "87aa933d-8ff6-4bf8-babe-379405698d4f", "embedding": null, "metadata": {"page_number": 6, "source": "s10846-019-01130-x.pdf", "window": "With the calculated probability, M-step computes the maximum likelihood mean value of each class. The estimated mean values are then used in E-step of the second 5 a Polaris Ranger XP and b driving course 7 Driving Course Segmentation: a user-aided obstacle candidates assignment and b univariate Gaussian distribution case Algorithm 1 EM for Gaussian mixture model in driving path segmentation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 a Polaris Ranger XP and b driving course", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d42fa29-d9d2-47f4-95b8-a42534dcebc9": {"__data__": {"id_": "3d42fa29-d9d2-47f4-95b8-a42534dcebc9", "embedding": null, "metadata": {"page_number": 7, "source": "s10846-019-01130-x.pdf", "window": "The estimated mean values are then used in E-step of the second 5 a Polaris Ranger XP and b driving course 7 Driving Course Segmentation: a user-aided obstacle candidates assignment and b univariate Gaussian distribution case Algorithm 1 EM for Gaussian mixture model in driving path segmentation. E-step Compute expected classes of each datapoint (x, which is each pixel) for each class: Obstacle (i=1) and Driving path (i=2) P (wiley Ae) = Powel Pw: lis) P(x\u00a2| wi ui (t),07 1) *pi(t) Vjar P(xe| wj.uj (0.021) \u00abpj (0) Computer maximum likelihood value of u given data\u2019s class membership distributions Ly Pwilre.Arexe i+ D = SS ponea M-step"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 Driving Course Segmentation: a user-aided obstacle candidates assignment and b univariate Gaussian distribution case", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e399a74c-6cb4-4eb9-9014-f338f5759929": {"__data__": {"id_": "e399a74c-6cb4-4eb9-9014-f338f5759929", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "5 a Polaris Ranger XP and b driving course 7 Driving Course Segmentation: a user-aided obstacle candidates assignment and b univariate Gaussian distribution case Algorithm 1 EM for Gaussian mixture model in driving path segmentation. E-step Compute expected classes of each datapoint (x, which is each pixel) for each class: Obstacle (i=1) and Driving path (i=2) P (wiley Ae) = Powel Pw: lis) P(x\u00a2| wi ui (t),07 1) *pi(t) Vjar P(xe| wj.uj (0.021) \u00abpj (0) Computer maximum likelihood value of u given data\u2019s class membership distributions Ly Pwilre.Arexe i+ D = SS ponea M-step Each dimension correspond to the number of selected samples in initial classification."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 1 EM for Gaussian mixture model in driving path segmentation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a28cecb-44fb-4d05-b4b7-44a6fb870dc4": {"__data__": {"id_": "1a28cecb-44fb-4d05-b4b7-44a6fb870dc4", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "7 Driving Course Segmentation: a user-aided obstacle candidates assignment and b univariate Gaussian distribution case Algorithm 1 EM for Gaussian mixture model in driving path segmentation. E-step Compute expected classes of each datapoint (x, which is each pixel) for each class: Obstacle (i=1) and Driving path (i=2) P (wiley Ae) = Powel Pw: lis) P(x\u00a2| wi ui (t),07 1) *pi(t) Vjar P(xe| wj.uj (0.021) \u00abpj (0) Computer maximum likelihood value of u given data\u2019s class membership distributions Ly Pwilre.Arexe i+ D = SS ponea M-step Each dimension correspond to the number of selected samples in initial classification. Compared to the univariate case, multivari- ate ones present clear segmentation results."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "E-step Compute expected classes of each datapoint (x, which is each pixel) for each class: Obstacle (i=1) and Driving path (i=2) P (wiley Ae) = Powel Pw: lis) P(x\u00a2| wi ui (t),07 1) *pi(t) Vjar P(xe| wj.uj (0.021) \u00abpj (0) Computer maximum likelihood value of u given data\u2019s class membership distributions Ly Pwilre.Arexe i+ D = SS ponea M-step", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f82a5355-9a87-475a-a4dd-528ea34012f4": {"__data__": {"id_": "f82a5355-9a87-475a-a4dd-528ea34012f4", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Algorithm 1 EM for Gaussian mixture model in driving path segmentation. E-step Compute expected classes of each datapoint (x, which is each pixel) for each class: Obstacle (i=1) and Driving path (i=2) P (wiley Ae) = Powel Pw: lis) P(x\u00a2| wi ui (t),07 1) *pi(t) Vjar P(xe| wj.uj (0.021) \u00abpj (0) Computer maximum likelihood value of u given data\u2019s class membership distributions Ly Pwilre.Arexe i+ D = SS ponea M-step Each dimension correspond to the number of selected samples in initial classification. Compared to the univariate case, multivari- ate ones present clear segmentation results. However, for most scenes (of driving course), when the dimension is bigger than 8, the performance did not change much."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each dimension correspond to the number of selected samples in initial classification.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c71b3ecc-089c-4cb8-9a9e-f11f92f883d8": {"__data__": {"id_": "c71b3ecc-089c-4cb8-9a9e-f11f92f883d8", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "E-step Compute expected classes of each datapoint (x, which is each pixel) for each class: Obstacle (i=1) and Driving path (i=2) P (wiley Ae) = Powel Pw: lis) P(x\u00a2| wi ui (t),07 1) *pi(t) Vjar P(xe| wj.uj (0.021) \u00abpj (0) Computer maximum likelihood value of u given data\u2019s class membership distributions Ly Pwilre.Arexe i+ D = SS ponea M-step Each dimension correspond to the number of selected samples in initial classification. Compared to the univariate case, multivari- ate ones present clear segmentation results. However, for most scenes (of driving course), when the dimension is bigger than 8, the performance did not change much. There- fore, samples between 8 and 10 were selected in the initial classification process."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Compared to the univariate case, multivari- ate ones present clear segmentation results.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebbfd42d-8032-4d4b-912d-9f7ad3dccc24": {"__data__": {"id_": "ebbfd42d-8032-4d4b-912d-9f7ad3dccc24", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Each dimension correspond to the number of selected samples in initial classification. Compared to the univariate case, multivari- ate ones present clear segmentation results. However, for most scenes (of driving course), when the dimension is bigger than 8, the performance did not change much. There- fore, samples between 8 and 10 were selected in the initial classification process. The path segmented image above (generated from EM algorithm using the original image which is captured by 2D camera) is combined with the 3D point cloud data (which is collected by the lidar system in DRC-Hubo+\u2019s sensor head) as demonstrated in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, for most scenes (of driving course), when the dimension is bigger than 8, the performance did not change much.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59b1ebb1-e1c4-4a6c-b4a9-041324885ae0": {"__data__": {"id_": "59b1ebb1-e1c4-4a6c-b4a9-041324885ae0", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Compared to the univariate case, multivari- ate ones present clear segmentation results. However, for most scenes (of driving course), when the dimension is bigger than 8, the performance did not change much. There- fore, samples between 8 and 10 were selected in the initial classification process. The path segmented image above (generated from EM algorithm using the original image which is captured by 2D camera) is combined with the 3D point cloud data (which is collected by the lidar system in DRC-Hubo+\u2019s sensor head) as demonstrated in Fig. Algorithm 2 presents more details with the process."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There- fore, samples between 8 and 10 were selected in the initial classification process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2520b4b-3672-49cb-81ef-ead640a42839": {"__data__": {"id_": "f2520b4b-3672-49cb-81ef-ead640a42839", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "However, for most scenes (of driving course), when the dimension is bigger than 8, the performance did not change much. There- fore, samples between 8 and 10 were selected in the initial classification process. The path segmented image above (generated from EM algorithm using the original image which is captured by 2D camera) is combined with the 3D point cloud data (which is collected by the lidar system in DRC-Hubo+\u2019s sensor head) as demonstrated in Fig. Algorithm 2 presents more details with the process. In the algorithm, fx and fy denote focal lengths of the 2D camera and cx and cy reflect the kinematic offsets between the 2D camera and the lidar system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The path segmented image above (generated from EM algorithm using the original image which is captured by 2D camera) is combined with the 3D point cloud data (which is collected by the lidar system in DRC-Hubo+\u2019s sensor head) as demonstrated in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9ea8c7c-ac24-48bf-bb85-37feeae615e2": {"__data__": {"id_": "c9ea8c7c-ac24-48bf-bb85-37feeae615e2", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "There- fore, samples between 8 and 10 were selected in the initial classification process. The path segmented image above (generated from EM algorithm using the original image which is captured by 2D camera) is combined with the 3D point cloud data (which is collected by the lidar system in DRC-Hubo+\u2019s sensor head) as demonstrated in Fig. Algorithm 2 presents more details with the process. In the algorithm, fx and fy denote focal lengths of the 2D camera and cx and cy reflect the kinematic offsets between the 2D camera and the lidar system. the driving path is divided from the scene in point cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 2 presents more details with the process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c36efcc-53ab-4827-a815-5e4f90233a42": {"__data__": {"id_": "1c36efcc-53ab-4827-a815-5e4f90233a42", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "The path segmented image above (generated from EM algorithm using the original image which is captured by 2D camera) is combined with the 3D point cloud data (which is collected by the lidar system in DRC-Hubo+\u2019s sensor head) as demonstrated in Fig. Algorithm 2 presents more details with the process. In the algorithm, fx and fy denote focal lengths of the 2D camera and cx and cy reflect the kinematic offsets between the 2D camera and the lidar system. the driving path is divided from the scene in point cloud data. Initially, the segmentation process is implemented only in the points which are inside the scope of the frontal camera perception."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the algorithm, fx and fy denote focal lengths of the 2D camera and cx and cy reflect the kinematic offsets between the 2D camera and the lidar system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed22652f-af06-4c76-81c4-8e203797960a": {"__data__": {"id_": "ed22652f-af06-4c76-81c4-8e203797960a", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Algorithm 2 presents more details with the process. In the algorithm, fx and fy denote focal lengths of the 2D camera and cx and cy reflect the kinematic offsets between the 2D camera and the lidar system. the driving path is divided from the scene in point cloud data. Initially, the segmentation process is implemented only in the points which are inside the scope of the frontal camera perception. Therefore, using the selected set of the processed points, the kinematic configuration of the driving path (slope and height variance) is calculated through the regression process."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the driving path is divided from the scene in point cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df6d6bf7-c8c7-4291-a22a-c5fd1fbacad5": {"__data__": {"id_": "df6d6bf7-c8c7-4291-a22a-c5fd1fbacad5", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "In the algorithm, fx and fy denote focal lengths of the 2D camera and cx and cy reflect the kinematic offsets between the 2D camera and the lidar system. the driving path is divided from the scene in point cloud data. Initially, the segmentation process is implemented only in the points which are inside the scope of the frontal camera perception. Therefore, using the selected set of the processed points, the kinematic configuration of the driving path (slope and height variance) is calculated through the regression process. The estimated kinematic data expanded the segmentation process for whole area of the point cloud"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Initially, the segmentation process is implemented only in the points which are inside the scope of the frontal camera perception.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc60c2b4-9502-4114-8203-664b8f954cad": {"__data__": {"id_": "dc60c2b4-9502-4114-8203-664b8f954cad", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "the driving path is divided from the scene in point cloud data. Initially, the segmentation process is implemented only in the points which are inside the scope of the frontal camera perception. Therefore, using the selected set of the processed points, the kinematic configuration of the driving path (slope and height variance) is calculated through the regression process. The estimated kinematic data expanded the segmentation process for whole area of the point cloud Algorithm 2 Fusion process between driving path segmented image and point cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, using the selected set of the processed points, the kinematic configuration of the driving path (slope and height variance) is calculated through the regression process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecc0602d-4f13-4157-953e-29638fcdf5c0": {"__data__": {"id_": "ecc0602d-4f13-4157-953e-29638fcdf5c0", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Initially, the segmentation process is implemented only in the points which are inside the scope of the frontal camera perception. Therefore, using the selected set of the processed points, the kinematic configuration of the driving path (slope and height variance) is calculated through the regression process. The estimated kinematic data expanded the segmentation process for whole area of the point cloud Algorithm 2 Fusion process between driving path segmented image and point cloud data. Input: Driving path segmented image from EM algorithm and original 3D point cloud data Output: Point cloud data combined with path segmentation information for all detected points in collected cloud data, do find 3D position (x, y and z) and calculate x\u2019 and y\u2019: x\u2019 = x/zand y\u2019 = y/z calculate u and v (estimated pixel location in the point cloud\u2019s 2D visualized image) and save the values: u= fy xx\u2019 +c, andv= fy * yy\u2019 +cy for all u, v in the point cloud image, do compare the nearest pixel in the path segmented image and determine whether it belongs to obstacle or path go back to the corresponding point of u and v in original 3D point cloud data and save the path information which is found from the nearest pixel comparison step above"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The estimated kinematic data expanded the segmentation process for whole area of the point cloud", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f50b1e0-1079-4a25-97f8-c3ec0f63cd2f": {"__data__": {"id_": "5f50b1e0-1079-4a25-97f8-c3ec0f63cd2f", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Therefore, using the selected set of the processed points, the kinematic configuration of the driving path (slope and height variance) is calculated through the regression process. The estimated kinematic data expanded the segmentation process for whole area of the point cloud Algorithm 2 Fusion process between driving path segmented image and point cloud data. Input: Driving path segmented image from EM algorithm and original 3D point cloud data Output: Point cloud data combined with path segmentation information for all detected points in collected cloud data, do find 3D position (x, y and z) and calculate x\u2019 and y\u2019: x\u2019 = x/zand y\u2019 = y/z calculate u and v (estimated pixel location in the point cloud\u2019s 2D visualized image) and save the values: u= fy xx\u2019 +c, andv= fy * yy\u2019 +cy for all u, v in the point cloud image, do compare the nearest pixel in the path segmented image and determine whether it belongs to obstacle or path go back to the corresponding point of u and v in original 3D point cloud data and save the path information which is found from the nearest pixel comparison step above The Lidar (Hokyuo UTM-30LX-EW) which is used requires tilting rotation to generate the 3D point cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 2 Fusion process between driving path segmented image and point cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cddafd7d-c218-402f-932b-77461f1adfd7": {"__data__": {"id_": "cddafd7d-c218-402f-932b-77461f1adfd7", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "The estimated kinematic data expanded the segmentation process for whole area of the point cloud Algorithm 2 Fusion process between driving path segmented image and point cloud data. Input: Driving path segmented image from EM algorithm and original 3D point cloud data Output: Point cloud data combined with path segmentation information for all detected points in collected cloud data, do find 3D position (x, y and z) and calculate x\u2019 and y\u2019: x\u2019 = x/zand y\u2019 = y/z calculate u and v (estimated pixel location in the point cloud\u2019s 2D visualized image) and save the values: u= fy xx\u2019 +c, andv= fy * yy\u2019 +cy for all u, v in the point cloud image, do compare the nearest pixel in the path segmented image and determine whether it belongs to obstacle or path go back to the corresponding point of u and v in original 3D point cloud data and save the path information which is found from the nearest pixel comparison step above The Lidar (Hokyuo UTM-30LX-EW) which is used requires tilting rotation to generate the 3D point cloud data. This process takes one to several seconds depending the desired resolution."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Input: Driving path segmented image from EM algorithm and original 3D point cloud data Output: Point cloud data combined with path segmentation information for all detected points in collected cloud data, do find 3D position (x, y and z) and calculate x\u2019 and y\u2019: x\u2019 = x/zand y\u2019 = y/z calculate u and v (estimated pixel location in the point cloud\u2019s 2D visualized image) and save the values: u= fy xx\u2019 +c, andv= fy * yy\u2019 +cy for all u, v in the point cloud image, do compare the nearest pixel in the path segmented image and determine whether it belongs to obstacle or path go back to the corresponding point of u and v in original 3D point cloud data and save the path information which is found from the nearest pixel comparison step above", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c637192e-847f-4744-a1f8-50229f9f4c35": {"__data__": {"id_": "c637192e-847f-4744-a1f8-50229f9f4c35", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Algorithm 2 Fusion process between driving path segmented image and point cloud data. Input: Driving path segmented image from EM algorithm and original 3D point cloud data Output: Point cloud data combined with path segmentation information for all detected points in collected cloud data, do find 3D position (x, y and z) and calculate x\u2019 and y\u2019: x\u2019 = x/zand y\u2019 = y/z calculate u and v (estimated pixel location in the point cloud\u2019s 2D visualized image) and save the values: u= fy xx\u2019 +c, andv= fy * yy\u2019 +cy for all u, v in the point cloud image, do compare the nearest pixel in the path segmented image and determine whether it belongs to obstacle or path go back to the corresponding point of u and v in original 3D point cloud data and save the path information which is found from the nearest pixel comparison step above The Lidar (Hokyuo UTM-30LX-EW) which is used requires tilting rotation to generate the 3D point cloud data. This process takes one to several seconds depending the desired resolution. While driving, several second scanning was not the issue when the vehicle stopped its moving (such as stop and wait for steering-wheel turning)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Lidar (Hokyuo UTM-30LX-EW) which is used requires tilting rotation to generate the 3D point cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8603616e-e9bc-419b-b465-83c7880a3bc5": {"__data__": {"id_": "8603616e-e9bc-419b-b465-83c7880a3bc5", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Input: Driving path segmented image from EM algorithm and original 3D point cloud data Output: Point cloud data combined with path segmentation information for all detected points in collected cloud data, do find 3D position (x, y and z) and calculate x\u2019 and y\u2019: x\u2019 = x/zand y\u2019 = y/z calculate u and v (estimated pixel location in the point cloud\u2019s 2D visualized image) and save the values: u= fy xx\u2019 +c, andv= fy * yy\u2019 +cy for all u, v in the point cloud image, do compare the nearest pixel in the path segmented image and determine whether it belongs to obstacle or path go back to the corresponding point of u and v in original 3D point cloud data and save the path information which is found from the nearest pixel comparison step above The Lidar (Hokyuo UTM-30LX-EW) which is used requires tilting rotation to generate the 3D point cloud data. This process takes one to several seconds depending the desired resolution. While driving, several second scanning was not the issue when the vehicle stopped its moving (such as stop and wait for steering-wheel turning). However, when it is moving, such time-taking scanning process can corrupt accuracy of the collected cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This process takes one to several seconds depending the desired resolution.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccf2b847-0472-4f68-bd38-87e37bb553ca": {"__data__": {"id_": "ccf2b847-0472-4f68-bd38-87e37bb553ca", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "The Lidar (Hokyuo UTM-30LX-EW) which is used requires tilting rotation to generate the 3D point cloud data. This process takes one to several seconds depending the desired resolution. While driving, several second scanning was not the issue when the vehicle stopped its moving (such as stop and wait for steering-wheel turning). However, when it is moving, such time-taking scanning process can corrupt accuracy of the collected cloud data. Therefore, the Lidar was fixed at previously-defined tilting angle (10 degree towards terrain in this study) with no rotation when the vehicle is moving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While driving, several second scanning was not the issue when the vehicle stopped its moving (such as stop and wait for steering-wheel turning).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e70ea54-6599-44e2-a8c0-0f6a81ab58a2": {"__data__": {"id_": "7e70ea54-6599-44e2-a8c0-0f6a81ab58a2", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "This process takes one to several seconds depending the desired resolution. While driving, several second scanning was not the issue when the vehicle stopped its moving (such as stop and wait for steering-wheel turning). However, when it is moving, such time-taking scanning process can corrupt accuracy of the collected cloud data. Therefore, the Lidar was fixed at previously-defined tilting angle (10 degree towards terrain in this study) with no rotation when the vehicle is moving. It resulted in acquisition of 2D point cloud data as shown in left figure of Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, when it is moving, such time-taking scanning process can corrupt accuracy of the collected cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f89d8eb-e6a1-41db-98d6-b4b791df5ae3": {"__data__": {"id_": "9f89d8eb-e6a1-41db-98d6-b4b791df5ae3", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "While driving, several second scanning was not the issue when the vehicle stopped its moving (such as stop and wait for steering-wheel turning). However, when it is moving, such time-taking scanning process can corrupt accuracy of the collected cloud data. Therefore, the Lidar was fixed at previously-defined tilting angle (10 degree towards terrain in this study) with no rotation when the vehicle is moving. It resulted in acquisition of 2D point cloud data as shown in left figure of Fig. The fusion and segmentation process which were used for 3D point cloud data above are applied to the 2D data in the same manner."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the Lidar was fixed at previously-defined tilting angle (10 degree towards terrain in this study) with no rotation when the vehicle is moving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41564be1-8004-4734-b1f0-0aa675d10a0b": {"__data__": {"id_": "41564be1-8004-4734-b1f0-0aa675d10a0b", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "However, when it is moving, such time-taking scanning process can corrupt accuracy of the collected cloud data. Therefore, the Lidar was fixed at previously-defined tilting angle (10 degree towards terrain in this study) with no rotation when the vehicle is moving. It resulted in acquisition of 2D point cloud data as shown in left figure of Fig. The fusion and segmentation process which were used for 3D point cloud data above are applied to the 2D data in the same manner. 10 demonstrates the divided non-obstacles (green) and obstacles (red) in the scene."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It resulted in acquisition of 2D point cloud data as shown in left figure of Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e23425a8-660e-4aaa-bacc-03b1dafd4ea0": {"__data__": {"id_": "e23425a8-660e-4aaa-bacc-03b1dafd4ea0", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Therefore, the Lidar was fixed at previously-defined tilting angle (10 degree towards terrain in this study) with no rotation when the vehicle is moving. It resulted in acquisition of 2D point cloud data as shown in left figure of Fig. The fusion and segmentation process which were used for 3D point cloud data above are applied to the 2D data in the same manner. 10 demonstrates the divided non-obstacles (green) and obstacles (red) in the scene. Right figure shows the only obstacles of the 2D point cloud data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The fusion and segmentation process which were used for 3D point cloud data above are applied to the 2D data in the same manner.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9186b92-4272-4bc5-bf77-1983b4d56c11": {"__data__": {"id_": "f9186b92-4272-4bc5-bf77-1983b4d56c11", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "It resulted in acquisition of 2D point cloud data as shown in left figure of Fig. The fusion and segmentation process which were used for 3D point cloud data above are applied to the 2D data in the same manner. 10 demonstrates the divided non-obstacles (green) and obstacles (red) in the scene. Right figure shows the only obstacles of the 2D point cloud data. The net result is that driving-path segmentation process is successfully implemented both in 2D and 3D point cloud data for safe driving of the vehicle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 demonstrates the divided non-obstacles (green) and obstacles (red) in the scene.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97d5c761-60f5-42a4-b1d6-dff94b101419": {"__data__": {"id_": "97d5c761-60f5-42a4-b1d6-dff94b101419", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "The fusion and segmentation process which were used for 3D point cloud data above are applied to the 2D data in the same manner. 10 demonstrates the divided non-obstacles (green) and obstacles (red) in the scene. Right figure shows the only obstacles of the 2D point cloud data. The net result is that driving-path segmentation process is successfully implemented both in 2D and 3D point cloud data for safe driving of the vehicle. The segmented 2D point cloud data is displayed in the user-control interface (of driving information module) with the captured images from each camera sensor (Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Right figure shows the only obstacles of the 2D point cloud data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bc32eae-98b8-4f8b-9fe4-f24ab577038a": {"__data__": {"id_": "3bc32eae-98b8-4f8b-9fe4-f24ab577038a", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "10 demonstrates the divided non-obstacles (green) and obstacles (red) in the scene. Right figure shows the only obstacles of the 2D point cloud data. The net result is that driving-path segmentation process is successfully implemented both in 2D and 3D point cloud data for safe driving of the vehicle. The segmented 2D point cloud data is displayed in the user-control interface (of driving information module) with the captured images from each camera sensor (Fig. Obstacles of the driving course is indicated as red points in the cloud data display."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The net result is that driving-path segmentation process is successfully implemented both in 2D and 3D point cloud data for safe driving of the vehicle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f7e7977-af31-4a9d-826b-bedb8172a0a2": {"__data__": {"id_": "6f7e7977-af31-4a9d-826b-bedb8172a0a2", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "Right figure shows the only obstacles of the 2D point cloud data. The net result is that driving-path segmentation process is successfully implemented both in 2D and 3D point cloud data for safe driving of the vehicle. The segmented 2D point cloud data is displayed in the user-control interface (of driving information module) with the captured images from each camera sensor (Fig. Obstacles of the driving course is indicated as red points in the cloud data display. Estimated path of the vehicle is also demonstrated in the display as a series of white box models."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The segmented 2D point cloud data is displayed in the user-control interface (of driving information module) with the captured images from each camera sensor (Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11f821e2-3748-43a5-8da4-e97ee68b11d5": {"__data__": {"id_": "11f821e2-3748-43a5-8da4-e97ee68b11d5", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "The net result is that driving-path segmentation process is successfully implemented both in 2D and 3D point cloud data for safe driving of the vehicle. The segmented 2D point cloud data is displayed in the user-control interface (of driving information module) with the captured images from each camera sensor (Fig. Obstacles of the driving course is indicated as red points in the cloud data display. Estimated path of the vehicle is also demonstrated in the display as a series of white box models. 9 Data Fusion between the processed image and point cloud: a top view and b canonical view, course-segmented cloud data: c top view and d canonical view"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Obstacles of the driving course is indicated as red points in the cloud data display.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39d13d07-7776-49da-a9d1-18b74867d0cb": {"__data__": {"id_": "39d13d07-7776-49da-a9d1-18b74867d0cb", "embedding": null, "metadata": {"page_number": 8, "source": "s10846-019-01130-x.pdf", "window": "The segmented 2D point cloud data is displayed in the user-control interface (of driving information module) with the captured images from each camera sensor (Fig. Obstacles of the driving course is indicated as red points in the cloud data display. Estimated path of the vehicle is also demonstrated in the display as a series of white box models. 9 Data Fusion between the processed image and point cloud: a top view and b canonical view, course-segmented cloud data: c top view and d canonical view The model is a bounding box of Polaris which DRC-Hubo+ drives and is projected on the 2D point cloud data space."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Estimated path of the vehicle is also demonstrated in the display as a series of white box models.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c356724-655c-4d09-acd2-64dfd70de266": {"__data__": {"id_": "5c356724-655c-4d09-acd2-64dfd70de266", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "Obstacles of the driving course is indicated as red points in the cloud data display. Estimated path of the vehicle is also demonstrated in the display as a series of white box models. 9 Data Fusion between the processed image and point cloud: a top view and b canonical view, course-segmented cloud data: c top view and d canonical view The model is a bounding box of Polaris which DRC-Hubo+ drives and is projected on the 2D point cloud data space. The estimated path is calculated based on forward kinematics modeling of the vehicle ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 Data Fusion between the processed image and point cloud: a top view and b canonical view, course-segmented cloud data: c top view and d canonical view", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9ccfe4c-1a4a-41d5-8903-6ae3c5a8cf92": {"__data__": {"id_": "a9ccfe4c-1a4a-41d5-8903-6ae3c5a8cf92", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "Estimated path of the vehicle is also demonstrated in the display as a series of white box models. 9 Data Fusion between the processed image and point cloud: a top view and b canonical view, course-segmented cloud data: c top view and d canonical view The model is a bounding box of Polaris which DRC-Hubo+ drives and is projected on the 2D point cloud data space. The estimated path is calculated based on forward kinematics modeling of the vehicle . For the kinematic analysis, 1) configuration information of Polaris, 2) speed of the vehicle, 3) total time duration of the estimated path and 4) steering wheel angle at the moment were used."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The model is a bounding box of Polaris which DRC-Hubo+ drives and is projected on the 2D point cloud data space.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cdf9e6bc-7d22-4c0e-aa47-5aba8b03c222": {"__data__": {"id_": "cdf9e6bc-7d22-4c0e-aa47-5aba8b03c222", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "9 Data Fusion between the processed image and point cloud: a top view and b canonical view, course-segmented cloud data: c top view and d canonical view The model is a bounding box of Polaris which DRC-Hubo+ drives and is projected on the 2D point cloud data space. The estimated path is calculated based on forward kinematics modeling of the vehicle . For the kinematic analysis, 1) configuration information of Polaris, 2) speed of the vehicle, 3) total time duration of the estimated path and 4) steering wheel angle at the moment were used. When the estimated path (bounding box series) become collapsed with obstacles in the scene, the obstacle points which are inside the path are counted and indicated as different color in the display."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The estimated path is calculated based on forward kinematics modeling of the vehicle .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aaca0783-7913-4271-b9dc-601bf8eaeb36": {"__data__": {"id_": "aaca0783-7913-4271-b9dc-601bf8eaeb36", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "The model is a bounding box of Polaris which DRC-Hubo+ drives and is projected on the 2D point cloud data space. The estimated path is calculated based on forward kinematics modeling of the vehicle . For the kinematic analysis, 1) configuration information of Polaris, 2) speed of the vehicle, 3) total time duration of the estimated path and 4) steering wheel angle at the moment were used. When the estimated path (bounding box series) become collapsed with obstacles in the scene, the obstacle points which are inside the path are counted and indicated as different color in the display. the optimal driving path of the scene is also computed and presented in the display of the"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the kinematic analysis, 1) configuration information of Polaris, 2) speed of the vehicle, 3) total time duration of the estimated path and 4) steering wheel angle at the moment were used.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08548898-0030-4dfc-a25e-bff91a71ef29": {"__data__": {"id_": "08548898-0030-4dfc-a25e-bff91a71ef29", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "The estimated path is calculated based on forward kinematics modeling of the vehicle . For the kinematic analysis, 1) configuration information of Polaris, 2) speed of the vehicle, 3) total time duration of the estimated path and 4) steering wheel angle at the moment were used. When the estimated path (bounding box series) become collapsed with obstacles in the scene, the obstacle points which are inside the path are counted and indicated as different color in the display. the optimal driving path of the scene is also computed and presented in the display of the The optimal path is calculated by finding the steering wheel angle which can minimize collisions with obstacles in the scene while vehicle is orienting to goal spot of the driving path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When the estimated path (bounding box series) become collapsed with obstacles in the scene, the obstacle points which are inside the path are counted and indicated as different color in the display.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce6e6fcf-7b5e-4b9f-8c60-51f49eedc9dd": {"__data__": {"id_": "ce6e6fcf-7b5e-4b9f-8c60-51f49eedc9dd", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "For the kinematic analysis, 1) configuration information of Polaris, 2) speed of the vehicle, 3) total time duration of the estimated path and 4) steering wheel angle at the moment were used. When the estimated path (bounding box series) become collapsed with obstacles in the scene, the obstacle points which are inside the path are counted and indicated as different color in the display. the optimal driving path of the scene is also computed and presented in the display of the The optimal path is calculated by finding the steering wheel angle which can minimize collisions with obstacles in the scene while vehicle is orienting to goal spot of the driving path. for each possible steering angle (which range is inside the mechanical constraint of the wheel), the corresponding candidate driving path is estimated first."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the optimal driving path of the scene is also computed and presented in the display of the", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "307b1555-9c77-44e5-bdae-24b4a2087bf0": {"__data__": {"id_": "307b1555-9c77-44e5-bdae-24b4a2087bf0", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "When the estimated path (bounding box series) become collapsed with obstacles in the scene, the obstacle points which are inside the path are counted and indicated as different color in the display. the optimal driving path of the scene is also computed and presented in the display of the The optimal path is calculated by finding the steering wheel angle which can minimize collisions with obstacles in the scene while vehicle is orienting to goal spot of the driving path. for each possible steering angle (which range is inside the mechanical constraint of the wheel), the corresponding candidate driving path is estimated first. Then, the numerical iteration which finds the angle that minimizes two penalty values are implemented (1) based on the greedy search algorithm ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The optimal path is calculated by finding the steering wheel angle which can minimize collisions with obstacles in the scene while vehicle is orienting to goal spot of the driving path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef1ddcb1-8ea7-445f-9dfe-f42065d7a33d": {"__data__": {"id_": "ef1ddcb1-8ea7-445f-9dfe-f42065d7a33d", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "the optimal driving path of the scene is also computed and presented in the display of the The optimal path is calculated by finding the steering wheel angle which can minimize collisions with obstacles in the scene while vehicle is orienting to goal spot of the driving path. for each possible steering angle (which range is inside the mechanical constraint of the wheel), the corresponding candidate driving path is estimated first. Then, the numerical iteration which finds the angle that minimizes two penalty values are implemented (1) based on the greedy search algorithm . The first penalty value is earned by counting the obstacle point which collapse with the candidate path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for each possible steering angle (which range is inside the mechanical constraint of the wheel), the corresponding candidate driving path is estimated first.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7a604dc-93e0-476e-9b20-7313c591698e": {"__data__": {"id_": "e7a604dc-93e0-476e-9b20-7313c591698e", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "The optimal path is calculated by finding the steering wheel angle which can minimize collisions with obstacles in the scene while vehicle is orienting to goal spot of the driving path. for each possible steering angle (which range is inside the mechanical constraint of the wheel), the corresponding candidate driving path is estimated first. Then, the numerical iteration which finds the angle that minimizes two penalty values are implemented (1) based on the greedy search algorithm . The first penalty value is earned by counting the obstacle point which collapse with the candidate path. The second penalty value is calculated by displacement between the goal-spot of the given driving"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the numerical iteration which finds the angle that minimizes two penalty values are implemented (1) based on the greedy search algorithm .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b9ceb46-4872-4c4c-81c4-c13ce7ab0a61": {"__data__": {"id_": "0b9ceb46-4872-4c4c-81c4-c13ce7ab0a61", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "for each possible steering angle (which range is inside the mechanical constraint of the wheel), the corresponding candidate driving path is estimated first. Then, the numerical iteration which finds the angle that minimizes two penalty values are implemented (1) based on the greedy search algorithm . The first penalty value is earned by counting the obstacle point which collapse with the candidate path. The second penalty value is calculated by displacement between the goal-spot of the given driving course and the end-point of the candidate path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first penalty value is earned by counting the obstacle point which collapse with the candidate path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5317b1d-2c0e-44cd-9e62-450728d2ae7d": {"__data__": {"id_": "c5317b1d-2c0e-44cd-9e62-450728d2ae7d", "embedding": null, "metadata": {"page_number": 9, "source": "s10846-019-01130-x.pdf", "window": "Then, the numerical iteration which finds the angle that minimizes two penalty values are implemented (1) based on the greedy search algorithm . The first penalty value is earned by counting the obstacle point which collapse with the candidate path. The second penalty value is calculated by displacement between the goal-spot of the given driving course and the end-point of the candidate path. The net result is that the candidate path which has the minimum penalty value can be assumed as the path which has lowest level of collision and shortest driving path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second penalty value is calculated by displacement between the goal-spot of the given driving", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b67fb03d-660d-4496-8bb9-ba54c0e8f9da": {"__data__": {"id_": "b67fb03d-660d-4496-8bb9-ba54c0e8f9da", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "The first penalty value is earned by counting the obstacle point which collapse with the candidate path. The second penalty value is calculated by displacement between the goal-spot of the given driving course and the end-point of the candidate path. The net result is that the candidate path which has the minimum penalty value can be assumed as the path which has lowest level of collision and shortest driving path. where \u03b8 is the candidate steering wheel rotation angle, v is the velocity of the driving vehicle and t is the total time duration of the computed path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "course and the end-point of the candidate path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "312330bb-3ce4-4c24-a9f3-145e68e04a71": {"__data__": {"id_": "312330bb-3ce4-4c24-a9f3-145e68e04a71", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "The second penalty value is calculated by displacement between the goal-spot of the given driving course and the end-point of the candidate path. The net result is that the candidate path which has the minimum penalty value can be assumed as the path which has lowest level of collision and shortest driving path. where \u03b8 is the candidate steering wheel rotation angle, v is the velocity of the driving vehicle and t is the total time duration of the computed path. penalty value functions which present collision-level and displacement from goal-spot respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The net result is that the candidate path which has the minimum penalty value can be assumed as the path which has lowest level of collision and shortest driving path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2436adbd-7070-42f0-94d4-1c291b49af41": {"__data__": {"id_": "2436adbd-7070-42f0-94d4-1c291b49af41", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "course and the end-point of the candidate path. The net result is that the candidate path which has the minimum penalty value can be assumed as the path which has lowest level of collision and shortest driving path. where \u03b8 is the candidate steering wheel rotation angle, v is the velocity of the driving vehicle and t is the total time duration of the computed path. penalty value functions which present collision-level and displacement from goal-spot respectively. While v and t are determined with the operator\u2019s choice, \u03b8 is generated from the iteration process above."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where \u03b8 is the candidate steering wheel rotation angle, v is the velocity of the driving vehicle and t is the total time duration of the computed path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5602e206-493d-4a22-b85c-9435ead7d23f": {"__data__": {"id_": "5602e206-493d-4a22-b85c-9435ead7d23f", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "The net result is that the candidate path which has the minimum penalty value can be assumed as the path which has lowest level of collision and shortest driving path. where \u03b8 is the candidate steering wheel rotation angle, v is the velocity of the driving vehicle and t is the total time duration of the computed path. penalty value functions which present collision-level and displacement from goal-spot respectively. While v and t are determined with the operator\u2019s choice, \u03b8 is generated from the iteration process above. In reinforcement learning, different weights on the cost value functions can result in diverse output trajectories."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "penalty value functions which present collision-level and displacement from goal-spot respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9fcb0bf-6866-4101-916f-a25141e9efb6": {"__data__": {"id_": "f9fcb0bf-6866-4101-916f-a25141e9efb6", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "where \u03b8 is the candidate steering wheel rotation angle, v is the velocity of the driving vehicle and t is the total time duration of the computed path. penalty value functions which present collision-level and displacement from goal-spot respectively. While v and t are determined with the operator\u2019s choice, \u03b8 is generated from the iteration process above. In reinforcement learning, different weights on the cost value functions can result in diverse output trajectories. Therefore, the weighting factor value (\u03b1 and \u03b2) on each penalty function should be chosen considering the relative importance of the corresponding penalty."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While v and t are determined with the operator\u2019s choice, \u03b8 is generated from the iteration process above.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ce6a96a-653c-4c7a-bb28-f75bdb679768": {"__data__": {"id_": "3ce6a96a-653c-4c7a-bb28-f75bdb679768", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "penalty value functions which present collision-level and displacement from goal-spot respectively. While v and t are determined with the operator\u2019s choice, \u03b8 is generated from the iteration process above. In reinforcement learning, different weights on the cost value functions can result in diverse output trajectories. Therefore, the weighting factor value (\u03b1 and \u03b2) on each penalty function should be chosen considering the relative importance of the corresponding penalty. In this study, 0.5 was assigned for both factors and this made the optimal path estimation process above"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In reinforcement learning, different weights on the cost value functions can result in diverse output trajectories.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2331142-dead-4f7e-a970-108573a30a6c": {"__data__": {"id_": "e2331142-dead-4f7e-a970-108573a30a6c", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "While v and t are determined with the operator\u2019s choice, \u03b8 is generated from the iteration process above. In reinforcement learning, different weights on the cost value functions can result in diverse output trajectories. Therefore, the weighting factor value (\u03b1 and \u03b2) on each penalty function should be chosen considering the relative importance of the corresponding penalty. In this study, 0.5 was assigned for both factors and this made the optimal path estimation process above has same dependency on collision avoidance and goal convergence."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the weighting factor value (\u03b1 and \u03b2) on each penalty function should be chosen considering the relative importance of the corresponding penalty.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c4a60fe-e288-428c-935e-0595598827b7": {"__data__": {"id_": "6c4a60fe-e288-428c-935e-0595598827b7", "embedding": null, "metadata": {"page_number": 10, "source": "s10846-019-01130-x.pdf", "window": "In reinforcement learning, different weights on the cost value functions can result in diverse output trajectories. Therefore, the weighting factor value (\u03b1 and \u03b2) on each penalty function should be chosen considering the relative importance of the corresponding penalty. In this study, 0.5 was assigned for both factors and this made the optimal path estimation process above has same dependency on collision avoidance and goal convergence. The computed path was used for guideline of human operator\u2019s driving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this study, 0.5 was assigned for both factors and this made the optimal path estimation process above", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb876959-35c3-420d-9c93-0d9acd80e6b4": {"__data__": {"id_": "bb876959-35c3-420d-9c93-0d9acd80e6b4", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "Therefore, the weighting factor value (\u03b1 and \u03b2) on each penalty function should be chosen considering the relative importance of the corresponding penalty. In this study, 0.5 was assigned for both factors and this made the optimal path estimation process above has same dependency on collision avoidance and goal convergence. The computed path was used for guideline of human operator\u2019s driving. During the driving task, the operator just needed to align the estimated path (white box) to the optimal path (green box) in the user-control interface."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "has same dependency on collision avoidance and goal convergence.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e2dbac8-e629-4ce4-b483-8c6e0970ee85": {"__data__": {"id_": "0e2dbac8-e629-4ce4-b483-8c6e0970ee85", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "In this study, 0.5 was assigned for both factors and this made the optimal path estimation process above has same dependency on collision avoidance and goal convergence. The computed path was used for guideline of human operator\u2019s driving. During the driving task, the operator just needed to align the estimated path (white box) to the optimal path (green box) in the user-control interface. For this experimentation, the mock-up of the DRC\u2019s driving course is built inside a warehouse."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The computed path was used for guideline of human operator\u2019s driving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f333039-6eaa-4252-a324-330b0717b3a9": {"__data__": {"id_": "8f333039-6eaa-4252-a324-330b0717b3a9", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "has same dependency on collision avoidance and goal convergence. The computed path was used for guideline of human operator\u2019s driving. During the driving task, the operator just needed to align the estimated path (white box) to the optimal path (green box) in the user-control interface. For this experimentation, the mock-up of the DRC\u2019s driving course is built inside a warehouse. estimated path (white box) become aligned with the optimal path (see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "During the driving task, the operator just needed to align the estimated path (white box) to the optimal path (green box) in the user-control interface.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6682a1c8-3c9e-48b3-8cfb-6bcc61fbcbcf": {"__data__": {"id_": "6682a1c8-3c9e-48b3-8cfb-6bcc61fbcbcf", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "The computed path was used for guideline of human operator\u2019s driving. During the driving task, the operator just needed to align the estimated path (white box) to the optimal path (green box) in the user-control interface. For this experimentation, the mock-up of the DRC\u2019s driving course is built inside a warehouse. estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment and the vehicle passed the barrier with no-collisions."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this experimentation, the mock-up of the DRC\u2019s driving course is built inside a warehouse.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "844f2dc9-fefb-444c-9ee5-cb650ce5315b": {"__data__": {"id_": "844f2dc9-fefb-444c-9ee5-cb650ce5315b", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "During the driving task, the operator just needed to align the estimated path (white box) to the optimal path (green box) in the user-control interface. For this experimentation, the mock-up of the DRC\u2019s driving course is built inside a warehouse. estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment and the vehicle passed the barrier with no-collisions. the optimal driving path (green box) recommended right-turn (see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "estimated path (white box) become aligned with the optimal path (see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c9ed32b-39d2-4a8a-9199-ded4563bc27a": {"__data__": {"id_": "6c9ed32b-39d2-4a8a-9199-ded4563bc27a", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "For this experimentation, the mock-up of the DRC\u2019s driving course is built inside a warehouse. estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment and the vehicle passed the barrier with no-collisions. the optimal driving path (green box) recommended right-turn (see Fig. It is because the goal spot of the given course is located on the right side of the vehicle at the moment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While the vehicle is moving forward, the operator kept the alignment and the vehicle passed the barrier with no-collisions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd3a9104-2731-4f6a-bd0d-50df9970d93f": {"__data__": {"id_": "fd3a9104-2731-4f6a-bd0d-50df9970d93f", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment and the vehicle passed the barrier with no-collisions. the optimal driving path (green box) recommended right-turn (see Fig. It is because the goal spot of the given course is located on the right side of the vehicle at the moment. Again, the operator followed the direction and rotated steering wheel until the estimated path (white box) become aligned with the optimal path (see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the optimal driving path (green box) recommended right-turn (see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d85c0f50-ccf3-4d79-b31a-457a1d25d633": {"__data__": {"id_": "d85c0f50-ccf3-4d79-b31a-457a1d25d633", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "While the vehicle is moving forward, the operator kept the alignment and the vehicle passed the barrier with no-collisions. the optimal driving path (green box) recommended right-turn (see Fig. It is because the goal spot of the given course is located on the right side of the vehicle at the moment. Again, the operator followed the direction and rotated steering wheel until the estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is because the goal spot of the given course is located on the right side of the vehicle at the moment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ac0ec1a-4044-4955-b886-6a31b6a25580": {"__data__": {"id_": "7ac0ec1a-4044-4955-b886-6a31b6a25580", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "the optimal driving path (green box) recommended right-turn (see Fig. It is because the goal spot of the given course is located on the right side of the vehicle at the moment. Again, the operator followed the direction and rotated steering wheel until the estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment. It made the vehicle became oriented to the goal-line of the given course."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Again, the operator followed the direction and rotated steering wheel until the estimated path (white box) become aligned with the optimal path (see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "66a65e5c-bdc6-4202-9d42-6429e98793ba": {"__data__": {"id_": "66a65e5c-bdc6-4202-9d42-6429e98793ba", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "It is because the goal spot of the given course is located on the right side of the vehicle at the moment. Again, the operator followed the direction and rotated steering wheel until the estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment. It made the vehicle became oriented to the goal-line of the given course. 13a shows the beginning of the driving course."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While the vehicle is moving forward, the operator kept the alignment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f1889d1-3c32-4a4a-90df-fd457d0d4822": {"__data__": {"id_": "4f1889d1-3c32-4a4a-90df-fd457d0d4822", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "Again, the operator followed the direction and rotated steering wheel until the estimated path (white box) become aligned with the optimal path (see Fig. While the vehicle is moving forward, the operator kept the alignment. It made the vehicle became oriented to the goal-line of the given course. 13a shows the beginning of the driving course. Since the barrier is not in the vehicle\u2019s estimated path, the optimal driving path (green box) just recommended the direction which is going toward the goal spot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It made the vehicle became oriented to the goal-line of the given course.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4b398e5-aaa8-4715-b7f8-1463c731c663": {"__data__": {"id_": "c4b398e5-aaa8-4715-b7f8-1463c731c663", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "While the vehicle is moving forward, the operator kept the alignment. It made the vehicle became oriented to the goal-line of the given course. 13a shows the beginning of the driving course. Since the barrier is not in the vehicle\u2019s estimated path, the optimal driving path (green box) just recommended the direction which is going toward the goal spot. The driving assistant system makes similar advice (path to the direction of the goal) even when the vehicle is in the mid-way if there are no obstacles in its estimated path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13a shows the beginning of the driving course.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d533572-3ea3-4c35-989b-dcd05f5acd83": {"__data__": {"id_": "7d533572-3ea3-4c35-989b-dcd05f5acd83", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "It made the vehicle became oriented to the goal-line of the given course. 13a shows the beginning of the driving course. Since the barrier is not in the vehicle\u2019s estimated path, the optimal driving path (green box) just recommended the direction which is going toward the goal spot. The driving assistant system makes similar advice (path to the direction of the goal) even when the vehicle is in the mid-way if there are no obstacles in its estimated path. the optimal driving path (green box) recommended left turn (see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the barrier is not in the vehicle\u2019s estimated path, the optimal driving path (green box) just recommended the direction which is going toward the goal spot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa679080-6ef8-4f00-abf5-f8dcb6fdf16c": {"__data__": {"id_": "fa679080-6ef8-4f00-abf5-f8dcb6fdf16c", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "13a shows the beginning of the driving course. Since the barrier is not in the vehicle\u2019s estimated path, the optimal driving path (green box) just recommended the direction which is going toward the goal spot. The driving assistant system makes similar advice (path to the direction of the goal) even when the vehicle is in the mid-way if there are no obstacles in its estimated path. the optimal driving path (green box) recommended left turn (see Fig. The operator followed the direction and rotated steering wheel until the"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The driving assistant system makes similar advice (path to the direction of the goal) even when the vehicle is in the mid-way if there are no obstacles in its estimated path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06c3f6cc-09af-4b37-a9b9-11962aaa5e6f": {"__data__": {"id_": "06c3f6cc-09af-4b37-a9b9-11962aaa5e6f", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "Since the barrier is not in the vehicle\u2019s estimated path, the optimal driving path (green box) just recommended the direction which is going toward the goal spot. The driving assistant system makes similar advice (path to the direction of the goal) even when the vehicle is in the mid-way if there are no obstacles in its estimated path. the optimal driving path (green box) recommended left turn (see Fig. The operator followed the direction and rotated steering wheel until the Among candidate paths, the driving assistant system chose the path with the minimum value of the equation which is computed by counting the obstacle pixels which collapse with the candidate path and by measuring the pixel-displacement between the goal-spot and the end-point of the candidate path."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the optimal driving path (green box) recommended left turn (see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95fafc13-3251-4d38-9115-7a6de292e48d": {"__data__": {"id_": "95fafc13-3251-4d38-9115-7a6de292e48d", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "The driving assistant system makes similar advice (path to the direction of the goal) even when the vehicle is in the mid-way if there are no obstacles in its estimated path. the optimal driving path (green box) recommended left turn (see Fig. The operator followed the direction and rotated steering wheel until the Among candidate paths, the driving assistant system chose the path with the minimum value of the equation which is computed by counting the obstacle pixels which collapse with the candidate path and by measuring the pixel-displacement between the goal-spot and the end-point of the candidate path. the newly built perception data processing system provided the estimated vehicle-path and it made the"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The operator followed the direction and rotated steering wheel until the", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd537f4b-8bf7-43e9-838e-f5ba685f5c48": {"__data__": {"id_": "cd537f4b-8bf7-43e9-838e-f5ba685f5c48", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "the optimal driving path (green box) recommended left turn (see Fig. The operator followed the direction and rotated steering wheel until the Among candidate paths, the driving assistant system chose the path with the minimum value of the equation which is computed by counting the obstacle pixels which collapse with the candidate path and by measuring the pixel-displacement between the goal-spot and the end-point of the candidate path. the newly built perception data processing system provided the estimated vehicle-path and it made the operator to predict the vehicle\u2019s position with the control input of the moment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among candidate paths, the driving assistant system chose the path with the minimum value of the equation which is computed by counting the obstacle pixels which collapse with the candidate path and by measuring the pixel-displacement between the goal-spot and the end-point of the candidate path.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80d90c91-bd33-4106-80bd-fa286982cf35": {"__data__": {"id_": "80d90c91-bd33-4106-80bd-fa286982cf35", "embedding": null, "metadata": {"page_number": 11, "source": "s10846-019-01130-x.pdf", "window": "The operator followed the direction and rotated steering wheel until the Among candidate paths, the driving assistant system chose the path with the minimum value of the equation which is computed by counting the obstacle pixels which collapse with the candidate path and by measuring the pixel-displacement between the goal-spot and the end-point of the candidate path. the newly built perception data processing system provided the estimated vehicle-path and it made the operator to predict the vehicle\u2019s position with the control input of the moment. Then, the most optimized path was also computed in terms of collision avoidance and driving distance and displayed in the user-control interface."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the newly built perception data processing system provided the estimated vehicle-path and it made the", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb2cf21f-d1cd-4d9d-9bfc-cfa5e8c8efb4": {"__data__": {"id_": "eb2cf21f-d1cd-4d9d-9bfc-cfa5e8c8efb4", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "Among candidate paths, the driving assistant system chose the path with the minimum value of the equation which is computed by counting the obstacle pixels which collapse with the candidate path and by measuring the pixel-displacement between the goal-spot and the end-point of the candidate path. the newly built perception data processing system provided the estimated vehicle-path and it made the operator to predict the vehicle\u2019s position with the control input of the moment. Then, the most optimized path was also computed in terms of collision avoidance and driving distance and displayed in the user-control interface. These features enabled the operator to control the vehicle with easier and more stable manner."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "operator to predict the vehicle\u2019s position with the control input of the moment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "caee149f-edbb-490f-a1e7-dd00cedf2859": {"__data__": {"id_": "caee149f-edbb-490f-a1e7-dd00cedf2859", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "the newly built perception data processing system provided the estimated vehicle-path and it made the operator to predict the vehicle\u2019s position with the control input of the moment. Then, the most optimized path was also computed in terms of collision avoidance and driving distance and displayed in the user-control interface. These features enabled the operator to control the vehicle with easier and more stable manner. As described in Section 5, the control input of the driving assistant system is the rotated angle of steering wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the most optimized path was also computed in terms of collision avoidance and driving distance and displayed in the user-control interface.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c62c5aa-4251-4316-8572-5bfd8b29bddd": {"__data__": {"id_": "4c62c5aa-4251-4316-8572-5bfd8b29bddd", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "operator to predict the vehicle\u2019s position with the control input of the moment. Then, the most optimized path was also computed in terms of collision avoidance and driving distance and displayed in the user-control interface. These features enabled the operator to control the vehicle with easier and more stable manner. As described in Section 5, the control input of the driving assistant system is the rotated angle of steering wheel. In the previous system (which was used for DRC-Trials), the authors typed the rotation angle manually using a keyboard."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These features enabled the operator to control the vehicle with easier and more stable manner.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2dad4f1c-e74c-49f2-b585-694ef9c2c853": {"__data__": {"id_": "2dad4f1c-e74c-49f2-b585-694ef9c2c853", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "Then, the most optimized path was also computed in terms of collision avoidance and driving distance and displayed in the user-control interface. These features enabled the operator to control the vehicle with easier and more stable manner. As described in Section 5, the control input of the driving assistant system is the rotated angle of steering wheel. In the previous system (which was used for DRC-Trials), the authors typed the rotation angle manually using a keyboard. However, such hard-coding resulted in significant delays in vehicle control."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As described in Section 5, the control input of the driving assistant system is the rotated angle of steering wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76777df7-7484-405b-a7d6-55209ee6166f": {"__data__": {"id_": "76777df7-7484-405b-a7d6-55209ee6166f", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "These features enabled the operator to control the vehicle with easier and more stable manner. As described in Section 5, the control input of the driving assistant system is the rotated angle of steering wheel. In the previous system (which was used for DRC-Trials), the authors typed the rotation angle manually using a keyboard. However, such hard-coding resulted in significant delays in vehicle control. system (which consists of steering wheel and pedal) is adopted (see Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the previous system (which was used for DRC-Trials), the authors typed the rotation angle manually using a keyboard.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc186a09-295c-4b17-b298-1f9c68a1b6d8": {"__data__": {"id_": "bc186a09-295c-4b17-b298-1f9c68a1b6d8", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "As described in Section 5, the control input of the driving assistant system is the rotated angle of steering wheel. In the previous system (which was used for DRC-Trials), the authors typed the rotation angle manually using a keyboard. However, such hard-coding resulted in significant delays in vehicle control. system (which consists of steering wheel and pedal) is adopted (see Fig. DRC-Hubo+ made the corresponding motions (as presented in control system architecture in Section 3)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, such hard-coding resulted in significant delays in vehicle control.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a796103-62a4-4ae5-8517-51440d933cc4": {"__data__": {"id_": "7a796103-62a4-4ae5-8517-51440d933cc4", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "In the previous system (which was used for DRC-Trials), the authors typed the rotation angle manually using a keyboard. However, such hard-coding resulted in significant delays in vehicle control. system (which consists of steering wheel and pedal) is adopted (see Fig. DRC-Hubo+ made the corresponding motions (as presented in control system architecture in Section 3). Rotation of the steering wheel in the racing system resulted in movements of wrist-yaw joint of DRC-Hubo+ (hand position control module in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "system (which consists of steering wheel and pedal) is adopted (see Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fc8cfae-29bc-4aa5-bb37-425035fe94fa": {"__data__": {"id_": "9fc8cfae-29bc-4aa5-bb37-425035fe94fa", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "However, such hard-coding resulted in significant delays in vehicle control. system (which consists of steering wheel and pedal) is adopted (see Fig. DRC-Hubo+ made the corresponding motions (as presented in control system architecture in Section 3). Rotation of the steering wheel in the racing system resulted in movements of wrist-yaw joint of DRC-Hubo+ (hand position control module in Fig. Pushing the gas pedal also made the rotation of ankle pitch joint (foot position control module in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo+ made the corresponding motions (as presented in control system architecture in Section 3).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52ea81f5-127b-41b5-9809-3e43ca114608": {"__data__": {"id_": "52ea81f5-127b-41b5-9809-3e43ca114608", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "system (which consists of steering wheel and pedal) is adopted (see Fig. DRC-Hubo+ made the corresponding motions (as presented in control system architecture in Section 3). Rotation of the steering wheel in the racing system resulted in movements of wrist-yaw joint of DRC-Hubo+ (hand position control module in Fig. Pushing the gas pedal also made the rotation of ankle pitch joint (foot position control module in Fig. Such controlled motion of DRC-Hubo+ made the real control of Polaris."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rotation of the steering wheel in the racing system resulted in movements of wrist-yaw joint of DRC-Hubo+ (hand position control module in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0dde45f6-50f4-4928-b28a-bf6b5a0620a3": {"__data__": {"id_": "0dde45f6-50f4-4928-b28a-bf6b5a0620a3", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "DRC-Hubo+ made the corresponding motions (as presented in control system architecture in Section 3). Rotation of the steering wheel in the racing system resulted in movements of wrist-yaw joint of DRC-Hubo+ (hand position control module in Fig. Pushing the gas pedal also made the rotation of ankle pitch joint (foot position control module in Fig. Such controlled motion of DRC-Hubo+ made the real control of Polaris. Considering kinematic discrepancy between end-effector of DRC-Hubo+ and controls (steering wheel with 0.35 m diameter and pedal with 0.09 m height & 0.05 m width) of Polaris, driving aid devices were designed and added to the vehicle as shown in Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pushing the gas pedal also made the rotation of ankle pitch joint (foot position control module in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4403aa7-e587-4f09-8bbc-41952560c7a6": {"__data__": {"id_": "e4403aa7-e587-4f09-8bbc-41952560c7a6", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "Rotation of the steering wheel in the racing system resulted in movements of wrist-yaw joint of DRC-Hubo+ (hand position control module in Fig. Pushing the gas pedal also made the rotation of ankle pitch joint (foot position control module in Fig. Such controlled motion of DRC-Hubo+ made the real control of Polaris. Considering kinematic discrepancy between end-effector of DRC-Hubo+ and controls (steering wheel with 0.35 m diameter and pedal with 0.09 m height & 0.05 m width) of Polaris, driving aid devices were designed and added to the vehicle as shown in Fig. 14 Control input system for vehicle driving task"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such controlled motion of DRC-Hubo+ made the real control of Polaris.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4e0b9b3-7588-48fc-ab1a-50b4e0c77254": {"__data__": {"id_": "c4e0b9b3-7588-48fc-ab1a-50b4e0c77254", "embedding": null, "metadata": {"page_number": 12, "source": "s10846-019-01130-x.pdf", "window": "Pushing the gas pedal also made the rotation of ankle pitch joint (foot position control module in Fig. Such controlled motion of DRC-Hubo+ made the real control of Polaris. Considering kinematic discrepancy between end-effector of DRC-Hubo+ and controls (steering wheel with 0.35 m diameter and pedal with 0.09 m height & 0.05 m width) of Polaris, driving aid devices were designed and added to the vehicle as shown in Fig. 14 Control input system for vehicle driving task wheel has a rounded center hub which DRC-Hubo+\u2019s hand can not grab firmly."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Considering kinematic discrepancy between end-effector of DRC-Hubo+ and controls (steering wheel with 0.35 m diameter and pedal with 0.09 m height & 0.05 m width) of Polaris, driving aid devices were designed and added to the vehicle as shown in Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5f5ee5c-1c8e-40e4-8b98-3359ca415f06": {"__data__": {"id_": "b5f5ee5c-1c8e-40e4-8b98-3359ca415f06", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "Such controlled motion of DRC-Hubo+ made the real control of Polaris. Considering kinematic discrepancy between end-effector of DRC-Hubo+ and controls (steering wheel with 0.35 m diameter and pedal with 0.09 m height & 0.05 m width) of Polaris, driving aid devices were designed and added to the vehicle as shown in Fig. 14 Control input system for vehicle driving task wheel has a rounded center hub which DRC-Hubo+\u2019s hand can not grab firmly. Since the vehicle\u2019s throttle foot pedal has curved design, DRC-Hubo+\u2019s foot also can not provide stable contact to the control."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 Control input system for vehicle driving task", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94bdb416-b2b7-47e2-887c-fbf36feece4d": {"__data__": {"id_": "94bdb416-b2b7-47e2-887c-fbf36feece4d", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "Considering kinematic discrepancy between end-effector of DRC-Hubo+ and controls (steering wheel with 0.35 m diameter and pedal with 0.09 m height & 0.05 m width) of Polaris, driving aid devices were designed and added to the vehicle as shown in Fig. 14 Control input system for vehicle driving task wheel has a rounded center hub which DRC-Hubo+\u2019s hand can not grab firmly. Since the vehicle\u2019s throttle foot pedal has curved design, DRC-Hubo+\u2019s foot also can not provide stable contact to the control. To solve the issues, the steering aid which has straight planes is placed on top of the vehicle\u2019s steering wheel and the pedal aid which has an pushing assistant component is placed on top of the vehicle ground."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "wheel has a rounded center hub which DRC-Hubo+\u2019s hand can not grab firmly.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e2f0f90-fad6-4c44-b776-41f9a57f514b": {"__data__": {"id_": "8e2f0f90-fad6-4c44-b776-41f9a57f514b", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "14 Control input system for vehicle driving task wheel has a rounded center hub which DRC-Hubo+\u2019s hand can not grab firmly. Since the vehicle\u2019s throttle foot pedal has curved design, DRC-Hubo+\u2019s foot also can not provide stable contact to the control. To solve the issues, the steering aid which has straight planes is placed on top of the vehicle\u2019s steering wheel and the pedal aid which has an pushing assistant component is placed on top of the vehicle ground. As DRC-Hubo+ pushes the \u201cpedal-assistant\u201d component of the pedal pushing aid, the \u201cpedal-pusher\u201d component is triggered and pushed the gas pedal of Polaris."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the vehicle\u2019s throttle foot pedal has curved design, DRC-Hubo+\u2019s foot also can not provide stable contact to the control.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f25fcd3d-217b-4431-bce6-45723e3d5d1f": {"__data__": {"id_": "f25fcd3d-217b-4431-bce6-45723e3d5d1f", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "wheel has a rounded center hub which DRC-Hubo+\u2019s hand can not grab firmly. Since the vehicle\u2019s throttle foot pedal has curved design, DRC-Hubo+\u2019s foot also can not provide stable contact to the control. To solve the issues, the steering aid which has straight planes is placed on top of the vehicle\u2019s steering wheel and the pedal aid which has an pushing assistant component is placed on top of the vehicle ground. As DRC-Hubo+ pushes the \u201cpedal-assistant\u201d component of the pedal pushing aid, the \u201cpedal-pusher\u201d component is triggered and pushed the gas pedal of Polaris. It was designed to facilitate egress motion of DRC-Hubo+ after driving task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To solve the issues, the steering aid which has straight planes is placed on top of the vehicle\u2019s steering wheel and the pedal aid which has an pushing assistant component is placed on top of the vehicle ground.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43311f4a-eea7-4b94-a80a-db9a50bc9e3d": {"__data__": {"id_": "43311f4a-eea7-4b94-a80a-db9a50bc9e3d", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "Since the vehicle\u2019s throttle foot pedal has curved design, DRC-Hubo+\u2019s foot also can not provide stable contact to the control. To solve the issues, the steering aid which has straight planes is placed on top of the vehicle\u2019s steering wheel and the pedal aid which has an pushing assistant component is placed on top of the vehicle ground. As DRC-Hubo+ pushes the \u201cpedal-assistant\u201d component of the pedal pushing aid, the \u201cpedal-pusher\u201d component is triggered and pushed the gas pedal of Polaris. It was designed to facilitate egress motion of DRC-Hubo+ after driving task. it controlled the pedal of Polaris successfully with benefits of the driving aid devices."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As DRC-Hubo+ pushes the \u201cpedal-assistant\u201d component of the pedal pushing aid, the \u201cpedal-pusher\u201d component is triggered and pushed the gas pedal of Polaris.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6806f8d-f624-4926-93fe-64852cb6bff3": {"__data__": {"id_": "a6806f8d-f624-4926-93fe-64852cb6bff3", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "To solve the issues, the steering aid which has straight planes is placed on top of the vehicle\u2019s steering wheel and the pedal aid which has an pushing assistant component is placed on top of the vehicle ground. As DRC-Hubo+ pushes the \u201cpedal-assistant\u201d component of the pedal pushing aid, the \u201cpedal-pusher\u201d component is triggered and pushed the gas pedal of Polaris. It was designed to facilitate egress motion of DRC-Hubo+ after driving task. it controlled the pedal of Polaris successfully with benefits of the driving aid devices. Before experimenting with DRC-Hubo+ and the developed driving system, some useful data were collected from human drivers and pure-tele-operated driving system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It was designed to facilitate egress motion of DRC-Hubo+ after driving task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bf38434-a155-43f7-a3ce-aefa50662019": {"__data__": {"id_": "3bf38434-a155-43f7-a3ce-aefa50662019", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "As DRC-Hubo+ pushes the \u201cpedal-assistant\u201d component of the pedal pushing aid, the \u201cpedal-pusher\u201d component is triggered and pushed the gas pedal of Polaris. It was designed to facilitate egress motion of DRC-Hubo+ after driving task. it controlled the pedal of Polaris successfully with benefits of the driving aid devices. Before experimenting with DRC-Hubo+ and the developed driving system, some useful data were collected from human drivers and pure-tele-operated driving system. the authors observed driving pattern of human drivers with different levels."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it controlled the pedal of Polaris successfully with benefits of the driving aid devices.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec77809c-d569-4960-b589-a95ccfffc6d7": {"__data__": {"id_": "ec77809c-d569-4960-b589-a95ccfffc6d7", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "It was designed to facilitate egress motion of DRC-Hubo+ after driving task. it controlled the pedal of Polaris successfully with benefits of the driving aid devices. Before experimenting with DRC-Hubo+ and the developed driving system, some useful data were collected from human drivers and pure-tele-operated driving system. the authors observed driving pattern of human drivers with different levels. The driving level was determined based on the possessed license types and driving period."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Before experimenting with DRC-Hubo+ and the developed driving system, some useful data were collected from human drivers and pure-tele-operated driving system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2f831ff-4192-4971-9bb0-07ae6ce392d9": {"__data__": {"id_": "e2f831ff-4192-4971-9bb0-07ae6ce392d9", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "it controlled the pedal of Polaris successfully with benefits of the driving aid devices. Before experimenting with DRC-Hubo+ and the developed driving system, some useful data were collected from human drivers and pure-tele-operated driving system. the authors observed driving pattern of human drivers with different levels. The driving level was determined based on the possessed license types and driving period. The collected measurement demonstrated meaningful differences among drivers."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the authors observed driving pattern of human drivers with different levels.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bcd38b3c-d075-4ab7-9501-832b0fd1dcc5": {"__data__": {"id_": "bcd38b3c-d075-4ab7-9501-832b0fd1dcc5", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "Before experimenting with DRC-Hubo+ and the developed driving system, some useful data were collected from human drivers and pure-tele-operated driving system. the authors observed driving pattern of human drivers with different levels. The driving level was determined based on the possessed license types and driving period. The collected measurement demonstrated meaningful differences among drivers. comparison between the experienced driver (Driver 1 who had served as an unmanned system pilot and trainer for 14 years in U.S. Army) and the beginner level one (Driver 2 who was a new driver with age 24)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The driving level was determined based on the possessed license types and driving period.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93316b76-8d96-4091-b81e-c9d5a7a8c459": {"__data__": {"id_": "93316b76-8d96-4091-b81e-c9d5a7a8c459", "embedding": null, "metadata": {"page_number": 13, "source": "s10846-019-01130-x.pdf", "window": "the authors observed driving pattern of human drivers with different levels. The driving level was determined based on the possessed license types and driving period. The collected measurement demonstrated meaningful differences among drivers. comparison between the experienced driver (Driver 1 who had served as an unmanned system pilot and trainer for 14 years in U.S. Army) and the beginner level one (Driver 2 who was a new driver with age 24). As demonstrated in the figure, the non-experienced drivers tended to stop and turn the vehicle\u2019s heading direction just in front of obstacles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The collected measurement demonstrated meaningful differences among drivers.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b078bdfc-a9d6-41a8-9dcc-909e234fe759": {"__data__": {"id_": "b078bdfc-a9d6-41a8-9dcc-909e234fe759", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "The driving level was determined based on the possessed license types and driving period. The collected measurement demonstrated meaningful differences among drivers. comparison between the experienced driver (Driver 1 who had served as an unmanned system pilot and trainer for 14 years in U.S. Army) and the beginner level one (Driver 2 who was a new driver with age 24). As demonstrated in the figure, the non-experienced drivers tended to stop and turn the vehicle\u2019s heading direction just in front of obstacles. It resulted in significant delay (driving time) and their measured average time record (to finish the course) was about 21-32 second."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "comparison between the experienced driver (Driver 1 who had served as an unmanned system pilot and trainer for 14 years in U.S. Army) and the beginner level one (Driver 2 who was a new driver with age 24).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b64b63b3-00d7-49b4-8995-67ef16daa102": {"__data__": {"id_": "b64b63b3-00d7-49b4-8995-67ef16daa102", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "The collected measurement demonstrated meaningful differences among drivers. comparison between the experienced driver (Driver 1 who had served as an unmanned system pilot and trainer for 14 years in U.S. Army) and the beginner level one (Driver 2 who was a new driver with age 24). As demonstrated in the figure, the non-experienced drivers tended to stop and turn the vehicle\u2019s heading direction just in front of obstacles. It resulted in significant delay (driving time) and their measured average time record (to finish the course) was about 21-32 second. On the other hand, the experienced driver gradually turn the heading direction while the vehicle is approaching obstacles (predictive driving skill)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As demonstrated in the figure, the non-experienced drivers tended to stop and turn the vehicle\u2019s heading direction just in front of obstacles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfa93f03-3be6-4d7f-bdb1-88c684be3109": {"__data__": {"id_": "bfa93f03-3be6-4d7f-bdb1-88c684be3109", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "comparison between the experienced driver (Driver 1 who had served as an unmanned system pilot and trainer for 14 years in U.S. Army) and the beginner level one (Driver 2 who was a new driver with age 24). As demonstrated in the figure, the non-experienced drivers tended to stop and turn the vehicle\u2019s heading direction just in front of obstacles. It resulted in significant delay (driving time) and their measured average time record (to finish the course) was about 21-32 second. On the other hand, the experienced driver gradually turn the heading direction while the vehicle is approaching obstacles (predictive driving skill). It resulted in continuous and smooth driving path and reduced their driving time (measured at 7 - 11 second)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It resulted in significant delay (driving time) and their measured average time record (to finish the course) was about 21-32 second.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c033b89-6c9c-4d16-b2a0-996ba40c57d5": {"__data__": {"id_": "2c033b89-6c9c-4d16-b2a0-996ba40c57d5", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "As demonstrated in the figure, the non-experienced drivers tended to stop and turn the vehicle\u2019s heading direction just in front of obstacles. It resulted in significant delay (driving time) and their measured average time record (to finish the course) was about 21-32 second. On the other hand, the experienced driver gradually turn the heading direction while the vehicle is approaching obstacles (predictive driving skill). It resulted in continuous and smooth driving path and reduced their driving time (measured at 7 - 11 second). For this experimentation, mock-up of DRC driving course is built in outdoor parking lot (130 meter length)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, the experienced driver gradually turn the heading direction while the vehicle is approaching obstacles (predictive driving skill).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "066ce23c-1d25-4912-ae85-80bc5c1a5d75": {"__data__": {"id_": "066ce23c-1d25-4912-ae85-80bc5c1a5d75", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "It resulted in significant delay (driving time) and their measured average time record (to finish the course) was about 21-32 second. On the other hand, the experienced driver gradually turn the heading direction while the vehicle is approaching obstacles (predictive driving skill). It resulted in continuous and smooth driving path and reduced their driving time (measured at 7 - 11 second). For this experimentation, mock-up of DRC driving course is built in outdoor parking lot (130 meter length). system in the driving course of DRC Finals mock-up site (on March 2015, DARPA built the competition mock-up site in Charleston, South Carolina and provided access to the finalists)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It resulted in continuous and smooth driving path and reduced their driving time (measured at 7 - 11 second).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "351faaff-5dbe-4c95-9ef4-40e1e3bc6934": {"__data__": {"id_": "351faaff-5dbe-4c95-9ef4-40e1e3bc6934", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "On the other hand, the experienced driver gradually turn the heading direction while the vehicle is approaching obstacles (predictive driving skill). It resulted in continuous and smooth driving path and reduced their driving time (measured at 7 - 11 second). For this experimentation, mock-up of DRC driving course is built in outdoor parking lot (130 meter length). system in the driving course of DRC Finals mock-up site (on March 2015, DARPA built the competition mock-up site in Charleston, South Carolina and provided access to the finalists). The measured average time record (to finish the course) was about 58 - 62 second with the KAIRO system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this experimentation, mock-up of DRC driving course is built in outdoor parking lot (130 meter length).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fda9f331-5ce3-40b5-a887-2a5ee1edd2bb": {"__data__": {"id_": "fda9f331-5ce3-40b5-a887-2a5ee1edd2bb", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "It resulted in continuous and smooth driving path and reduced their driving time (measured at 7 - 11 second). For this experimentation, mock-up of DRC driving course is built in outdoor parking lot (130 meter length). system in the driving course of DRC Finals mock-up site (on March 2015, DARPA built the competition mock-up site in Charleston, South Carolina and provided access to the finalists). The measured average time record (to finish the course) was about 58 - 62 second with the KAIRO system. The KAIRO system only provided real-time 2D camera image feedback to the operator."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "system in the driving course of DRC Finals mock-up site (on March 2015, DARPA built the competition mock-up site in Charleston, South Carolina and provided access to the finalists).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4aedae2-1b2c-4dea-8e99-bb785e5d66d8": {"__data__": {"id_": "e4aedae2-1b2c-4dea-8e99-bb785e5d66d8", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "For this experimentation, mock-up of DRC driving course is built in outdoor parking lot (130 meter length). system in the driving course of DRC Finals mock-up site (on March 2015, DARPA built the competition mock-up site in Charleston, South Carolina and provided access to the finalists). The measured average time record (to finish the course) was about 58 - 62 second with the KAIRO system. The KAIRO system only provided real-time 2D camera image feedback to the operator. Therefore, the human operator had difficulties with making rapid decision while avoiding collision with obstacles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The measured average time record (to finish the course) was about 58 - 62 second with the KAIRO system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07de06e7-c88a-44a2-9016-5c300179423a": {"__data__": {"id_": "07de06e7-c88a-44a2-9016-5c300179423a", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "system in the driving course of DRC Finals mock-up site (on March 2015, DARPA built the competition mock-up site in Charleston, South Carolina and provided access to the finalists). The measured average time record (to finish the course) was about 58 - 62 second with the KAIRO system. The KAIRO system only provided real-time 2D camera image feedback to the operator. Therefore, the human operator had difficulties with making rapid decision while avoiding collision with obstacles. Furthermore, since the system was solely based on the decision of human operator, achieving more cut in driving time was difficult."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The KAIRO system only provided real-time 2D camera image feedback to the operator.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5f8b6eb-52b9-4ba3-a118-5f1c826113d1": {"__data__": {"id_": "b5f8b6eb-52b9-4ba3-a118-5f1c826113d1", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "The measured average time record (to finish the course) was about 58 - 62 second with the KAIRO system. The KAIRO system only provided real-time 2D camera image feedback to the operator. Therefore, the human operator had difficulties with making rapid decision while avoiding collision with obstacles. Furthermore, since the system was solely based on the decision of human operator, achieving more cut in driving time was difficult. The collected data above (from experiments with human driver and the pure tele-operation system) provided the lesson that the driving assistant system for predictive driving is essential for reduction in driving time."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the human operator had difficulties with making rapid decision while avoiding collision with obstacles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0bcac2e-194d-4385-bc43-a12adffd75eb": {"__data__": {"id_": "c0bcac2e-194d-4385-bc43-a12adffd75eb", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "The KAIRO system only provided real-time 2D camera image feedback to the operator. Therefore, the human operator had difficulties with making rapid decision while avoiding collision with obstacles. Furthermore, since the system was solely based on the decision of human operator, achieving more cut in driving time was difficult. The collected data above (from experiments with human driver and the pure tele-operation system) provided the lesson that the driving assistant system for predictive driving is essential for reduction in driving time. DRC-Hubo+ was initially controlled through only tele-operation control (with no driving assistant functionality) similar to the KAIRO"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, since the system was solely based on the decision of human operator, achieving more cut in driving time was difficult.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "906dc49d-2393-4999-a9fe-e608ca092a3a": {"__data__": {"id_": "906dc49d-2393-4999-a9fe-e608ca092a3a", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "Therefore, the human operator had difficulties with making rapid decision while avoiding collision with obstacles. Furthermore, since the system was solely based on the decision of human operator, achieving more cut in driving time was difficult. The collected data above (from experiments with human driver and the pure tele-operation system) provided the lesson that the driving assistant system for predictive driving is essential for reduction in driving time. DRC-Hubo+ was initially controlled through only tele-operation control (with no driving assistant functionality) similar to the KAIRO DRC-Hubo+ took 58 to 270 seconds to finish the task."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The collected data above (from experiments with human driver and the pure tele-operation system) provided the lesson that the driving assistant system for predictive driving is essential for reduction in driving time.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28f3f80b-2436-43bc-9ae2-cace8f4b9801": {"__data__": {"id_": "28f3f80b-2436-43bc-9ae2-cace8f4b9801", "embedding": null, "metadata": {"page_number": 14, "source": "s10846-019-01130-x.pdf", "window": "Furthermore, since the system was solely based on the decision of human operator, achieving more cut in driving time was difficult. The collected data above (from experiments with human driver and the pure tele-operation system) provided the lesson that the driving assistant system for predictive driving is essential for reduction in driving time. DRC-Hubo+ was initially controlled through only tele-operation control (with no driving assistant functionality) similar to the KAIRO DRC-Hubo+ took 58 to 270 seconds to finish the task. The recored was measured between 29 - 55 second through multiple runs."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo+ was initially controlled through only tele-operation control (with no driving assistant functionality) similar to the KAIRO", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e7d5f38-d82e-4fe6-828b-68ed239ee029": {"__data__": {"id_": "8e7d5f38-d82e-4fe6-828b-68ed239ee029", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "The collected data above (from experiments with human driver and the pure tele-operation system) provided the lesson that the driving assistant system for predictive driving is essential for reduction in driving time. DRC-Hubo+ was initially controlled through only tele-operation control (with no driving assistant functionality) similar to the KAIRO DRC-Hubo+ took 58 to 270 seconds to finish the task. The recored was measured between 29 - 55 second through multiple runs. The interesting fact is that DRC-Hubo+ demonstrated the driving pattern similar to the experienced human driver\u2019s one."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo+ took 58 to 270 seconds to finish the task.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7417ade-eac5-4a1e-8488-cbcc617384ce": {"__data__": {"id_": "c7417ade-eac5-4a1e-8488-cbcc617384ce", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "DRC-Hubo+ was initially controlled through only tele-operation control (with no driving assistant functionality) similar to the KAIRO DRC-Hubo+ took 58 to 270 seconds to finish the task. The recored was measured between 29 - 55 second through multiple runs. The interesting fact is that DRC-Hubo+ demonstrated the driving pattern similar to the experienced human driver\u2019s one. With the benefits of driving assistance, DRC-Hubo+ pro-actively manipulated control-input of the vehicle while it was approaching obstacles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The recored was measured between 29 - 55 second through multiple runs.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b085394e-89e2-48f1-a531-4c9353b8ac86": {"__data__": {"id_": "b085394e-89e2-48f1-a531-4c9353b8ac86", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "DRC-Hubo+ took 58 to 270 seconds to finish the task. The recored was measured between 29 - 55 second through multiple runs. The interesting fact is that DRC-Hubo+ demonstrated the driving pattern similar to the experienced human driver\u2019s one. With the benefits of driving assistance, DRC-Hubo+ pro-actively manipulated control-input of the vehicle while it was approaching obstacles. It resulted in reduction of driving time as well as robust collision avoidance."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The interesting fact is that DRC-Hubo+ demonstrated the driving pattern similar to the experienced human driver\u2019s one.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51bfbdb2-df5d-43af-8088-2434ef93e3da": {"__data__": {"id_": "51bfbdb2-df5d-43af-8088-2434ef93e3da", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "The recored was measured between 29 - 55 second through multiple runs. The interesting fact is that DRC-Hubo+ demonstrated the driving pattern similar to the experienced human driver\u2019s one. With the benefits of driving assistance, DRC-Hubo+ pro-actively manipulated control-input of the vehicle while it was approaching obstacles. It resulted in reduction of driving time as well as robust collision avoidance. When 5 minutes were assigned to the time limit of the driving task completion (based on the rule of DRC-Finals which asks the completion of 8 tasks within 60 minutes including penalty time), the averaged success rate of the task was slightly over than 92 percent from the measured tests."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With the benefits of driving assistance, DRC-Hubo+ pro-actively manipulated control-input of the vehicle while it was approaching obstacles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbc61e67-5953-47a1-ab17-ab353a6a4927": {"__data__": {"id_": "fbc61e67-5953-47a1-ab17-ab353a6a4927", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "The interesting fact is that DRC-Hubo+ demonstrated the driving pattern similar to the experienced human driver\u2019s one. With the benefits of driving assistance, DRC-Hubo+ pro-actively manipulated control-input of the vehicle while it was approaching obstacles. It resulted in reduction of driving time as well as robust collision avoidance. When 5 minutes were assigned to the time limit of the driving task completion (based on the rule of DRC-Finals which asks the completion of 8 tasks within 60 minutes including penalty time), the averaged success rate of the task was slightly over than 92 percent from the measured tests. The time-record measurement of vehicle driving task is carried out by calculating the time difference between the \u201cdriving-start-point\u201d (which Polaris started to move) and the \u201cdriving-end-point\u201d (which rear tires of Polaris passed the"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It resulted in reduction of driving time as well as robust collision avoidance.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "668a5f84-1c36-48c0-aed7-90fa73985937": {"__data__": {"id_": "668a5f84-1c36-48c0-aed7-90fa73985937", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "With the benefits of driving assistance, DRC-Hubo+ pro-actively manipulated control-input of the vehicle while it was approaching obstacles. It resulted in reduction of driving time as well as robust collision avoidance. When 5 minutes were assigned to the time limit of the driving task completion (based on the rule of DRC-Finals which asks the completion of 8 tasks within 60 minutes including penalty time), the averaged success rate of the task was slightly over than 92 percent from the measured tests. The time-record measurement of vehicle driving task is carried out by calculating the time difference between the \u201cdriving-start-point\u201d (which Polaris started to move) and the \u201cdriving-end-point\u201d (which rear tires of Polaris passed the Tasks Success rate [%] Driving 92.1 29 - 122 29 - 55 with driving assistant system Door 94.4 43 - 95 Valve 93.0 55 - 129 Rubble 86.3 30 - 228 Stairs 91.2 218 - 298 Avg."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When 5 minutes were assigned to the time limit of the driving task completion (based on the rule of DRC-Finals which asks the completion of 8 tasks within 60 minutes including penalty time), the averaged success rate of the task was slightly over than 92 percent from the measured tests.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9144241d-aa68-4c37-b73c-e5cb255cdcce": {"__data__": {"id_": "9144241d-aa68-4c37-b73c-e5cb255cdcce", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "It resulted in reduction of driving time as well as robust collision avoidance. When 5 minutes were assigned to the time limit of the driving task completion (based on the rule of DRC-Finals which asks the completion of 8 tasks within 60 minutes including penalty time), the averaged success rate of the task was slightly over than 92 percent from the measured tests. The time-record measurement of vehicle driving task is carried out by calculating the time difference between the \u201cdriving-start-point\u201d (which Polaris started to move) and the \u201cdriving-end-point\u201d (which rear tires of Polaris passed the Tasks Success rate [%] Driving 92.1 29 - 122 29 - 55 with driving assistant system Door 94.4 43 - 95 Valve 93.0 55 - 129 Rubble 86.3 30 - 228 Stairs 91.2 218 - 298 Avg. To keep consistent performance for driving task, the same operator controlled DRC-Hubo+ during the team\u2019s DRC preparation period."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The time-record measurement of vehicle driving task is carried out by calculating the time difference between the \u201cdriving-start-point\u201d (which Polaris started to move) and the \u201cdriving-end-point\u201d (which rear tires of Polaris passed the", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7562786a-7465-4872-8d5e-0d40e9e70269": {"__data__": {"id_": "7562786a-7465-4872-8d5e-0d40e9e70269", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "When 5 minutes were assigned to the time limit of the driving task completion (based on the rule of DRC-Finals which asks the completion of 8 tasks within 60 minutes including penalty time), the averaged success rate of the task was slightly over than 92 percent from the measured tests. The time-record measurement of vehicle driving task is carried out by calculating the time difference between the \u201cdriving-start-point\u201d (which Polaris started to move) and the \u201cdriving-end-point\u201d (which rear tires of Polaris passed the Tasks Success rate [%] Driving 92.1 29 - 122 29 - 55 with driving assistant system Door 94.4 43 - 95 Valve 93.0 55 - 129 Rubble 86.3 30 - 228 Stairs 91.2 218 - 298 Avg. To keep consistent performance for driving task, the same operator controlled DRC-Hubo+ during the team\u2019s DRC preparation period. To minimize mistakes which can be caused by improvised decisions, the operator was trained to mainly follow the advised commands from the driving assistant system (which is described in Section 5.2)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tasks Success rate [%] Driving 92.1 29 - 122 29 - 55 with driving assistant system Door 94.4 43 - 95 Valve 93.0 55 - 129 Rubble 86.3 30 - 228 Stairs 91.2 218 - 298 Avg.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f797806-623c-4ac4-8474-4331425ebb40": {"__data__": {"id_": "1f797806-623c-4ac4-8474-4331425ebb40", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "The time-record measurement of vehicle driving task is carried out by calculating the time difference between the \u201cdriving-start-point\u201d (which Polaris started to move) and the \u201cdriving-end-point\u201d (which rear tires of Polaris passed the Tasks Success rate [%] Driving 92.1 29 - 122 29 - 55 with driving assistant system Door 94.4 43 - 95 Valve 93.0 55 - 129 Rubble 86.3 30 - 228 Stairs 91.2 218 - 298 Avg. To keep consistent performance for driving task, the same operator controlled DRC-Hubo+ during the team\u2019s DRC preparation period. To minimize mistakes which can be caused by improvised decisions, the operator was trained to mainly follow the advised commands from the driving assistant system (which is described in Section 5.2). DRC-Hubo+ and the developed driving system verified its performance in real-world setting."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To keep consistent performance for driving task, the same operator controlled DRC-Hubo+ during the team\u2019s DRC preparation period.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c017de8-576e-4e98-80b1-ffff03c0e2cf": {"__data__": {"id_": "1c017de8-576e-4e98-80b1-ffff03c0e2cf", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "Tasks Success rate [%] Driving 92.1 29 - 122 29 - 55 with driving assistant system Door 94.4 43 - 95 Valve 93.0 55 - 129 Rubble 86.3 30 - 228 Stairs 91.2 218 - 298 Avg. To keep consistent performance for driving task, the same operator controlled DRC-Hubo+ during the team\u2019s DRC preparation period. To minimize mistakes which can be caused by improvised decisions, the operator was trained to mainly follow the advised commands from the driving assistant system (which is described in Section 5.2). DRC-Hubo+ and the developed driving system verified its performance in real-world setting. In DRC-Finals, DRC- Hubo+ successfully drove the driving course which DARPA built and provided (Fig."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To minimize mistakes which can be caused by improvised decisions, the operator was trained to mainly follow the advised commands from the driving assistant system (which is described in Section 5.2).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69a211bb-eaaa-4a08-9e07-5ae3ad3eec2e": {"__data__": {"id_": "69a211bb-eaaa-4a08-9e07-5ae3ad3eec2e", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "To keep consistent performance for driving task, the same operator controlled DRC-Hubo+ during the team\u2019s DRC preparation period. To minimize mistakes which can be caused by improvised decisions, the operator was trained to mainly follow the advised commands from the driving assistant system (which is described in Section 5.2). DRC-Hubo+ and the developed driving system verified its performance in real-world setting. In DRC-Finals, DRC- Hubo+ successfully drove the driving course which DARPA built and provided (Fig. 20 DRC-Hubo+\u2019s driving assistant system in the mock-up course"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo+ and the developed driving system verified its performance in real-world setting.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ed73a6f-597b-413a-bea0-70414352ce49": {"__data__": {"id_": "9ed73a6f-597b-413a-bea0-70414352ce49", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "To minimize mistakes which can be caused by improvised decisions, the operator was trained to mainly follow the advised commands from the driving assistant system (which is described in Section 5.2). DRC-Hubo+ and the developed driving system verified its performance in real-world setting. In DRC-Finals, DRC- Hubo+ successfully drove the driving course which DARPA built and provided (Fig. 20 DRC-Hubo+\u2019s driving assistant system in the mock-up course and was placed in the first place against other world-wide finalists."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In DRC-Finals, DRC- Hubo+ successfully drove the driving course which DARPA built and provided (Fig.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de43bddd-49a2-4336-a75e-5ba57a025b8a": {"__data__": {"id_": "de43bddd-49a2-4336-a75e-5ba57a025b8a", "embedding": null, "metadata": {"page_number": 16, "source": "s10846-019-01130-x.pdf", "window": "DRC-Hubo+ and the developed driving system verified its performance in real-world setting. In DRC-Finals, DRC- Hubo+ successfully drove the driving course which DARPA built and provided (Fig. 20 DRC-Hubo+\u2019s driving assistant system in the mock-up course and was placed in the first place against other world-wide finalists. The presented approach of Team DRC-Hubo@UNLV is also broad-casted in PBS\u2019 NOVA program on February 24th, 2016 ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20 DRC-Hubo+\u2019s driving assistant system in the mock-up course", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e932a48-eeaf-4a0b-921b-36c271973d66": {"__data__": {"id_": "5e932a48-eeaf-4a0b-921b-36c271973d66", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "In DRC-Finals, DRC- Hubo+ successfully drove the driving course which DARPA built and provided (Fig. 20 DRC-Hubo+\u2019s driving assistant system in the mock-up course and was placed in the first place against other world-wide finalists. The presented approach of Team DRC-Hubo@UNLV is also broad-casted in PBS\u2019 NOVA program on February 24th, 2016 . each limb of the new platform\u2019s lower body is designed to be manually adjustable."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and was placed in the first place against other world-wide finalists.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1acb276-a969-4dd6-a81c-4ca4acd26c80": {"__data__": {"id_": "b1acb276-a969-4dd6-a81c-4ca4acd26c80", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "20 DRC-Hubo+\u2019s driving assistant system in the mock-up course and was placed in the first place against other world-wide finalists. The presented approach of Team DRC-Hubo@UNLV is also broad-casted in PBS\u2019 NOVA program on February 24th, 2016 . each limb of the new platform\u2019s lower body is designed to be manually adjustable. As such, the kinematic features of the robot can be optimized depending on the given vehicle\u2019s structure."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The presented approach of Team DRC-Hubo@UNLV is also broad-casted in PBS\u2019 NOVA program on February 24th, 2016 .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "518870c1-315e-40eb-8fc1-a8c2d8ec700a": {"__data__": {"id_": "518870c1-315e-40eb-8fc1-a8c2d8ec700a", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "and was placed in the first place against other world-wide finalists. The presented approach of Team DRC-Hubo@UNLV is also broad-casted in PBS\u2019 NOVA program on February 24th, 2016 . each limb of the new platform\u2019s lower body is designed to be manually adjustable. As such, the kinematic features of the robot can be optimized depending on the given vehicle\u2019s structure. similar efforts with the kinematically recon- figurable humanoid is under study ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "each limb of the new platform\u2019s lower body is designed to be manually adjustable.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "775268c9-4b5f-497a-988c-ce3f29f1f8af": {"__data__": {"id_": "775268c9-4b5f-497a-988c-ce3f29f1f8af", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "The presented approach of Team DRC-Hubo@UNLV is also broad-casted in PBS\u2019 NOVA program on February 24th, 2016 . each limb of the new platform\u2019s lower body is designed to be manually adjustable. As such, the kinematic features of the robot can be optimized depending on the given vehicle\u2019s structure. similar efforts with the kinematically recon- figurable humanoid is under study . DRC-Hubo+ which its hardware design focused mainly on the given vehicle platform can not efficiently manipulate different kinds of vehicles dues to its kinematic limits."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, the kinematic features of the robot can be optimized depending on the given vehicle\u2019s structure.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ee9250b-2dd1-4822-8045-73dc006775ef": {"__data__": {"id_": "2ee9250b-2dd1-4822-8045-73dc006775ef", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "each limb of the new platform\u2019s lower body is designed to be manually adjustable. As such, the kinematic features of the robot can be optimized depending on the given vehicle\u2019s structure. similar efforts with the kinematically recon- figurable humanoid is under study . DRC-Hubo+ which its hardware design focused mainly on the given vehicle platform can not efficiently manipulate different kinds of vehicles dues to its kinematic limits. Tasks S F Best team and record Driving Door Valve Rubble Stairs 19 18 16 8 7 4 5 7 15 16 DRC-Hubo@UNLV: 00:55 NEDO-JSK: 00:50 KAIST: 00:33 DRC-Hubo@UNLV: 00:57 NEDO-JSK: 02:25 02:12 02:08 01:46 03:00 04:39"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "similar efforts with the kinematically recon- figurable humanoid is under study .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbd2fdcf-60a7-40fc-a62c-c862ab892f5a": {"__data__": {"id_": "dbd2fdcf-60a7-40fc-a62c-c862ab892f5a", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "As such, the kinematic features of the robot can be optimized depending on the given vehicle\u2019s structure. similar efforts with the kinematically recon- figurable humanoid is under study . DRC-Hubo+ which its hardware design focused mainly on the given vehicle platform can not efficiently manipulate different kinds of vehicles dues to its kinematic limits. Tasks S F Best team and record Driving Door Valve Rubble Stairs 19 18 16 8 7 4 5 7 15 16 DRC-Hubo@UNLV: 00:55 NEDO-JSK: 00:50 KAIST: 00:33 DRC-Hubo@UNLV: 00:57 NEDO-JSK: 02:25 02:12 02:08 01:46 03:00 04:39 a technical overview of Team DRC-Hubo\u2019s approach to the driving task in DRC-Finals is presented."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo+ which its hardware design focused mainly on the given vehicle platform can not efficiently manipulate different kinds of vehicles dues to its kinematic limits.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78c41961-0bd1-4692-b5f2-5b893918569e": {"__data__": {"id_": "78c41961-0bd1-4692-b5f2-5b893918569e", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "similar efforts with the kinematically recon- figurable humanoid is under study . DRC-Hubo+ which its hardware design focused mainly on the given vehicle platform can not efficiently manipulate different kinds of vehicles dues to its kinematic limits. Tasks S F Best team and record Driving Door Valve Rubble Stairs 19 18 16 8 7 4 5 7 15 16 DRC-Hubo@UNLV: 00:55 NEDO-JSK: 00:50 KAIST: 00:33 DRC-Hubo@UNLV: 00:57 NEDO-JSK: 02:25 02:12 02:08 01:46 03:00 04:39 a technical overview of Team DRC-Hubo\u2019s approach to the driving task in DRC-Finals is presented. First, updates in both hardware and software aspects of DRC-Hubo+ were described compared to the its previous platform."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tasks S F Best team and record Driving Door Valve Rubble Stairs 19 18 16 8 7 4 5 7 15 16 DRC-Hubo@UNLV: 00:55 NEDO-JSK: 00:50 KAIST: 00:33 DRC-Hubo@UNLV: 00:57 NEDO-JSK: 02:25 02:12 02:08 01:46 03:00 04:39", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d07687c7-f6f2-41ff-b629-2b405e711c7c": {"__data__": {"id_": "d07687c7-f6f2-41ff-b629-2b405e711c7c", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "DRC-Hubo+ which its hardware design focused mainly on the given vehicle platform can not efficiently manipulate different kinds of vehicles dues to its kinematic limits. Tasks S F Best team and record Driving Door Valve Rubble Stairs 19 18 16 8 7 4 5 7 15 16 DRC-Hubo@UNLV: 00:55 NEDO-JSK: 00:50 KAIST: 00:33 DRC-Hubo@UNLV: 00:57 NEDO-JSK: 02:25 02:12 02:08 01:46 03:00 04:39 a technical overview of Team DRC-Hubo\u2019s approach to the driving task in DRC-Finals is presented. First, updates in both hardware and software aspects of DRC-Hubo+ were described compared to the its previous platform. Then, the control system architecture which consists of sensor, robot and operator computers was presented."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a technical overview of Team DRC-Hubo\u2019s approach to the driving task in DRC-Finals is presented.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd060354-6956-47b5-8967-1dabd08afa49": {"__data__": {"id_": "cd060354-6956-47b5-8967-1dabd08afa49", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "Tasks S F Best team and record Driving Door Valve Rubble Stairs 19 18 16 8 7 4 5 7 15 16 DRC-Hubo@UNLV: 00:55 NEDO-JSK: 00:50 KAIST: 00:33 DRC-Hubo@UNLV: 00:57 NEDO-JSK: 02:25 02:12 02:08 01:46 03:00 04:39 a technical overview of Team DRC-Hubo\u2019s approach to the driving task in DRC-Finals is presented. First, updates in both hardware and software aspects of DRC-Hubo+ were described compared to the its previous platform. Then, the control system architecture which consists of sensor, robot and operator computers was presented. Especially, the sensor data fusion process and the advanced driving assistant techniques (core functionalities of the driving information module in the system) were emphasized in this paper."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, updates in both hardware and software aspects of DRC-Hubo+ were described compared to the its previous platform.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af4ee6bc-9383-4e3f-92e3-c49c93f900af": {"__data__": {"id_": "af4ee6bc-9383-4e3f-92e3-c49c93f900af", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "a technical overview of Team DRC-Hubo\u2019s approach to the driving task in DRC-Finals is presented. First, updates in both hardware and software aspects of DRC-Hubo+ were described compared to the its previous platform. Then, the control system architecture which consists of sensor, robot and operator computers was presented. Especially, the sensor data fusion process and the advanced driving assistant techniques (core functionalities of the driving information module in the system) were emphasized in this paper. Next, the performance analysis of human drivers and KAIRO system was provided to demonstrate the importance of predictive driving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, the control system architecture which consists of sensor, robot and operator computers was presented.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "702a2475-6fdf-4385-88db-6ddb7670a702": {"__data__": {"id_": "702a2475-6fdf-4385-88db-6ddb7670a702", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "First, updates in both hardware and software aspects of DRC-Hubo+ were described compared to the its previous platform. Then, the control system architecture which consists of sensor, robot and operator computers was presented. Especially, the sensor data fusion process and the advanced driving assistant techniques (core functionalities of the driving information module in the system) were emphasized in this paper. Next, the performance analysis of human drivers and KAIRO system was provided to demonstrate the importance of predictive driving. Lastly, test-evaluation and verification of the developed system were provided through experimentation which DRC-Hubo+ drove the vehicle in the mock-up and the official course"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Especially, the sensor data fusion process and the advanced driving assistant techniques (core functionalities of the driving information module in the system) were emphasized in this paper.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3fbaaab-fe1a-47d2-91b4-2740a1348f21": {"__data__": {"id_": "b3fbaaab-fe1a-47d2-91b4-2740a1348f21", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "Then, the control system architecture which consists of sensor, robot and operator computers was presented. Especially, the sensor data fusion process and the advanced driving assistant techniques (core functionalities of the driving information module in the system) were emphasized in this paper. Next, the performance analysis of human drivers and KAIRO system was provided to demonstrate the importance of predictive driving. Lastly, test-evaluation and verification of the developed system were provided through experimentation which DRC-Hubo+ drove the vehicle in the mock-up and the official course DRC-Hubo+\u2019s mechanical design is mainly focused on the Polaros Ranger XP which DARPA chosen for the competition."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, the performance analysis of human drivers and KAIRO system was provided to demonstrate the importance of predictive driving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54de447a-a2a3-4208-b1b7-2cf33945b289": {"__data__": {"id_": "54de447a-a2a3-4208-b1b7-2cf33945b289", "embedding": null, "metadata": {"page_number": 17, "source": "s10846-019-01130-x.pdf", "window": "Especially, the sensor data fusion process and the advanced driving assistant techniques (core functionalities of the driving information module in the system) were emphasized in this paper. Next, the performance analysis of human drivers and KAIRO system was provided to demonstrate the importance of predictive driving. Lastly, test-evaluation and verification of the developed system were provided through experimentation which DRC-Hubo+ drove the vehicle in the mock-up and the official course DRC-Hubo+\u2019s mechanical design is mainly focused on the Polaros Ranger XP which DARPA chosen for the competition. Due to the limitations, it could not efficiently manipulate other types of vehicles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lastly, test-evaluation and verification of the developed system were provided through experimentation which DRC-Hubo+ drove the vehicle in the mock-up and the official course", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f713de5-35fc-41a7-a218-5bb28385878c": {"__data__": {"id_": "8f713de5-35fc-41a7-a218-5bb28385878c", "embedding": null, "metadata": {"page_number": 18, "source": "s10846-019-01130-x.pdf", "window": "Next, the performance analysis of human drivers and KAIRO system was provided to demonstrate the importance of predictive driving. Lastly, test-evaluation and verification of the developed system were provided through experimentation which DRC-Hubo+ drove the vehicle in the mock-up and the official course DRC-Hubo+\u2019s mechanical design is mainly focused on the Polaros Ranger XP which DARPA chosen for the competition. Due to the limitations, it could not efficiently manipulate other types of vehicles. Currently, the new platform which can optimize its kinematic adaptability depending on the assigned vehicle is under development."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DRC-Hubo+\u2019s mechanical design is mainly focused on the Polaros Ranger XP which DARPA chosen for the competition.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c5b3905-6335-4a89-90aa-e21b184aead2": {"__data__": {"id_": "1c5b3905-6335-4a89-90aa-e21b184aead2", "embedding": null, "metadata": {"page_number": 18, "source": "s10846-019-01130-x.pdf", "window": "Lastly, test-evaluation and verification of the developed system were provided through experimentation which DRC-Hubo+ drove the vehicle in the mock-up and the official course DRC-Hubo+\u2019s mechanical design is mainly focused on the Polaros Ranger XP which DARPA chosen for the competition. Due to the limitations, it could not efficiently manipulate other types of vehicles. Currently, the new platform which can optimize its kinematic adaptability depending on the assigned vehicle is under development."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Due to the limitations, it could not efficiently manipulate other types of vehicles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "591cd493-095b-49e5-9315-3b2f25aad2c4": {"__data__": {"id_": "591cd493-095b-49e5-9315-3b2f25aad2c4", "embedding": null, "metadata": {"page_number": 18, "source": "s10846-019-01130-x.pdf", "window": "DRC-Hubo+\u2019s mechanical design is mainly focused on the Polaros Ranger XP which DARPA chosen for the competition. Due to the limitations, it could not efficiently manipulate other types of vehicles. Currently, the new platform which can optimize its kinematic adaptability depending on the assigned vehicle is under development."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Currently, the new platform which can optimize its kinematic adaptability depending on the assigned vehicle is under development.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e207d619-4c96-46d1-b80a-fd95ef2fbefd": {"__data__": {"id_": "e207d619-4c96-46d1-b80a-fd95ef2fbefd", "embedding": null, "metadata": {"page_number": 1, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "A Study of Developed Hardware and Learning-Based Software and operate the steering wheel with both arms. Digital Object Identifier Date of current version: 12 May 2020"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Study of Developed Hardware and Learning-Based Software", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7fbd565-1de4-4750-ab5b-f1860491695c": {"__data__": {"id_": "b7fbd565-1de4-4750-ab5b-f1860491695c", "embedding": null, "metadata": {"page_number": 1, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "A Study of Developed Hardware and Learning-Based Software and operate the steering wheel with both arms. Digital Object Identifier Date of current version: 12 May 2020 TRANSLATIONS AND CONTENT MINING ARE PERMITTED FOR ACADEMIC RESEARCH ONLY."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and operate the steering wheel with both arms.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34b5e631-8caf-465d-b262-742e0c7920ae": {"__data__": {"id_": "34b5e631-8caf-465d-b262-742e0c7920ae", "embedding": null, "metadata": {"page_number": 1, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "A Study of Developed Hardware and Learning-Based Software and operate the steering wheel with both arms. Digital Object Identifier Date of current version: 12 May 2020 TRANSLATIONS AND CONTENT MINING ARE PERMITTED FOR ACADEMIC RESEARCH ONLY. PERSONAL USE IS ALSO PERMITTED, BUT REPUBLICATION/REDISTRIBUTION REQUIRES IEEE PERMISSION."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Digital Object Identifier Date of current version: 12 May 2020", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "273f63c7-fc72-4643-8389-e885edb86da4": {"__data__": {"id_": "273f63c7-fc72-4643-8389-e885edb86da4", "embedding": null, "metadata": {"page_number": 1, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "and operate the steering wheel with both arms. Digital Object Identifier Date of current version: 12 May 2020 TRANSLATIONS AND CONTENT MINING ARE PERMITTED FOR ACADEMIC RESEARCH ONLY. PERSONAL USE IS ALSO PERMITTED, BUT REPUBLICATION/REDISTRIBUTION REQUIRES IEEE PERMISSION. 84 \u2022 IEEE ROBOTICS & AUTOMATION MAGAZINE \u2022 SEPTEMBER 2020"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "TRANSLATIONS AND CONTENT MINING ARE PERMITTED FOR ACADEMIC RESEARCH ONLY.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99d5f27e-6fb5-431c-909f-f45c6c3fb63f": {"__data__": {"id_": "99d5f27e-6fb5-431c-909f-f45c6c3fb63f", "embedding": null, "metadata": {"page_number": 1, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Digital Object Identifier Date of current version: 12 May 2020 TRANSLATIONS AND CONTENT MINING ARE PERMITTED FOR ACADEMIC RESEARCH ONLY. PERSONAL USE IS ALSO PERMITTED, BUT REPUBLICATION/REDISTRIBUTION REQUIRES IEEE PERMISSION. 84 \u2022 IEEE ROBOTICS & AUTOMATION MAGAZINE \u2022 SEPTEMBER 2020 we show that Musashi succeeded in the pedal and steering wheel operations with recognition."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PERSONAL USE IS ALSO PERMITTED, BUT REPUBLICATION/REDISTRIBUTION REQUIRES IEEE PERMISSION.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bcd8c3e8-bd49-4cfb-8f95-e72610e8353d": {"__data__": {"id_": "bcd8c3e8-bd49-4cfb-8f95-e72610e8353d", "embedding": null, "metadata": {"page_number": 1, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "TRANSLATIONS AND CONTENT MINING ARE PERMITTED FOR ACADEMIC RESEARCH ONLY. PERSONAL USE IS ALSO PERMITTED, BUT REPUBLICATION/REDISTRIBUTION REQUIRES IEEE PERMISSION. 84 \u2022 IEEE ROBOTICS & AUTOMATION MAGAZINE \u2022 SEPTEMBER 2020 we show that Musashi succeeded in the pedal and steering wheel operations with recognition. various research studies in autonomous driving are in progress , ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "84 \u2022 IEEE ROBOTICS & AUTOMATION MAGAZINE \u2022 SEPTEMBER 2020", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f91424e-c1ed-4e93-a54d-72276d74f039": {"__data__": {"id_": "0f91424e-c1ed-4e93-a54d-72276d74f039", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "PERSONAL USE IS ALSO PERMITTED, BUT REPUBLICATION/REDISTRIBUTION REQUIRES IEEE PERMISSION. 84 \u2022 IEEE ROBOTICS & AUTOMATION MAGAZINE \u2022 SEPTEMBER 2020 we show that Musashi succeeded in the pedal and steering wheel operations with recognition. various research studies in autonomous driving are in progress , . Some companies have achieved a certain level of auto nomous driving, e.g., headway vehicle following and autonomous parking."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we show that Musashi succeeded in the pedal and steering wheel operations with recognition.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4784e008-7fbf-47a7-93e2-744ab9efc4b3": {"__data__": {"id_": "4784e008-7fbf-47a7-93e2-744ab9efc4b3", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "84 \u2022 IEEE ROBOTICS & AUTOMATION MAGAZINE \u2022 SEPTEMBER 2020 we show that Musashi succeeded in the pedal and steering wheel operations with recognition. various research studies in autonomous driving are in progress , . Some companies have achieved a certain level of auto nomous driving, e.g., headway vehicle following and autonomous parking. These cars are equipped with power ful cameras, lidar, GPS, and processors for accurate and"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "various research studies in autonomous driving are in progress , .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33de3b59-f854-49b4-a36a-c6668bb06f86": {"__data__": {"id_": "33de3b59-f854-49b4-a36a-c6668bb06f86", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we show that Musashi succeeded in the pedal and steering wheel operations with recognition. various research studies in autonomous driving are in progress , . Some companies have achieved a certain level of auto nomous driving, e.g., headway vehicle following and autonomous parking. These cars are equipped with power ful cameras, lidar, GPS, and processors for accurate and starting with the autonomous driving task at the DARPA Robotics Challenge (DRC) , there are research studies on autonomous driving by humanoid robots , ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some companies have achieved a certain level of auto nomous driving, e.g., headway vehicle following and autonomous parking.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcbc5356-cedd-4f7a-8e19-d5a11a140591": {"__data__": {"id_": "fcbc5356-cedd-4f7a-8e19-d5a11a140591", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "various research studies in autonomous driving are in progress , . Some companies have achieved a certain level of auto nomous driving, e.g., headway vehicle following and autonomous parking. These cars are equipped with power ful cameras, lidar, GPS, and processors for accurate and starting with the autonomous driving task at the DARPA Robotics Challenge (DRC) , there are research studies on autonomous driving by humanoid robots , . Humanoid robots are equipped with various sensors for visual, acoustic, and force information."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These cars are equipped with power ful cameras, lidar, GPS, and processors for accurate and", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75ee28e2-0ada-46c8-89ce-ff9e09e96050": {"__data__": {"id_": "75ee28e2-0ada-46c8-89ce-ff9e09e96050", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Some companies have achieved a certain level of auto nomous driving, e.g., headway vehicle following and autonomous parking. These cars are equipped with power ful cameras, lidar, GPS, and processors for accurate and starting with the autonomous driving task at the DARPA Robotics Challenge (DRC) , there are research studies on autonomous driving by humanoid robots , . Humanoid robots are equipped with various sensors for visual, acoustic, and force information. Using these sensors, the robot can get into the car and drive it."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "starting with the autonomous driving task at the DARPA Robotics Challenge (DRC) , there are research studies on autonomous driving by humanoid robots , .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6318d7ff-63bb-4d47-a7af-7e4b30868208": {"__data__": {"id_": "6318d7ff-63bb-4d47-a7af-7e4b30868208", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "These cars are equipped with power ful cameras, lidar, GPS, and processors for accurate and starting with the autonomous driving task at the DARPA Robotics Challenge (DRC) , there are research studies on autonomous driving by humanoid robots , . Humanoid robots are equipped with various sensors for visual, acoustic, and force information. Using these sensors, the robot can get into the car and drive it. In addition, unlike ordinary autonomous driving, the humanoid robot is expected to do various other tasks, e.g., carrying heavy baggage, assisting the elderly, doing housework, and aiding with disaster response."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Humanoid robots are equipped with various sensors for visual, acoustic, and force information.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfc8aa9e-9954-4082-a987-04dd29019642": {"__data__": {"id_": "cfc8aa9e-9954-4082-a987-04dd29019642", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "starting with the autonomous driving task at the DARPA Robotics Challenge (DRC) , there are research studies on autonomous driving by humanoid robots , . Humanoid robots are equipped with various sensors for visual, acoustic, and force information. Using these sensors, the robot can get into the car and drive it. In addition, unlike ordinary autonomous driving, the humanoid robot is expected to do various other tasks, e.g., carrying heavy baggage, assisting the elderly, doing housework, and aiding with disaster response. However, because the humanoid robot lacks body flexibility and deviates from human body proportions, in the DRC, a steering wheel was operated by one arm and a special jig was required for sitting down on the seat."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using these sensors, the robot can get into the car and drive it.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5599510b-ecf3-4968-91c7-00c96970b4a9": {"__data__": {"id_": "5599510b-ecf3-4968-91c7-00c96970b4a9", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Humanoid robots are equipped with various sensors for visual, acoustic, and force information. Using these sensors, the robot can get into the car and drive it. In addition, unlike ordinary autonomous driving, the humanoid robot is expected to do various other tasks, e.g., carrying heavy baggage, assisting the elderly, doing housework, and aiding with disaster response. However, because the humanoid robot lacks body flexibility and deviates from human body proportions, in the DRC, a steering wheel was operated by one arm and a special jig was required for sitting down on the seat. one that imitates the human actuation system is the musculoskeletal humanoid \u2013 ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, unlike ordinary autonomous driving, the humanoid robot is expected to do various other tasks, e.g., carrying heavy baggage, assisting the elderly, doing housework, and aiding with disaster response.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef80d1fa-1617-4653-abb6-2f0460659a3f": {"__data__": {"id_": "ef80d1fa-1617-4653-abb6-2f0460659a3f", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Using these sensors, the robot can get into the car and drive it. In addition, unlike ordinary autonomous driving, the humanoid robot is expected to do various other tasks, e.g., carrying heavy baggage, assisting the elderly, doing housework, and aiding with disaster response. However, because the humanoid robot lacks body flexibility and deviates from human body proportions, in the DRC, a steering wheel was operated by one arm and a special jig was required for sitting down on the seat. one that imitates the human actuation system is the musculoskeletal humanoid \u2013 . The musculoskeletal humanoid is actuated not by motors arranged at each axis but by pneumatic actua tors or muscle actuators with motors that imitate human muscles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, because the humanoid robot lacks body flexibility and deviates from human body proportions, in the DRC, a steering wheel was operated by one arm and a special jig was required for sitting down on the seat.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92561c60-dede-4b85-b104-0bce545ad5d3": {"__data__": {"id_": "92561c60-dede-4b85-b104-0bce545ad5d3", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In addition, unlike ordinary autonomous driving, the humanoid robot is expected to do various other tasks, e.g., carrying heavy baggage, assisting the elderly, doing housework, and aiding with disaster response. However, because the humanoid robot lacks body flexibility and deviates from human body proportions, in the DRC, a steering wheel was operated by one arm and a special jig was required for sitting down on the seat. one that imitates the human actuation system is the musculoskeletal humanoid \u2013 . The musculoskeletal humanoid is actuated not by motors arranged at each axis but by pneumatic actua tors or muscle actuators with motors that imitate human muscles. The body is flexible compared with the ordinary axisdriven humanoid due to its muscle elasticity and under actuation, and thus it is suitable for motions with complex environmental contact."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "one that imitates the human actuation system is the musculoskeletal humanoid \u2013 .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bfcad58-ea02-44cb-ba90-9f9c8c7bd53b": {"__data__": {"id_": "6bfcad58-ea02-44cb-ba90-9f9c8c7bd53b", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "However, because the humanoid robot lacks body flexibility and deviates from human body proportions, in the DRC, a steering wheel was operated by one arm and a special jig was required for sitting down on the seat. one that imitates the human actuation system is the musculoskeletal humanoid \u2013 . The musculoskeletal humanoid is actuated not by motors arranged at each axis but by pneumatic actua tors or muscle actuators with motors that imitate human muscles. The body is flexible compared with the ordinary axisdriven humanoid due to its muscle elasticity and under actuation, and thus it is suitable for motions with complex environmental contact. Therefore, the musculoskeletal hu manoid can sit down on the car seat easily without a special jig and operate the steering wheel flexibly by both arms."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The musculoskeletal humanoid is actuated not by motors arranged at each axis but by pneumatic actua tors or muscle actuators with motors that imitate human muscles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d26a13ce-9a22-4bb0-ab47-c65a3d7d5416": {"__data__": {"id_": "d26a13ce-9a22-4bb0-ab47-c65a3d7d5416", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "one that imitates the human actuation system is the musculoskeletal humanoid \u2013 . The musculoskeletal humanoid is actuated not by motors arranged at each axis but by pneumatic actua tors or muscle actuators with motors that imitate human muscles. The body is flexible compared with the ordinary axisdriven humanoid due to its muscle elasticity and under actuation, and thus it is suitable for motions with complex environmental contact. Therefore, the musculoskeletal hu manoid can sit down on the car seat easily without a special jig and operate the steering wheel flexibly by both arms. Also, this robot can be used as a more realistic crash test dummy because of its humanlike body structure and actuation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The body is flexible compared with the ordinary axisdriven humanoid due to its muscle elasticity and under actuation, and thus it is suitable for motions with complex environmental contact.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e79b465-7069-4996-bc22-5649ad17a28a": {"__data__": {"id_": "4e79b465-7069-4996-bc22-5649ad17a28a", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The musculoskeletal humanoid is actuated not by motors arranged at each axis but by pneumatic actua tors or muscle actuators with motors that imitate human muscles. The body is flexible compared with the ordinary axisdriven humanoid due to its muscle elasticity and under actuation, and thus it is suitable for motions with complex environmental contact. Therefore, the musculoskeletal hu manoid can sit down on the car seat easily without a special jig and operate the steering wheel flexibly by both arms. Also, this robot can be used as a more realistic crash test dummy because of its humanlike body structure and actuation. we introduce our project of autonomous driving by musculoskeletal humanoids (Figure 1)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the musculoskeletal hu manoid can sit down on the car seat easily without a special jig and operate the steering wheel flexibly by both arms.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c88a50e-629c-4f7d-9e96-4bfe8fec29da": {"__data__": {"id_": "1c88a50e-629c-4f7d-9e96-4bfe8fec29da", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The body is flexible compared with the ordinary axisdriven humanoid due to its muscle elasticity and under actuation, and thus it is suitable for motions with complex environmental contact. Therefore, the musculoskeletal hu manoid can sit down on the car seat easily without a special jig and operate the steering wheel flexibly by both arms. Also, this robot can be used as a more realistic crash test dummy because of its humanlike body structure and actuation. we introduce our project of autonomous driving by musculoskeletal humanoids (Figure 1). In particu lar, we describe the characteristics of the hardware and learn ingbased software to move the flexible body."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, this robot can be used as a more realistic crash test dummy because of its humanlike body structure and actuation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "647080c8-8242-4380-b00a-cc2361f34978": {"__data__": {"id_": "647080c8-8242-4380-b00a-cc2361f34978", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Therefore, the musculoskeletal hu manoid can sit down on the car seat easily without a special jig and operate the steering wheel flexibly by both arms. Also, this robot can be used as a more realistic crash test dummy because of its humanlike body structure and actuation. we introduce our project of autonomous driving by musculoskeletal humanoids (Figure 1). In particu lar, we describe the characteristics of the hardware and learn ingbased software to move the flexible body. We reconsider the developed hardware , \u2013 and software \u2013 in the context of autonomous driving."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we introduce our project of autonomous driving by musculoskeletal humanoids (Figure 1).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c981bd17-97d3-4a20-be6e-33f75f9864d6": {"__data__": {"id_": "c981bd17-97d3-4a20-be6e-33f75f9864d6", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, this robot can be used as a more realistic crash test dummy because of its humanlike body structure and actuation. we introduce our project of autonomous driving by musculoskeletal humanoids (Figure 1). In particu lar, we describe the characteristics of the hardware and learn ingbased software to move the flexible body. We reconsider the developed hardware , \u2013 and software \u2013 in the context of autonomous driving. Also, we develop the respective components of autonomous driving and conduct experiments integrating them using the hardware and soft ware characteristics."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particu lar, we describe the characteristics of the hardware and learn ingbased software to move the flexible body.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e49789b-6176-4c0b-b07b-abba86b7ddb6": {"__data__": {"id_": "4e49789b-6176-4c0b-b07b-abba86b7ddb6", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we introduce our project of autonomous driving by musculoskeletal humanoids (Figure 1). In particu lar, we describe the characteristics of the hardware and learn ingbased software to move the flexible body. We reconsider the developed hardware , \u2013 and software \u2013 in the context of autonomous driving. Also, we develop the respective components of autonomous driving and conduct experiments integrating them using the hardware and soft ware characteristics. The important points for a robot to drive a car made for hu mans are considered to be 1) body proportion, 2) body"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We reconsider the developed hardware , \u2013 and software \u2013 in the context of autonomous driving.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33d17f0a-c77c-45d4-8481-5a5b63aee2f5": {"__data__": {"id_": "33d17f0a-c77c-45d4-8481-5a5b63aee2f5", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In particu lar, we describe the characteristics of the hardware and learn ingbased software to move the flexible body. We reconsider the developed hardware , \u2013 and software \u2013 in the context of autonomous driving. Also, we develop the respective components of autonomous driving and conduct experiments integrating them using the hardware and soft ware characteristics. The important points for a robot to drive a car made for hu mans are considered to be 1) body proportion, 2) body and 3) redundant sensors as well as a learning sys tem using them."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, we develop the respective components of autonomous driving and conduct experiments integrating them using the hardware and soft ware characteristics.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c445e2bf-4d13-4688-bd12-1696cb9b29e4": {"__data__": {"id_": "c445e2bf-4d13-4688-bd12-1696cb9b29e4", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We reconsider the developed hardware , \u2013 and software \u2013 in the context of autonomous driving. Also, we develop the respective components of autonomous driving and conduct experiments integrating them using the hardware and soft ware characteristics. The important points for a robot to drive a car made for hu mans are considered to be 1) body proportion, 2) body and 3) redundant sensors as well as a learning sys tem using them. In this article, we first determine the arrange ment of joints and the length of each bone to fit the proportion and joint structure of humans and attach muscle modules to the bones."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The important points for a robot to drive a car made for hu mans are considered to be 1) body proportion, 2) body", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acf25c4b-63ab-4b67-b4f6-9bb181f3d160": {"__data__": {"id_": "acf25c4b-63ab-4b67-b4f6-9bb181f3d160", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, we develop the respective components of autonomous driving and conduct experiments integrating them using the hardware and soft ware characteristics. The important points for a robot to drive a car made for hu mans are considered to be 1) body proportion, 2) body and 3) redundant sensors as well as a learning sys tem using them. In this article, we first determine the arrange ment of joints and the length of each bone to fit the proportion and joint structure of humans and attach muscle modules to the bones. In addition, a nonlinear elastic unit (NEU) is attached to the end of each muscle, and the robot can fit into the complex environment with the unit and the original flexi bility of muscle wire."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and 3) redundant sensors as well as a learning sys tem using them.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b3bddbf-0343-45c6-ba47-3bbb3a1b4bba": {"__data__": {"id_": "5b3bddbf-0343-45c6-ba47-3bbb3a1b4bba", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The important points for a robot to drive a car made for hu mans are considered to be 1) body proportion, 2) body and 3) redundant sensors as well as a learning sys tem using them. In this article, we first determine the arrange ment of joints and the length of each bone to fit the proportion and joint structure of humans and attach muscle modules to the bones. In addition, a nonlinear elastic unit (NEU) is attached to the end of each muscle, and the robot can fit into the complex environment with the unit and the original flexi bility of muscle wire. Finally, we design a structure including not only the muscle module that can measure muscle length, muscle tension, and muscle temperature but also a flexible hand with contact sensors, movable eyes with highresolution cameras, and a foot with sixaxis force sensors that can mea sure force along the entire surface as redundant sensors."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this article, we first determine the arrange ment of joints and the length of each bone to fit the proportion and joint structure of humans and attach muscle modules to the bones.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3de21842-4b77-42a2-8cff-b131b5791348": {"__data__": {"id_": "3de21842-4b77-42a2-8cff-b131b5791348", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "and 3) redundant sensors as well as a learning sys tem using them. In this article, we first determine the arrange ment of joints and the length of each bone to fit the proportion and joint structure of humans and attach muscle modules to the bones. In addition, a nonlinear elastic unit (NEU) is attached to the end of each muscle, and the robot can fit into the complex environment with the unit and the original flexi bility of muscle wire. Finally, we design a structure including not only the muscle module that can measure muscle length, muscle tension, and muscle temperature but also a flexible hand with contact sensors, movable eyes with highresolution cameras, and a foot with sixaxis force sensors that can mea sure force along the entire surface as redundant sensors. In the section \u201cHardware Details,\u201d we describe in detail the design of muscle modules and NEUs that create body flexibil ity and the joint modules, eyes, hands, and feet that form the redundant sensors and learning system."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, a nonlinear elastic unit (NEU) is attached to the end of each muscle, and the robot can fit into the complex environment with the unit and the original flexi bility of muscle wire.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af33cb68-371d-4084-b51d-69600fcce609": {"__data__": {"id_": "af33cb68-371d-4084-b51d-69600fcce609", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In this article, we first determine the arrange ment of joints and the length of each bone to fit the proportion and joint structure of humans and attach muscle modules to the bones. In addition, a nonlinear elastic unit (NEU) is attached to the end of each muscle, and the robot can fit into the complex environment with the unit and the original flexi bility of muscle wire. Finally, we design a structure including not only the muscle module that can measure muscle length, muscle tension, and muscle temperature but also a flexible hand with contact sensors, movable eyes with highresolution cameras, and a foot with sixaxis force sensors that can mea sure force along the entire surface as redundant sensors. In the section \u201cHardware Details,\u201d we describe in detail the design of muscle modules and NEUs that create body flexibil ity and the joint modules, eyes, hands, and feet that form the redundant sensors and learning system. We demonstrate the hardware overview of the developed musculoskeletal humanoid Musashi in Figures 2 and 3."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we design a structure including not only the muscle module that can measure muscle length, muscle tension, and muscle temperature but also a flexible hand with contact sensors, movable eyes with highresolution cameras, and a foot with sixaxis force sensors that can mea sure force along the entire surface as redundant sensors.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "093b8c7e-edcd-4fe1-be4e-c3194f96af17": {"__data__": {"id_": "093b8c7e-edcd-4fe1-be4e-c3194f96af17", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In addition, a nonlinear elastic unit (NEU) is attached to the end of each muscle, and the robot can fit into the complex environment with the unit and the original flexi bility of muscle wire. Finally, we design a structure including not only the muscle module that can measure muscle length, muscle tension, and muscle temperature but also a flexible hand with contact sensors, movable eyes with highresolution cameras, and a foot with sixaxis force sensors that can mea sure force along the entire surface as redundant sensors. In the section \u201cHardware Details,\u201d we describe in detail the design of muscle modules and NEUs that create body flexibil ity and the joint modules, eyes, hands, and feet that form the redundant sensors and learning system. We demonstrate the hardware overview of the developed musculoskeletal humanoid Musashi in Figures 2 and 3. Currently, Musashi has 74 muscles and 39 joints, excluding"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the section \u201cHardware Details,\u201d we describe in detail the design of muscle modules and NEUs that create body flexibil ity and the joint modules, eyes, hands, and feet that form the redundant sensors and learning system.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc92271a-2555-4e70-9456-6c944e1d302f": {"__data__": {"id_": "bc92271a-2555-4e70-9456-6c944e1d302f", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Finally, we design a structure including not only the muscle module that can measure muscle length, muscle tension, and muscle temperature but also a flexible hand with contact sensors, movable eyes with highresolution cameras, and a foot with sixaxis force sensors that can mea sure force along the entire surface as redundant sensors. In the section \u201cHardware Details,\u201d we describe in detail the design of muscle modules and NEUs that create body flexibil ity and the joint modules, eyes, hands, and feet that form the redundant sensors and learning system. We demonstrate the hardware overview of the developed musculoskeletal humanoid Musashi in Figures 2 and 3. Currently, Musashi has 74 muscles and 39 joints, excluding \u25cf Figure 2(a) shows the basic musculoskeletal structure of Musashi."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We demonstrate the hardware overview of the developed musculoskeletal humanoid Musashi in Figures 2 and 3.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a62366b0-6219-4084-b805-8b7d7fadd131": {"__data__": {"id_": "a62366b0-6219-4084-b805-8b7d7fadd131", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In the section \u201cHardware Details,\u201d we describe in detail the design of muscle modules and NEUs that create body flexibil ity and the joint modules, eyes, hands, and feet that form the redundant sensors and learning system. We demonstrate the hardware overview of the developed musculoskeletal humanoid Musashi in Figures 2 and 3. Currently, Musashi has 74 muscles and 39 joints, excluding \u25cf Figure 2(a) shows the basic musculoskeletal structure of Musashi. The joint module connects the generic bone frames, and muscle modules are attached to the frame."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Currently, Musashi has 74 muscles and 39 joints, excluding", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85f08d77-3da8-4895-b05a-f0e9c416f2e3": {"__data__": {"id_": "85f08d77-3da8-4895-b05a-f0e9c416f2e3", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We demonstrate the hardware overview of the developed musculoskeletal humanoid Musashi in Figures 2 and 3. Currently, Musashi has 74 muscles and 39 joints, excluding \u25cf Figure 2(a) shows the basic musculoskeletal structure of Musashi. The joint module connects the generic bone frames, and muscle modules are attached to the frame. The abrasionresistant synthetic fiber Dyneema comes out as a muscle from the tension measurement unit of the muscle module and is folded back by the muscle relay unit."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cf Figure 2(a) shows the basic musculoskeletal structure of Musashi.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3067a66-7d00-43d9-b68d-5592b5773fed": {"__data__": {"id_": "b3067a66-7d00-43d9-b68d-5592b5773fed", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Currently, Musashi has 74 muscles and 39 joints, excluding \u25cf Figure 2(a) shows the basic musculoskeletal structure of Musashi. The joint module connects the generic bone frames, and muscle modules are attached to the frame. The abrasionresistant synthetic fiber Dyneema comes out as a muscle from the tension measurement unit of the muscle module and is folded back by the muscle relay unit. The NEU is attached to the end of the muscle, and a soft foam and spring cover the muscle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The joint module connects the generic bone frames, and muscle modules are attached to the frame.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d550a27-d889-467a-bf1e-37af15ddcfe6": {"__data__": {"id_": "0d550a27-d889-467a-bf1e-37af15ddcfe6", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "\u25cf Figure 2(a) shows the basic musculoskeletal structure of Musashi. The joint module connects the generic bone frames, and muscle modules are attached to the frame. The abrasionresistant synthetic fiber Dyneema comes out as a muscle from the tension measurement unit of the muscle module and is folded back by the muscle relay unit. The NEU is attached to the end of the muscle, and a soft foam and spring cover the muscle. We describe muscles mainly conducting the current task as agonist muscles and the others as antagonist muscles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The abrasionresistant synthetic fiber Dyneema comes out as a muscle from the tension measurement unit of the muscle module and is folded back by the muscle relay unit.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "495e66c2-db43-4596-84b4-6bc1636dd68e": {"__data__": {"id_": "495e66c2-db43-4596-84b4-6bc1636dd68e", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The joint module connects the generic bone frames, and muscle modules are attached to the frame. The abrasionresistant synthetic fiber Dyneema comes out as a muscle from the tension measurement unit of the muscle module and is folded back by the muscle relay unit. The NEU is attached to the end of the muscle, and a soft foam and spring cover the muscle. We describe muscles mainly conducting the current task as agonist muscles and the others as antagonist muscles. \u25cf Figure 2(b) is the structure of the left arm of Musashi, constructed by the modules of 2(c)\u2013(e) and explained subsequently."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The NEU is attached to the end of the muscle, and a soft foam and spring cover the muscle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03b53952-123c-4e85-8c33-cdc4819d8840": {"__data__": {"id_": "03b53952-123c-4e85-8c33-cdc4819d8840", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The abrasionresistant synthetic fiber Dyneema comes out as a muscle from the tension measurement unit of the muscle module and is folded back by the muscle relay unit. The NEU is attached to the end of the muscle, and a soft foam and spring cover the muscle. We describe muscles mainly conducting the current task as agonist muscles and the others as antagonist muscles. \u25cf Figure 2(b) is the structure of the left arm of Musashi, constructed by the modules of 2(c)\u2013(e) and explained subsequently. The body structure is easily constructed by the modules, muscle attachments connecting bones and muscles, and joint attachments connecting bones and joints."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We describe muscles mainly conducting the current task as agonist muscles and the others as antagonist muscles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "057aea98-61e3-480b-848f-2fb642f9fa03": {"__data__": {"id_": "057aea98-61e3-480b-848f-2fb642f9fa03", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The NEU is attached to the end of the muscle, and a soft foam and spring cover the muscle. We describe muscles mainly conducting the current task as agonist muscles and the others as antagonist muscles. \u25cf Figure 2(b) is the structure of the left arm of Musashi, constructed by the modules of 2(c)\u2013(e) and explained subsequently. The body structure is easily constructed by the modules, muscle attachments connecting bones and muscles, and joint attachments connecting bones and joints. \u25cf Figure 2(c) displays the detailed structure of the joint module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cf Figure 2(b) is the structure of the left arm of Musashi, constructed by the modules of 2(c)\u2013(e) and explained subsequently.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22afdcd1-377a-4f8d-97df-1a70dc9602d2": {"__data__": {"id_": "22afdcd1-377a-4f8d-97df-1a70dc9602d2", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We describe muscles mainly conducting the current task as agonist muscles and the others as antagonist muscles. \u25cf Figure 2(b) is the structure of the left arm of Musashi, constructed by the modules of 2(c)\u2013(e) and explained subsequently. The body structure is easily constructed by the modules, muscle attachments connecting bones and muscles, and joint attachments connecting bones and joints. \u25cf Figure 2(c) displays the detailed structure of the joint module. We can express all the basic human joints by rearranging two center parts and three axis parts."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The body structure is easily constructed by the modules, muscle attachments connecting bones and muscles, and joint attachments connecting bones and joints.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bafc29a-1bb8-455d-8d26-3ba4acf2d04e": {"__data__": {"id_": "3bafc29a-1bb8-455d-8d26-3ba4acf2d04e", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "\u25cf Figure 2(b) is the structure of the left arm of Musashi, constructed by the modules of 2(c)\u2013(e) and explained subsequently. The body structure is easily constructed by the modules, muscle attachments connecting bones and muscles, and joint attachments connecting bones and joints. \u25cf Figure 2(c) displays the detailed structure of the joint module. We can express all the basic human joints by rearranging two center parts and three axis parts. inertial measurement units (IMUs), and a circuit integrating sensor data are packaged in the joint module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cf Figure 2(c) displays the detailed structure of the joint module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a9b44a1-a61f-4652-a24a-2a0557edf47d": {"__data__": {"id_": "0a9b44a1-a61f-4652-a24a-2a0557edf47d", "embedding": null, "metadata": {"page_number": 2, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The body structure is easily constructed by the modules, muscle attachments connecting bones and muscles, and joint attachments connecting bones and joints. \u25cf Figure 2(c) displays the detailed structure of the joint module. We can express all the basic human joints by rearranging two center parts and three axis parts. inertial measurement units (IMUs), and a circuit integrating sensor data are packaged in the joint module. We can change the body structure easily by transforming this joint module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can express all the basic human joints by rearranging two center parts and three axis parts.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0351e24b-0280-408e-a308-04f61dec4085": {"__data__": {"id_": "0351e24b-0280-408e-a308-04f61dec4085", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "\u25cf Figure 2(c) displays the detailed structure of the joint module. We can express all the basic human joints by rearranging two center parts and three axis parts. inertial measurement units (IMUs), and a circuit integrating sensor data are packaged in the joint module. We can change the body structure easily by transforming this joint module. \u25cf Figure 2(d) illustrates the detailed structure of the muscle module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "inertial measurement units (IMUs), and a circuit integrating sensor data are packaged in the joint module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca2084cd-b41f-4bd8-a9eb-390150ac1190": {"__data__": {"id_": "ca2084cd-b41f-4bd8-a9eb-390150ac1190", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We can express all the basic human joints by rearranging two center parts and three axis parts. inertial measurement units (IMUs), and a circuit integrating sensor data are packaged in the joint module. We can change the body structure easily by transforming this joint module. \u25cf Figure 2(d) illustrates the detailed structure of the muscle module. We use two kinds of muscle modules depending on the body part."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can change the body structure easily by transforming this joint module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b30da191-a3fc-4651-a44d-a9c667c22198": {"__data__": {"id_": "b30da191-a3fc-4651-a44d-a9c667c22198", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "inertial measurement units (IMUs), and a circuit integrating sensor data are packaged in the joint module. We can change the body structure easily by transforming this joint module. \u25cf Figure 2(d) illustrates the detailed structure of the muscle module. We use two kinds of muscle modules depending on the body part. One is a sensor-driver integrated mus- cle module , which can drastically improve reliability and maintainability by packaging a motor, motor driver, thermal sensor, tension measurement unit, and so on into one module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cf Figure 2(d) illustrates the detailed structure of the muscle module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49cd0ed3-2ef3-44c9-b4a5-d4f9fe5aeb99": {"__data__": {"id_": "49cd0ed3-2ef3-44c9-b4a5-d4f9fe5aeb99", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We can change the body structure easily by transforming this joint module. \u25cf Figure 2(d) illustrates the detailed structure of the muscle module. We use two kinds of muscle modules depending on the body part. One is a sensor-driver integrated mus- cle module , which can drastically improve reliability and maintainability by packaging a motor, motor driver, thermal sensor, tension measurement unit, and so on into one module. The muscle is actuated by winding a muscle wire with a pulley."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use two kinds of muscle modules depending on the body part.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c9b256b-9923-4f2b-b0b7-126ce880a277": {"__data__": {"id_": "4c9b256b-9923-4f2b-b0b7-126ce880a277", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "\u25cf Figure 2(d) illustrates the detailed structure of the muscle module. We use two kinds of muscle modules depending on the body part. One is a sensor-driver integrated mus- cle module , which can drastically improve reliability and maintainability by packaging a motor, motor driver, thermal sensor, tension measurement unit, and so on into one module. The muscle is actuated by winding a muscle wire with a pulley. The other is a miniature bone- muscle module ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One is a sensor-driver integrated mus- cle module , which can drastically improve reliability and maintainability by packaging a motor, motor driver, thermal sensor, tension measurement unit, and so on into one module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe65919f-b312-4472-b9a7-5a9974f9b36b": {"__data__": {"id_": "fe65919f-b312-4472-b9a7-5a9974f9b36b", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We use two kinds of muscle modules depending on the body part. One is a sensor-driver integrated mus- cle module , which can drastically improve reliability and maintainability by packaging a motor, motor driver, thermal sensor, tension measurement unit, and so on into one module. The muscle is actuated by winding a muscle wire with a pulley. The other is a miniature bone- muscle module . Although the basic concept is the same as in , the module is used not only as a muscle but also as a bone frame and dissipates heat to metal by packaging two smaller actuators into one module and filling the space between the two actuators with metal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The muscle is actuated by winding a muscle wire with a pulley.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "464102fa-b32d-48e2-83cd-76105518917b": {"__data__": {"id_": "464102fa-b32d-48e2-83cd-76105518917b", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "One is a sensor-driver integrated mus- cle module , which can drastically improve reliability and maintainability by packaging a motor, motor driver, thermal sensor, tension measurement unit, and so on into one module. The muscle is actuated by winding a muscle wire with a pulley. The other is a miniature bone- muscle module . Although the basic concept is the same as in , the module is used not only as a muscle but also as a bone frame and dissipates heat to metal by packaging two smaller actuators into one module and filling the space between the two actuators with metal. We can get muscle tension, length, and temperature as sensor data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The other is a miniature bone- muscle module .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53f014fc-b6a9-4ed3-8cea-88e007ac41fb": {"__data__": {"id_": "53f014fc-b6a9-4ed3-8cea-88e007ac41fb", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The muscle is actuated by winding a muscle wire with a pulley. The other is a miniature bone- muscle module . Although the basic concept is the same as in , the module is used not only as a muscle but also as a bone frame and dissipates heat to metal by packaging two smaller actuators into one module and filling the space between the two actuators with metal. We can get muscle tension, length, and temperature as sensor data. Also, as indicated in the right figure of 2(d), we can realize various muscle routes by rearranging the"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although the basic concept is the same as in , the module is used not only as a muscle but also as a bone frame and dissipates heat to metal by packaging two smaller actuators into one module and filling the space between the two actuators with metal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0497449-4d2a-4a12-ad8f-2789d83ae807": {"__data__": {"id_": "b0497449-4d2a-4a12-ad8f-2789d83ae807", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The other is a miniature bone- muscle module . Although the basic concept is the same as in , the module is used not only as a muscle but also as a bone frame and dissipates heat to metal by packaging two smaller actuators into one module and filling the space between the two actuators with metal. We can get muscle tension, length, and temperature as sensor data. Also, as indicated in the right figure of 2(d), we can realize various muscle routes by rearranging the Previous NEUs are con- structed by metal and springs, and the structure is not appropriate for environmental contact."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can get muscle tension, length, and temperature as sensor data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f18c9fe1-2b1e-4525-b081-56d3556a0a98": {"__data__": {"id_": "f18c9fe1-2b1e-4525-b081-56d3556a0a98", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Although the basic concept is the same as in , the module is used not only as a muscle but also as a bone frame and dissipates heat to metal by packaging two smaller actuators into one module and filling the space between the two actuators with metal. We can get muscle tension, length, and temperature as sensor data. Also, as indicated in the right figure of 2(d), we can realize various muscle routes by rearranging the Previous NEUs are con- structed by metal and springs, and the structure is not appropriate for environmental contact. Therefore, we realize the nonlinear elasticity using the compression of rubber, by covering the grommet rubber with Dyneema."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, as indicated in the right figure of 2(d), we can realize various muscle routes by rearranging the", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07a4f12f-6339-4035-959d-3b789afc5323": {"__data__": {"id_": "07a4f12f-6339-4035-959d-3b789afc5323", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We can get muscle tension, length, and temperature as sensor data. Also, as indicated in the right figure of 2(d), we can realize various muscle routes by rearranging the Previous NEUs are con- structed by metal and springs, and the structure is not appropriate for environmental contact. Therefore, we realize the nonlinear elasticity using the compression of rubber, by covering the grommet rubber with Dyneema. Because the NEU is constructed with only rubber and Dyneema, the structure itself is flexible, and it is suitable for environmental contact."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Previous NEUs are con- structed by metal and springs, and the structure is not appropriate for environmental contact.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b69d30ac-6905-437e-bd96-1334008a552a": {"__data__": {"id_": "b69d30ac-6905-437e-bd96-1334008a552a", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, as indicated in the right figure of 2(d), we can realize various muscle routes by rearranging the Previous NEUs are con- structed by metal and springs, and the structure is not appropriate for environmental contact. Therefore, we realize the nonlinear elasticity using the compression of rubber, by covering the grommet rubber with Dyneema. Because the NEU is constructed with only rubber and Dyneema, the structure itself is flexible, and it is suitable for environmental contact. \u25cf Figure 3(a) is the head of Musashi with the movable eye unit ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, we realize the nonlinear elasticity using the compression of rubber, by covering the grommet rubber with Dyneema.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a98c75ab-6011-48f5-918b-eee7d89a6e5c": {"__data__": {"id_": "a98c75ab-6011-48f5-918b-eee7d89a6e5c", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Previous NEUs are con- structed by metal and springs, and the structure is not appropriate for environmental contact. Therefore, we realize the nonlinear elasticity using the compression of rubber, by covering the grommet rubber with Dyneema. Because the NEU is constructed with only rubber and Dyneema, the structure itself is flexible, and it is suitable for environmental contact. \u25cf Figure 3(a) is the head of Musashi with the movable eye unit . The unit includes three joints: a pan joint in each eye and a tilt joint."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because the NEU is constructed with only rubber and Dyneema, the structure itself is flexible, and it is suitable for environmental contact.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01503144-2d64-4a36-be4a-f42903d97768": {"__data__": {"id_": "01503144-2d64-4a36-be4a-f42903d97768", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Therefore, we realize the nonlinear elasticity using the compression of rubber, by covering the grommet rubber with Dyneema. Because the NEU is constructed with only rubber and Dyneema, the structure itself is flexible, and it is suitable for environmental contact. \u25cf Figure 3(a) is the head of Musashi with the movable eye unit . The unit includes three joints: a pan joint in each eye and a tilt joint. We use DFK-AFUJ003 (ImagingSource, Inc.) as the eyes, and we can change the image resolution, focus, exposure, and so forth."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cf Figure 3(a) is the head of Musashi with the movable eye unit .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7724cdbc-3541-475d-8b52-1ba5e315e1c7": {"__data__": {"id_": "7724cdbc-3541-475d-8b52-1ba5e315e1c7", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Because the NEU is constructed with only rubber and Dyneema, the structure itself is flexible, and it is suitable for environmental contact. \u25cf Figure 3(a) is the head of Musashi with the movable eye unit . The unit includes three joints: a pan joint in each eye and a tilt joint. We use DFK-AFUJ003 (ImagingSource, Inc.) as the eyes, and we can change the image resolution, focus, exposure, and so forth. Although we considered using lidars/radars, we decid- ed consistently to mimic the structure of human beings in detail."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The unit includes three joints: a pan joint in each eye and a tilt joint.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d40e2c66-427f-43dc-99ee-2be2b750e8bf": {"__data__": {"id_": "d40e2c66-427f-43dc-99ee-2be2b750e8bf", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "\u25cf Figure 3(a) is the head of Musashi with the movable eye unit . The unit includes three joints: a pan joint in each eye and a tilt joint. We use DFK-AFUJ003 (ImagingSource, Inc.) as the eyes, and we can change the image resolution, focus, exposure, and so forth. Although we considered using lidars/radars, we decid- ed consistently to mimic the structure of human beings in detail. \u25cf Figure 3(b) is the hand of Musashi with machined springs ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use DFK-AFUJ003 (ImagingSource, Inc.) as the eyes, and we can change the image resolution, focus, exposure, and so forth.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a111898d-d2f6-419c-975d-72f932b98dd2": {"__data__": {"id_": "a111898d-d2f6-419c-975d-72f932b98dd2", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The unit includes three joints: a pan joint in each eye and a tilt joint. We use DFK-AFUJ003 (ImagingSource, Inc.) as the eyes, and we can change the image resolution, focus, exposure, and so forth. Although we considered using lidars/radars, we decid- ed consistently to mimic the structure of human beings in detail. \u25cf Figure 3(b) is the hand of Musashi with machined springs . The hand does not break, even if hit by a hammer, because of the high flexibility."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although we considered using lidars/radars, we decid- ed consistently to mimic the structure of human beings in detail.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b95592ed-5b7d-46f8-a759-e9bc99fd021c": {"__data__": {"id_": "b95592ed-5b7d-46f8-a759-e9bc99fd021c", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We use DFK-AFUJ003 (ImagingSource, Inc.) as the eyes, and we can change the image resolution, focus, exposure, and so forth. Although we considered using lidars/radars, we decid- ed consistently to mimic the structure of human beings in detail. \u25cf Figure 3(b) is the hand of Musashi with machined springs . The hand does not break, even if hit by a hammer, because of the high flexibility. Muscles are antagonistically arranged at the proximal phalanges of each finger, and the finger stiffness can be changed by pulling the muscles and compressing the springs."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cf Figure 3(b) is the hand of Musashi with machined springs .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b16f29d0-c044-46ff-8091-bddb56717138": {"__data__": {"id_": "b16f29d0-c044-46ff-8091-bddb56717138", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Although we considered using lidars/radars, we decid- ed consistently to mimic the structure of human beings in detail. \u25cf Figure 3(b) is the hand of Musashi with machined springs . The hand does not break, even if hit by a hammer, because of the high flexibility. Muscles are antagonistically arranged at the proximal phalanges of each finger, and the finger stiffness can be changed by pulling the muscles and compressing the springs. Nine load cells are arranged at each fingertip and the palm of each hand, and contact force can be measured from them."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The hand does not break, even if hit by a hammer, because of the high flexibility.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36ab6a7b-1a2d-4c73-9688-95bca8520ceb": {"__data__": {"id_": "36ab6a7b-1a2d-4c73-9688-95bca8520ceb", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "\u25cf Figure 3(b) is the hand of Musashi with machined springs . The hand does not break, even if hit by a hammer, because of the high flexibility. Muscles are antagonistically arranged at the proximal phalanges of each finger, and the finger stiffness can be changed by pulling the muscles and compressing the springs. Nine load cells are arranged at each fingertip and the palm of each hand, and contact force can be measured from them. \u25cf Figure 3(c) is the foot of Musashi with six-axis core-shell force sensors ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Muscles are antagonistically arranged at the proximal phalanges of each finger, and the finger stiffness can be changed by pulling the muscles and compressing the springs.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3cba695a-8130-4789-b5e4-8335550ed03f": {"__data__": {"id_": "3cba695a-8130-4789-b5e4-8335550ed03f", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The hand does not break, even if hit by a hammer, because of the high flexibility. Muscles are antagonistically arranged at the proximal phalanges of each finger, and the finger stiffness can be changed by pulling the muscles and compressing the springs. Nine load cells are arranged at each fingertip and the palm of each hand, and contact force can be measured from them. \u25cf Figure 3(c) is the foot of Musashi with six-axis core-shell force sensors . The core-shell structure is arranged at the toe and heel, and multiple load cells are arranged between the core and shell."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nine load cells are arranged at each fingertip and the palm of each hand, and contact force can be measured from them.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "765a1989-ad89-4cca-9793-29b0db748bb4": {"__data__": {"id_": "765a1989-ad89-4cca-9793-29b0db748bb4", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Muscles are antagonistically arranged at the proximal phalanges of each finger, and the finger stiffness can be changed by pulling the muscles and compressing the springs. Nine load cells are arranged at each fingertip and the palm of each hand, and contact force can be measured from them. \u25cf Figure 3(c) is the foot of Musashi with six-axis core-shell force sensors . The core-shell structure is arranged at the toe and heel, and multiple load cells are arranged between the core and shell. This structure can measure all of the force applied to the entire shell surface, and so the force to the instep of the foot can be measured."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cf Figure 3(c) is the foot of Musashi with six-axis core-shell force sensors .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97a0aefc-2512-4357-9725-fb60b4b24915": {"__data__": {"id_": "97a0aefc-2512-4357-9725-fb60b4b24915", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Nine load cells are arranged at each fingertip and the palm of each hand, and contact force can be measured from them. \u25cf Figure 3(c) is the foot of Musashi with six-axis core-shell force sensors . The core-shell structure is arranged at the toe and heel, and multiple load cells are arranged between the core and shell. This structure can measure all of the force applied to the entire shell surface, and so the force to the instep of the foot can be measured. and foot of Musashi in terms of autonomous driving in Figure 4 (presented in the multimedia material)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The core-shell structure is arranged at the toe and heel, and multiple load cells are arranged between the core and shell.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13f84fa2-9828-49bc-9727-b7f76491459d": {"__data__": {"id_": "13f84fa2-9828-49bc-9727-b7f76491459d", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "\u25cf Figure 3(c) is the foot of Musashi with six-axis core-shell force sensors . The core-shell structure is arranged at the toe and heel, and multiple load cells are arranged between the core and shell. This structure can measure all of the force applied to the entire shell surface, and so the force to the instep of the foot can be measured. and foot of Musashi in terms of autonomous driving in Figure 4 (presented in the multimedia material). robot can not only make the body flexible but also change the stiffness freely."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This structure can measure all of the force applied to the entire shell surface, and so the force to the instep of the foot can be measured.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a23708d3-01b1-4792-98a6-accdf6230a0d": {"__data__": {"id_": "a23708d3-01b1-4792-98a6-accdf6230a0d", "embedding": null, "metadata": {"page_number": 4, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The core-shell structure is arranged at the toe and heel, and multiple load cells are arranged between the core and shell. This structure can measure all of the force applied to the entire shell surface, and so the force to the instep of the foot can be measured. and foot of Musashi in terms of autonomous driving in Figure 4 (presented in the multimedia material). robot can not only make the body flexible but also change the stiffness freely. As demonstrated in Figure 4(a), the flexible body structure makes it possible to operate a steering wheel with both arms, which could not be seen in the DRC."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and foot of Musashi in terms of autonomous driving in Figure 4 (presented in the multimedia material).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8467ef40-f77b-4c4b-9f98-e597ce30cb09": {"__data__": {"id_": "8467ef40-f77b-4c4b-9f98-e597ce30cb09", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This structure can measure all of the force applied to the entire shell surface, and so the force to the instep of the foot can be measured. and foot of Musashi in terms of autonomous driving in Figure 4 (presented in the multimedia material). robot can not only make the body flexible but also change the stiffness freely. As demonstrated in Figure 4(a), the flexible body structure makes it possible to operate a steering wheel with both arms, which could not be seen in the DRC. Also, as depicted in Figure 4(b), the robot can change the impact response by changing its body stiffness."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "robot can not only make the body flexible but also change the stiffness freely.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02683187-7f56-4101-a593-d2e4aacb306c": {"__data__": {"id_": "02683187-7f56-4101-a593-d2e4aacb306c", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "and foot of Musashi in terms of autonomous driving in Figure 4 (presented in the multimedia material). robot can not only make the body flexible but also change the stiffness freely. As demonstrated in Figure 4(a), the flexible body structure makes it possible to operate a steering wheel with both arms, which could not be seen in the DRC. Also, as depicted in Figure 4(b), the robot can change the impact response by changing its body stiffness. When dropping a 5-kg ball to the robot from a 1-m height, the maximum mus- cle tension is 150 N with low stiffness and 250 N with high stiffness."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As demonstrated in Figure 4(a), the flexible body structure makes it possible to operate a steering wheel with both arms, which could not be seen in the DRC.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac18d023-1990-4fc0-b763-cea32dfb2456": {"__data__": {"id_": "ac18d023-1990-4fc0-b763-cea32dfb2456", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "robot can not only make the body flexible but also change the stiffness freely. As demonstrated in Figure 4(a), the flexible body structure makes it possible to operate a steering wheel with both arms, which could not be seen in the DRC. Also, as depicted in Figure 4(b), the robot can change the impact response by changing its body stiffness. When dropping a 5-kg ball to the robot from a 1-m height, the maximum mus- cle tension is 150 N with low stiffness and 250 N with high stiffness. This resembles the increase of human arm stiffness during a car crash, and the use of Musashi as a crash test"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, as depicted in Figure 4(b), the robot can change the impact response by changing its body stiffness.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "787db609-32c1-4a63-bfb1-66ec9f4bb595": {"__data__": {"id_": "787db609-32c1-4a63-bfb1-66ec9f4bb595", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "As demonstrated in Figure 4(a), the flexible body structure makes it possible to operate a steering wheel with both arms, which could not be seen in the DRC. Also, as depicted in Figure 4(b), the robot can change the impact response by changing its body stiffness. When dropping a 5-kg ball to the robot from a 1-m height, the maximum mus- cle tension is 150 N with low stiffness and 250 N with high stiffness. This resembles the increase of human arm stiffness during a car crash, and the use of Musashi as a crash test the robot can recognize objects with a wide field of view as indicated in Fig- ure 4(c)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When dropping a 5-kg ball to the robot from a 1-m height, the maximum mus- cle tension is 150 N with low stiffness and 250 N with high stiffness.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e736a53-f85f-4753-8f6a-94c0eea906a4": {"__data__": {"id_": "0e736a53-f85f-4753-8f6a-94c0eea906a4", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, as depicted in Figure 4(b), the robot can change the impact response by changing its body stiffness. When dropping a 5-kg ball to the robot from a 1-m height, the maximum mus- cle tension is 150 N with low stiffness and 250 N with high stiffness. This resembles the increase of human arm stiffness during a car crash, and the use of Musashi as a crash test the robot can recognize objects with a wide field of view as indicated in Fig- ure 4(c). Also, the robot can recognize a human in a side mir- ror using the high-resolution camera, as shown in Figure 4(d)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This resembles the increase of human arm stiffness during a car crash, and the use of Musashi as a crash test", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3df090c-21d5-4383-b5fb-3a4eafa40f34": {"__data__": {"id_": "f3df090c-21d5-4383-b5fb-3a4eafa40f34", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "When dropping a 5-kg ball to the robot from a 1-m height, the maximum mus- cle tension is 150 N with low stiffness and 250 N with high stiffness. This resembles the increase of human arm stiffness during a car crash, and the use of Musashi as a crash test the robot can recognize objects with a wide field of view as indicated in Fig- ure 4(c). Also, the robot can recognize a human in a side mir- ror using the high-resolution camera, as shown in Figure 4(d). Using the flexible hand with machined springs, the robot can adapt the grasp shape to various situations inside the car, such as pulling a handbrake [Figure 4(e)] and rotating a key [Fig - ure 4(f)]."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot can recognize objects with a wide field of view as indicated in Fig- ure 4(c).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d1bf235-ae8a-45ea-9892-aa3993159787": {"__data__": {"id_": "3d1bf235-ae8a-45ea-9892-aa3993159787", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This resembles the increase of human arm stiffness during a car crash, and the use of Musashi as a crash test the robot can recognize objects with a wide field of view as indicated in Fig- ure 4(c). Also, the robot can recognize a human in a side mir- ror using the high-resolution camera, as shown in Figure 4(d). Using the flexible hand with machined springs, the robot can adapt the grasp shape to various situations inside the car, such as pulling a handbrake [Figure 4(e)] and rotating a key [Fig - ure 4(f)]. Also, using the variable-stiffness mechanism of fin- gers, as displayed in Figure 4(g), the robot can operate a blinker lever correctly with high stiffness."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, the robot can recognize a human in a side mir- ror using the high-resolution camera, as shown in Figure 4(d).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "683fa2dd-5162-49e8-bc10-b8bdcadd1559": {"__data__": {"id_": "683fa2dd-5162-49e8-bc10-b8bdcadd1559", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "the robot can recognize objects with a wide field of view as indicated in Fig- ure 4(c). Also, the robot can recognize a human in a side mir- ror using the high-resolution camera, as shown in Figure 4(d). Using the flexible hand with machined springs, the robot can adapt the grasp shape to various situations inside the car, such as pulling a handbrake [Figure 4(e)] and rotating a key [Fig - ure 4(f)]. Also, using the variable-stiffness mechanism of fin- gers, as displayed in Figure 4(g), the robot can operate a blinker lever correctly with high stiffness. Using the foot, which can measure the force applied to its entire surface, the robot can"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using the flexible hand with machined springs, the robot can adapt the grasp shape to various situations inside the car, such as pulling a handbrake [Figure 4(e)] and rotating a key [Fig - ure 4(f)].", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16a92def-febc-4d7b-a3f7-f85f064ae547": {"__data__": {"id_": "16a92def-febc-4d7b-a3f7-f85f064ae547", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, the robot can recognize a human in a side mir- ror using the high-resolution camera, as shown in Figure 4(d). Using the flexible hand with machined springs, the robot can adapt the grasp shape to various situations inside the car, such as pulling a handbrake [Figure 4(e)] and rotating a key [Fig - ure 4(f)]. Also, using the variable-stiffness mechanism of fin- gers, as displayed in Figure 4(g), the robot can operate a blinker lever correctly with high stiffness. Using the foot, which can measure the force applied to its entire surface, the robot can The realization of the respective components of autonomous driving using the characteristics of the developed hardware: (a) the steering wheel operation with both arms, (b) the variable-stiffness control using the NEUs, (c) the field of view of the movable eye unit, (d) a human detection experiment in the side mirror, (e) pulling a handbrake, (f) rotating a key, (g) operating a blinker lever by changing the stiffness of fingers, and (h) recovering from slipping during brake pedal operation using the developed foot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, using the variable-stiffness mechanism of fin- gers, as displayed in Figure 4(g), the robot can operate a blinker lever correctly with high stiffness.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35cecdb6-973b-4c34-9478-584d81a4429e": {"__data__": {"id_": "35cecdb6-973b-4c34-9478-584d81a4429e", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Using the flexible hand with machined springs, the robot can adapt the grasp shape to various situations inside the car, such as pulling a handbrake [Figure 4(e)] and rotating a key [Fig - ure 4(f)]. Also, using the variable-stiffness mechanism of fin- gers, as displayed in Figure 4(g), the robot can operate a blinker lever correctly with high stiffness. Using the foot, which can measure the force applied to its entire surface, the robot can The realization of the respective components of autonomous driving using the characteristics of the developed hardware: (a) the steering wheel operation with both arms, (b) the variable-stiffness control using the NEUs, (c) the field of view of the movable eye unit, (d) a human detection experiment in the side mirror, (e) pulling a handbrake, (f) rotating a key, (g) operating a blinker lever by changing the stiffness of fingers, and (h) recovering from slipping during brake pedal operation using the developed foot. we use a simple conditional branching based on"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using the foot, which can measure the force applied to its entire surface, the robot can", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d901f3e5-a9bf-40ea-a47e-a1b8c840456d": {"__data__": {"id_": "d901f3e5-a9bf-40ea-a47e-a1b8c840456d", "embedding": null, "metadata": {"page_number": 5, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, using the variable-stiffness mechanism of fin- gers, as displayed in Figure 4(g), the robot can operate a blinker lever correctly with high stiffness. Using the foot, which can measure the force applied to its entire surface, the robot can The realization of the respective components of autonomous driving using the characteristics of the developed hardware: (a) the steering wheel operation with both arms, (b) the variable-stiffness control using the NEUs, (c) the field of view of the movable eye unit, (d) a human detection experiment in the side mirror, (e) pulling a handbrake, (f) rotating a key, (g) operating a blinker lever by changing the stiffness of fingers, and (h) recovering from slipping during brake pedal operation using the developed foot. we use a simple conditional branching based on We show the overview of the learning-based software system in Figure 5."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The realization of the respective components of autonomous driving using the characteristics of the developed hardware: (a) the steering wheel operation with both arms, (b) the variable-stiffness control using the NEUs, (c) the field of view of the movable eye unit, (d) a human detection experiment in the side mirror, (e) pulling a handbrake, (f) rotating a key, (g) operating a blinker lever by changing the stiffness of fingers, and (h) recovering from slipping during brake pedal operation using the developed foot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce853ad3-dab2-410e-8b66-c8f5e1c46aa5": {"__data__": {"id_": "ce853ad3-dab2-410e-8b66-c8f5e1c46aa5", "embedding": null, "metadata": {"page_number": 6, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Using the foot, which can measure the force applied to its entire surface, the robot can The realization of the respective components of autonomous driving using the characteristics of the developed hardware: (a) the steering wheel operation with both arms, (b) the variable-stiffness control using the NEUs, (c) the field of view of the movable eye unit, (d) a human detection experiment in the side mirror, (e) pulling a handbrake, (f) rotating a key, (g) operating a blinker lever by changing the stiffness of fingers, and (h) recovering from slipping during brake pedal operation using the developed foot. we use a simple conditional branching based on We show the overview of the learning-based software system in Figure 5. We describe the current muscle length as l, current muscle tension as f, current joint angle as ,i current muscle temperature as c, the muscle Jacobian as G, current image as I, current sound as S, task state as stask current state obtained from , vision as svision and current state obtained , from sound as ssound Also, { , ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we use a simple conditional branching based on", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f356d76-ee31-47cb-ab29-0a945d391e1b": {"__data__": {"id_": "4f356d76-ee31-47cb-ab29-0a945d391e1b", "embedding": null, "metadata": {"page_number": 6, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The realization of the respective components of autonomous driving using the characteristics of the developed hardware: (a) the steering wheel operation with both arms, (b) the variable-stiffness control using the NEUs, (c) the field of view of the movable eye unit, (d) a human detection experiment in the side mirror, (e) pulling a handbrake, (f) rotating a key, (g) operating a blinker lever by changing the stiffness of fingers, and (h) recovering from slipping during brake pedal operation using the developed foot. we use a simple conditional branching based on We show the overview of the learning-based software system in Figure 5. We describe the current muscle length as l, current muscle tension as f, current joint angle as ,i current muscle temperature as c, the muscle Jacobian as G, current image as I, current sound as S, task state as stask current state obtained from , vision as svision and current state obtained , from sound as ssound Also, { , . l i , f stask , } ref is the control command of each state, and esti is the estimated joint angle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We show the overview of the learning-based software system in Figure 5.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cbf9004a-14c0-41a9-93c6-7759ad546682": {"__data__": {"id_": "cbf9004a-14c0-41a9-93c6-7759ad546682", "embedding": null, "metadata": {"page_number": 6, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we use a simple conditional branching based on We show the overview of the learning-based software system in Figure 5. We describe the current muscle length as l, current muscle tension as f, current joint angle as ,i current muscle temperature as c, the muscle Jacobian as G, current image as I, current sound as S, task state as stask current state obtained from , vision as svision and current state obtained , from sound as ssound Also, { , . l i , f stask , } ref is the control command of each state, and esti is the estimated joint angle. The basic components of this system are the intersensory network module (static module) [Figure 5(a)], dynamic task control network module (dynamic module) [Fig- ure 5(b)], reflex module [Figure 5(c)], and recognition module [Figure 5(d)]."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We describe the current muscle length as l, current muscle tension as f, current joint angle as ,i current muscle temperature as c, the muscle Jacobian as G, current image as I, current sound as S, task state as stask current state obtained from , vision as svision and current state obtained , from sound as ssound Also, { , .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2666cbef-f4d4-47ab-a99a-45ef66f264d4": {"__data__": {"id_": "2666cbef-f4d4-47ab-a99a-45ef66f264d4", "embedding": null, "metadata": {"page_number": 6, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We show the overview of the learning-based software system in Figure 5. We describe the current muscle length as l, current muscle tension as f, current joint angle as ,i current muscle temperature as c, the muscle Jacobian as G, current image as I, current sound as S, task state as stask current state obtained from , vision as svision and current state obtained , from sound as ssound Also, { , . l i , f stask , } ref is the control command of each state, and esti is the estimated joint angle. The basic components of this system are the intersensory network module (static module) [Figure 5(a)], dynamic task control network module (dynamic module) [Fig- ure 5(b)], reflex module [Figure 5(c)], and recognition module [Figure 5(d)]. The static module [Figure 5(a)] acquires the following function hstatic :"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "l i , f stask , } ref is the control command of each state, and esti is the estimated joint angle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0774129-fe44-459c-a854-cf4c1a59508d": {"__data__": {"id_": "c0774129-fe44-459c-a854-cf4c1a59508d", "embedding": null, "metadata": {"page_number": 6, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We describe the current muscle length as l, current muscle tension as f, current joint angle as ,i current muscle temperature as c, the muscle Jacobian as G, current image as I, current sound as S, task state as stask current state obtained from , vision as svision and current state obtained , from sound as ssound Also, { , . l i , f stask , } ref is the control command of each state, and esti is the estimated joint angle. The basic components of this system are the intersensory network module (static module) [Figure 5(a)], dynamic task control network module (dynamic module) [Fig- ure 5(b)], reflex module [Figure 5(c)], and recognition module [Figure 5(d)]. The static module [Figure 5(a)] acquires the following function hstatic : into (1), lref to send to the robot is calculated."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The basic components of this system are the intersensory network module (static module) [Figure 5(a)], dynamic task control network module (dynamic module) [Fig- ure 5(b)], reflex module [Figure 5(c)], and recognition module [Figure 5(d)].", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4e67bde-8edb-4475-a415-667ad3f7c1c4": {"__data__": {"id_": "f4e67bde-8edb-4475-a415-667ad3f7c1c4", "embedding": null, "metadata": {"page_number": 6, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "l i , f stask , } ref is the control command of each state, and esti is the estimated joint angle. The basic components of this system are the intersensory network module (static module) [Figure 5(a)], dynamic task control network module (dynamic module) [Fig- ure 5(b)], reflex module [Figure 5(c)], and recognition module [Figure 5(d)]. The static module [Figure 5(a)] acquires the following function hstatic : into (1), lref to send to the robot is calculated. This function is expressed by a neural network and is initialized by human made data."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The static module [Figure 5(a)] acquires the following function hstatic :", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9cef4ce6-733b-415c-9275-55bd772a77f1": {"__data__": {"id_": "9cef4ce6-733b-415c-9275-55bd772a77f1", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The basic components of this system are the intersensory network module (static module) [Figure 5(a)], dynamic task control network module (dynamic module) [Fig- ure 5(b)], reflex module [Figure 5(c)], and recognition module [Figure 5(d)]. The static module [Figure 5(a)] acquires the following function hstatic : into (1), lref to send to the robot is calculated. This function is expressed by a neural network and is initialized by human made data. Then, we update the network online by using the actual robot sensor information ( , i f l , ) At every movement, ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "into (1), lref to send to the robot is calculated.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24ace6ff-47f7-4945-95c5-faf26b5b5bed": {"__data__": {"id_": "24ace6ff-47f7-4945-95c5-faf26b5b5bed", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The static module [Figure 5(a)] acquires the following function hstatic : into (1), lref to send to the robot is calculated. This function is expressed by a neural network and is initialized by human made data. Then, we update the network online by using the actual robot sensor information ( , i f l , ) At every movement, . the network is updated, and the robot becomes able to realize refi , f ref accurately."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This function is expressed by a neural network and is initialized by human made data.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d39fa593-f982-4591-8e49-6930edaa0060": {"__data__": {"id_": "d39fa593-f982-4591-8e49-6930edaa0060", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "into (1), lref to send to the robot is calculated. This function is expressed by a neural network and is initialized by human made data. Then, we update the network online by using the actual robot sensor information ( , i f l , ) At every movement, . the network is updated, and the robot becomes able to realize refi , f ref accurately. In this study, i can be obtained from the joint module, but the ordinary musculoskeletal humanoid does not have joint angle sensors due to complex joints, such as the ball or scapula joints."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, we update the network online by using the actual robot sensor information ( , i f l , ) At every movement, .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83498cb2-5826-4328-966c-9f1ce4072c6e": {"__data__": {"id_": "83498cb2-5826-4328-966c-9f1ce4072c6e", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This function is expressed by a neural network and is initialized by human made data. Then, we update the network online by using the actual robot sensor information ( , i f l , ) At every movement, . the network is updated, and the robot becomes able to realize refi , f ref accurately. In this study, i can be obtained from the joint module, but the ordinary musculoskeletal humanoid does not have joint angle sensors due to complex joints, such as the ball or scapula joints. In that case, i must be obtained from a motion capture or vision sensor."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the network is updated, and the robot becomes able to realize refi , f ref accurately.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "155c6d67-9e66-4ae3-babf-b39ac8bc8310": {"__data__": {"id_": "155c6d67-9e66-4ae3-babf-b39ac8bc8310", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Then, we update the network online by using the actual robot sensor information ( , i f l , ) At every movement, . the network is updated, and the robot becomes able to realize refi , f ref accurately. In this study, i can be obtained from the joint module, but the ordinary musculoskeletal humanoid does not have joint angle sensors due to complex joints, such as the ball or scapula joints. In that case, i must be obtained from a motion capture or vision sensor. Also, by using this network hstatic, not only control but also estimation of joint angles esti are enabled by using an extended Kalman filter and the change in muscle length and tension."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this study, i can be obtained from the joint module, but the ordinary musculoskeletal humanoid does not have joint angle sensors due to complex joints, such as the ball or scapula joints.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "214ccb85-281b-4d5a-90d3-73f4338803be": {"__data__": {"id_": "214ccb85-281b-4d5a-90d3-73f4338803be", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "the network is updated, and the robot becomes able to realize refi , f ref accurately. In this study, i can be obtained from the joint module, but the ordinary musculoskeletal humanoid does not have joint angle sensors due to complex joints, such as the ball or scapula joints. In that case, i must be obtained from a motion capture or vision sensor. Also, by using this network hstatic, not only control but also estimation of joint angles esti are enabled by using an extended Kalman filter and the change in muscle length and tension. The dynamic module [Figure 5(b)] acquires the function hdynamic"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In that case, i must be obtained from a motion capture or vision sensor.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "348b29e3-5db0-40e8-b0c9-2aefe7395e9b": {"__data__": {"id_": "348b29e3-5db0-40e8-b0c9-2aefe7395e9b", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In this study, i can be obtained from the joint module, but the ordinary musculoskeletal humanoid does not have joint angle sensors due to complex joints, such as the ball or scapula joints. In that case, i must be obtained from a motion capture or vision sensor. Also, by using this network hstatic, not only control but also estimation of joint angles esti are enabled by using an extended Kalman filter and the change in muscle length and tension. The dynamic module [Figure 5(b)] acquires the function hdynamic where { s task , } u [ t 1 2 is a vector vertically arranging { , t ] s task , } u taskl from t1 to t2 time steps, s stask is the initial task state ({ , ,t o in this study), u is the control command ( refi or lref estio , } l l , esti , in this study), and N is the number of time steps to expand this state equation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, by using this network hstatic, not only control but also estimation of joint angles esti are enabled by using an extended Kalman filter and the change in muscle length and tension.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "070ce05d-7674-4a6f-98cc-189edd9189e2": {"__data__": {"id_": "070ce05d-7674-4a6f-98cc-189edd9189e2", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In that case, i must be obtained from a motion capture or vision sensor. Also, by using this network hstatic, not only control but also estimation of joint angles esti are enabled by using an extended Kalman filter and the change in muscle length and tension. The dynamic module [Figure 5(b)] acquires the function hdynamic where { s task , } u [ t 1 2 is a vector vertically arranging { , t ] s task , } u taskl from t1 to t2 time steps, s stask is the initial task state ({ , ,t o in this study), u is the control command ( refi or lref estio , } l l , esti , in this study), and N is the number of time steps to expand this state equation. This function is equivalent to a network repre senting a dynamic transition of the task state by a control com mand sequence."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dynamic module [Figure 5(b)] acquires the function hdynamic", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2603c433-85cd-4c63-8c17-6ca379b0fb24": {"__data__": {"id_": "2603c433-85cd-4c63-8c17-6ca379b0fb24", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, by using this network hstatic, not only control but also estimation of joint angles esti are enabled by using an extended Kalman filter and the change in muscle length and tension. The dynamic module [Figure 5(b)] acquires the function hdynamic where { s task , } u [ t 1 2 is a vector vertically arranging { , t ] s task , } u taskl from t1 to t2 time steps, s stask is the initial task state ({ , ,t o in this study), u is the control command ( refi or lref estio , } l l , esti , in this study), and N is the number of time steps to expand this state equation. This function is equivalent to a network repre senting a dynamic transition of the task state by a control com mand sequence. This network can be trained by gathering data of the observed task state transition when sending random con trol commands."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where { s task , } u [ t 1 2 is a vector vertically arranging { , t ] s task , } u taskl from t1 to t2 time steps, s stask is the initial task state ({ , ,t o in this study), u is the control command ( refi or lref estio , } l l , esti , in this study), and N is the number of time steps to expand this state equation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb6c30ad-828a-4186-b725-e886ab526091": {"__data__": {"id_": "cb6c30ad-828a-4186-b725-e886ab526091", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The dynamic module [Figure 5(b)] acquires the function hdynamic where { s task , } u [ t 1 2 is a vector vertically arranging { , t ] s task , } u taskl from t1 to t2 time steps, s stask is the initial task state ({ , ,t o in this study), u is the control command ( refi or lref estio , } l l , esti , in this study), and N is the number of time steps to expand this state equation. This function is equivalent to a network repre senting a dynamic transition of the task state by a control com mand sequence. This network can be trained by gathering data of the observed task state transition when sending random con trol commands. When realizing a certain task state ref stask , we execute the following equations after setting the initial u:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This function is equivalent to a network repre senting a dynamic transition of the task state by a control com mand sequence.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9df13367-0980-4d04-abcd-5ec86152cc57": {"__data__": {"id_": "9df13367-0980-4d04-abcd-5ec86152cc57", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "where { s task , } u [ t 1 2 is a vector vertically arranging { , t ] s task , } u taskl from t1 to t2 time steps, s stask is the initial task state ({ , ,t o in this study), u is the control command ( refi or lref estio , } l l , esti , in this study), and N is the number of time steps to expand this state equation. This function is equivalent to a network repre senting a dynamic transition of the task state by a control com mand sequence. This network can be trained by gathering data of the observed task state transition when sending random con trol commands. When realizing a certain task state ref stask , we execute the following equations after setting the initial u: Also, MSE is mean squared error, Eadj is the MSE of the values at adjacent time steps, a is a weight constant, and b is an update rate."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This network can be trained by gathering data of the observed task state transition when sending random con trol commands.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c89970ad-e67f-4ca6-866c-16342819768f": {"__data__": {"id_": "c89970ad-e67f-4ca6-866c-16342819768f", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This function is equivalent to a network repre senting a dynamic transition of the task state by a control com mand sequence. This network can be trained by gathering data of the observed task state transition when sending random con trol commands. When realizing a certain task state ref stask , we execute the following equations after setting the initial u: Also, MSE is mean squared error, Eadj is the MSE of the values at adjacent time steps, a is a weight constant, and b is an update rate. We cal culate the loss between the predicted and target stask add the , loss to smooth the target control command sequence, and update the control command sequence by backpropagation ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When realizing a certain task state ref stask , we execute the following equations after setting the initial u:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f277e7f-1f0e-4f2a-affa-67b375fdfa17": {"__data__": {"id_": "7f277e7f-1f0e-4f2a-affa-67b375fdfa17", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This network can be trained by gathering data of the observed task state transition when sending random con trol commands. When realizing a certain task state ref stask , we execute the following equations after setting the initial u: Also, MSE is mean squared error, Eadj is the MSE of the values at adjacent time steps, a is a weight constant, and b is an update rate. We cal culate the loss between the predicted and target stask add the , loss to smooth the target control command sequence, and update the control command sequence by backpropagation . By repeating this procedure, useq init is updated to accurately realize ref stask , and the task is executed by sending ut init to the actual robot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, MSE is mean squared error, Eadj is the MSE of the values at adjacent time steps, a is a weight constant, and b is an update rate.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fec8cc29-296b-446b-832e-c8cee8e2b9f9": {"__data__": {"id_": "fec8cc29-296b-446b-832e-c8cee8e2b9f9", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "When realizing a certain task state ref stask , we execute the following equations after setting the initial u: Also, MSE is mean squared error, Eadj is the MSE of the values at adjacent time steps, a is a weight constant, and b is an update rate. We cal culate the loss between the predicted and target stask add the , loss to smooth the target control command sequence, and update the control command sequence by backpropagation . By repeating this procedure, useq init is updated to accurately realize ref stask , and the task is executed by sending ut init to the actual robot. While Figure 5(a) shows the network regarding static motions, Figure 5(b) is the network regarding dynamic motions, and its learning is difficult due to more variables."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We cal culate the loss between the predicted and target stask add the , loss to smooth the target control command sequence, and update the control command sequence by backpropagation .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1ae9a87-9a13-4d14-8875-4cc3762b49b2": {"__data__": {"id_": "e1ae9a87-9a13-4d14-8875-4cc3762b49b2", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, MSE is mean squared error, Eadj is the MSE of the values at adjacent time steps, a is a weight constant, and b is an update rate. We cal culate the loss between the predicted and target stask add the , loss to smooth the target control command sequence, and update the control command sequence by backpropagation . By repeating this procedure, useq init is updated to accurately realize ref stask , and the task is executed by sending ut init to the actual robot. While Figure 5(a) shows the network regarding static motions, Figure 5(b) is the network regarding dynamic motions, and its learning is difficult due to more variables. Thus the robot is moved mainly by the mechanism of Fig ure 5(a), but the mechanism in Figure 5(b) is constructed offline and used when conducting tasks in which dynamic and accurate motions are required."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By repeating this procedure, useq init is updated to accurately realize ref stask , and the task is executed by sending ut init to the actual robot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ba9ec66-2d83-4209-ad0b-492d9ecdab2d": {"__data__": {"id_": "9ba9ec66-2d83-4209-ad0b-492d9ecdab2d", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We cal culate the loss between the predicted and target stask add the , loss to smooth the target control command sequence, and update the control command sequence by backpropagation . By repeating this procedure, useq init is updated to accurately realize ref stask , and the task is executed by sending ut init to the actual robot. While Figure 5(a) shows the network regarding static motions, Figure 5(b) is the network regarding dynamic motions, and its learning is difficult due to more variables. Thus the robot is moved mainly by the mechanism of Fig ure 5(a), but the mechanism in Figure 5(b) is constructed offline and used when conducting tasks in which dynamic and accurate motions are required. The reflex module [Figure 5(c)] is executed at high fre quency."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While Figure 5(a) shows the network regarding static motions, Figure 5(b) is the network regarding dynamic motions, and its learning is difficult due to more variables.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2cfcdc5-0bbb-469c-936f-4540ba98589e": {"__data__": {"id_": "c2cfcdc5-0bbb-469c-936f-4540ba98589e", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "By repeating this procedure, useq init is updated to accurately realize ref stask , and the task is executed by sending ut init to the actual robot. While Figure 5(a) shows the network regarding static motions, Figure 5(b) is the network regarding dynamic motions, and its learning is difficult due to more variables. Thus the robot is moved mainly by the mechanism of Fig ure 5(a), but the mechanism in Figure 5(b) is constructed offline and used when conducting tasks in which dynamic and accurate motions are required. The reflex module [Figure 5(c)] is executed at high fre quency. we calcu late the necessary muscle tension x that achieves the necessary joint torque necx by solving quadratic programming:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus the robot is moved mainly by the mechanism of Fig ure 5(a), but the mechanism in Figure 5(b) is constructed offline and used when conducting tasks in which dynamic and accurate motions are required.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3825d84b-088d-43f5-a57d-a81aba68641b": {"__data__": {"id_": "3825d84b-088d-43f5-a57d-a81aba68641b", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "While Figure 5(a) shows the network regarding static motions, Figure 5(b) is the network regarding dynamic motions, and its learning is difficult due to more variables. Thus the robot is moved mainly by the mechanism of Fig ure 5(a), but the mechanism in Figure 5(b) is constructed offline and used when conducting tasks in which dynamic and accurate motions are required. The reflex module [Figure 5(c)] is executed at high fre quency. we calcu late the necessary muscle tension x that achieves the necessary joint torque necx by solving quadratic programming: W W1 2 are weight matrices and f min is the minimum muscle tension."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reflex module [Figure 5(c)] is executed at high fre quency.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20d1c5b7-78f2-467f-8a53-3acb9a924dd5": {"__data__": {"id_": "20d1c5b7-78f2-467f-8a53-3acb9a924dd5", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Thus the robot is moved mainly by the mechanism of Fig ure 5(a), but the mechanism in Figure 5(b) is constructed offline and used when conducting tasks in which dynamic and accurate motions are required. The reflex module [Figure 5(c)] is executed at high fre quency. we calcu late the necessary muscle tension x that achieves the necessary joint torque necx by solving quadratic programming: W W1 2 are weight matrices and f min is the minimum muscle tension. We sort muscles by x in ascending order and gradually elongate them in order starting with unnecessary antagonist muscles with smaller tension."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we calcu late the necessary muscle tension x that achieves the necessary joint torque necx by solving quadratic programming:", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d803afc-6f92-466e-bb2d-519e9d48b617": {"__data__": {"id_": "5d803afc-6f92-466e-bb2d-519e9d48b617", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The reflex module [Figure 5(c)] is executed at high fre quency. we calcu late the necessary muscle tension x that achieves the necessary joint torque necx by solving quadratic programming: W W1 2 are weight matrices and f min is the minimum muscle tension. We sort muscles by x in ascending order and gradually elongate them in order starting with unnecessary antagonist muscles with smaller tension. In detail, we gradu ally increase the muscle relaxation value lrelaxD and add it to the muscle length to send to the actual robot."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "W W1 2 are weight matrices and f min is the minimum muscle tension.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07e3f314-e2f7-4087-83da-d54b0e231a09": {"__data__": {"id_": "07e3f314-e2f7-4087-83da-d54b0e231a09", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we calcu late the necessary muscle tension x that achieves the necessary joint torque necx by solving quadratic programming: W W1 2 are weight matrices and f min is the minimum muscle tension. We sort muscles by x in ascending order and gradually elongate them in order starting with unnecessary antagonist muscles with smaller tension. In detail, we gradu ally increase the muscle relaxation value lrelaxD and add it to the muscle length to send to the actual robot. When the mus cle tension becomes smaller than f ,min the muscle to elongate is changed to the next muscle, and this control stops when the current joint angle is changed more than a threshold."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We sort muscles by x in ascending order and gradually elongate them in order starting with unnecessary antagonist muscles with smaller tension.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5db1f66-0ef8-4b1d-80c8-b3cc79cd40c8": {"__data__": {"id_": "a5db1f66-0ef8-4b1d-80c8-b3cc79cd40c8", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "W W1 2 are weight matrices and f min is the minimum muscle tension. We sort muscles by x in ascending order and gradually elongate them in order starting with unnecessary antagonist muscles with smaller tension. In detail, we gradu ally increase the muscle relaxation value lrelaxD and add it to the muscle length to send to the actual robot. When the mus cle tension becomes smaller than f ,min the muscle to elongate is changed to the next muscle, and this control stops when the current joint angle is changed more than a threshold. At a moving state, we sort muscles by x in descending order and gradually decrease lrelaxD starting with necessary muscles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In detail, we gradu ally increase the muscle relaxation value lrelaxD and add it to the muscle length to send to the actual robot.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1b1b124-aa3b-4e2f-a287-8a5e37831301": {"__data__": {"id_": "f1b1b124-aa3b-4e2f-a287-8a5e37831301", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We sort muscles by x in ascending order and gradually elongate them in order starting with unnecessary antagonist muscles with smaller tension. In detail, we gradu ally increase the muscle relaxation value lrelaxD and add it to the muscle length to send to the actual robot. When the mus cle tension becomes smaller than f ,min the muscle to elongate is changed to the next muscle, and this control stops when the current joint angle is changed more than a threshold. At a moving state, we sort muscles by x in descending order and gradually decrease lrelaxD starting with necessary muscles. This reflex can inhibit unnecessary muscle tension of antagonist muscles due to the model error."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When the mus cle tension becomes smaller than f ,min the muscle to elongate is changed to the next muscle, and this control stops when the current joint angle is changed more than a threshold.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1874ec88-199b-4aa6-ae4a-dbc1f356f779": {"__data__": {"id_": "1874ec88-199b-4aa6-ae4a-dbc1f356f779", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In detail, we gradu ally increase the muscle relaxation value lrelaxD and add it to the muscle length to send to the actual robot. When the mus cle tension becomes smaller than f ,min the muscle to elongate is changed to the next muscle, and this control stops when the current joint angle is changed more than a threshold. At a moving state, we sort muscles by x in descending order and gradually decrease lrelaxD starting with necessary muscles. This reflex can inhibit unnecessary muscle tension of antagonist muscles due to the model error. Also, for example, when resting the arms on the table, the body and environment are constrained, and so not only the antagonist muscle tension but also the agonist mus cle tension can be reduced because the joint angle does not change even when relaxing agonist muscles."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At a moving state, we sort muscles by x in descending order and gradually decrease lrelaxD starting with necessary muscles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b84f75f2-6cf9-4721-ad2e-8c1997b829da": {"__data__": {"id_": "b84f75f2-6cf9-4721-ad2e-8c1997b829da", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "When the mus cle tension becomes smaller than f ,min the muscle to elongate is changed to the next muscle, and this control stops when the current joint angle is changed more than a threshold. At a moving state, we sort muscles by x in descending order and gradually decrease lrelaxD starting with necessary muscles. This reflex can inhibit unnecessary muscle tension of antagonist muscles due to the model error. Also, for example, when resting the arms on the table, the body and environment are constrained, and so not only the antagonist muscle tension but also the agonist mus cle tension can be reduced because the joint angle does not change even when relaxing agonist muscles. Thus, the robot becomes able to rest its body and move continuously for a longer time."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This reflex can inhibit unnecessary muscle tension of antagonist muscles due to the model error.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15d7fe38-76ac-4c70-a201-c65ba108a9d2": {"__data__": {"id_": "15d7fe38-76ac-4c70-a201-c65ba108a9d2", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "At a moving state, we sort muscles by x in descending order and gradually decrease lrelaxD starting with necessary muscles. This reflex can inhibit unnecessary muscle tension of antagonist muscles due to the model error. Also, for example, when resting the arms on the table, the body and environment are constrained, and so not only the antagonist muscle tension but also the agonist mus cle tension can be reduced because the joint angle does not change even when relaxing agonist muscles. Thus, the robot becomes able to rest its body and move continuously for a longer time. Safety reflex is a simple control that elongates muscle length in order not to break motors due to high mus cle tension and temperature."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, for example, when resting the arms on the table, the body and environment are constrained, and so not only the antagonist muscle tension but also the agonist mus cle tension can be reduced because the joint angle does not change even when relaxing agonist muscles.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b6bd240-efc2-4fec-b15b-b420e729fc28": {"__data__": {"id_": "1b6bd240-efc2-4fec-b15b-b420e729fc28", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This reflex can inhibit unnecessary muscle tension of antagonist muscles due to the model error. Also, for example, when resting the arms on the table, the body and environment are constrained, and so not only the antagonist muscle tension but also the agonist mus cle tension can be reduced because the joint angle does not change even when relaxing agonist muscles. Thus, the robot becomes able to rest its body and move continuously for a longer time. Safety reflex is a simple control that elongates muscle length in order not to break motors due to high mus cle tension and temperature. f c lim is a threshold of muscle tension or temperature where { , } to begin the reflex, K{ , }f c is a gain, refD is an ideal elongation lsafe value, and D l{ min max , } is a minimum or maximum change of lsafeD at one time step."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, the robot becomes able to rest its body and move continuously for a longer time.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef0f4242-b98e-48ec-9abb-305d3b6fb5ee": {"__data__": {"id_": "ef0f4242-b98e-48ec-9abb-305d3b6fb5ee", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, for example, when resting the arms on the table, the body and environment are constrained, and so not only the antagonist muscle tension but also the agonist mus cle tension can be reduced because the joint angle does not change even when relaxing agonist muscles. Thus, the robot becomes able to rest its body and move continuously for a longer time. Safety reflex is a simple control that elongates muscle length in order not to break motors due to high mus cle tension and temperature. f c lim is a threshold of muscle tension or temperature where { , } to begin the reflex, K{ , }f c is a gain, refD is an ideal elongation lsafe value, and D l{ min max , } is a minimum or maximum change of lsafeD at one time step. By elongating the muscle length con sidering muscle tension and temperature, we can inhibit the burnout of motors."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Safety reflex is a simple control that elongates muscle length in order not to break motors due to high mus cle tension and temperature.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "084f9735-27c2-4628-b209-af3c7c84bd63": {"__data__": {"id_": "084f9735-27c2-4628-b209-af3c7c84bd63", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Thus, the robot becomes able to rest its body and move continuously for a longer time. Safety reflex is a simple control that elongates muscle length in order not to break motors due to high mus cle tension and temperature. f c lim is a threshold of muscle tension or temperature where { , } to begin the reflex, K{ , }f c is a gain, refD is an ideal elongation lsafe value, and D l{ min max , } is a minimum or maximum change of lsafeD at one time step. By elongating the muscle length con sidering muscle tension and temperature, we can inhibit the burnout of motors. The recognition module [Figure 5(d)] is divided into object and sound recognition."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "f c lim is a threshold of muscle tension or temperature where { , } to begin the reflex, K{ , }f c is a gain, refD is an ideal elongation lsafe value, and D l{ min max , } is a minimum or maximum change of lsafeD at one time step.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47f3a565-d5c3-441b-be03-8b7a4101257d": {"__data__": {"id_": "47f3a565-d5c3-441b-be03-8b7a4101257d", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Safety reflex is a simple control that elongates muscle length in order not to break motors due to high mus cle tension and temperature. f c lim is a threshold of muscle tension or temperature where { , } to begin the reflex, K{ , }f c is a gain, refD is an ideal elongation lsafe value, and D l{ min max , } is a minimum or maximum change of lsafeD at one time step. By elongating the muscle length con sidering muscle tension and temperature, we can inhibit the burnout of motors. The recognition module [Figure 5(d)] is divided into object and sound recognition. In this study, we use Yolo v3 for object recognition."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By elongating the muscle length con sidering muscle tension and temperature, we can inhibit the burnout of motors.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b75efe4-9be9-4d3c-a572-24e18921dd9b": {"__data__": {"id_": "9b75efe4-9be9-4d3c-a572-24e18921dd9b", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "f c lim is a threshold of muscle tension or temperature where { , } to begin the reflex, K{ , }f c is a gain, refD is an ideal elongation lsafe value, and D l{ min max , } is a minimum or maximum change of lsafeD at one time step. By elongating the muscle length con sidering muscle tension and temperature, we can inhibit the burnout of motors. The recognition module [Figure 5(d)] is divided into object and sound recognition. In this study, we use Yolo v3 for object recognition. The labels mainly used in this study are \u201ccar,\u201d \u201cperson,\u201d and \u201ctraffic light.\u201d Also, regarding sound recog nition, the sound is converted into a mel spectrum, and the network hsound is trained to output sound class from the spec trum."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The recognition module [Figure 5(d)] is divided into object and sound recognition.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd48145d-ea98-46fa-b7cf-ceb6102456e6": {"__data__": {"id_": "dd48145d-ea98-46fa-b7cf-ceb6102456e6", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "By elongating the muscle length con sidering muscle tension and temperature, we can inhibit the burnout of motors. The recognition module [Figure 5(d)] is divided into object and sound recognition. In this study, we use Yolo v3 for object recognition. The labels mainly used in this study are \u201ccar,\u201d \u201cperson,\u201d and \u201ctraffic light.\u201d Also, regarding sound recog nition, the sound is converted into a mel spectrum, and the network hsound is trained to output sound class from the spec trum. In this study, this module recognizes whether or not the sound is a car horn."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this study, we use Yolo v3 for object recognition.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8508070-349b-4b0e-86c4-c770ec383d8c": {"__data__": {"id_": "a8508070-349b-4b0e-86c4-c770ec383d8c", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The recognition module [Figure 5(d)] is divided into object and sound recognition. In this study, we use Yolo v3 for object recognition. The labels mainly used in this study are \u201ccar,\u201d \u201cperson,\u201d and \u201ctraffic light.\u201d Also, regarding sound recog nition, the sound is converted into a mel spectrum, and the network hsound is trained to output sound class from the spec trum. In this study, this module recognizes whether or not the sound is a car horn. We present the use of each software component for autono mous driving in Figure 6 (presented in the multimedia"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The labels mainly used in this study are \u201ccar,\u201d \u201cperson,\u201d and \u201ctraffic light.\u201d Also, regarding sound recog nition, the sound is converted into a mel spectrum, and the network hsound is trained to output sound class from the spec trum.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d0e9e43-6a01-4280-a108-4f9319e13558": {"__data__": {"id_": "2d0e9e43-6a01-4280-a108-4f9319e13558", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In this study, we use Yolo v3 for object recognition. The labels mainly used in this study are \u201ccar,\u201d \u201cperson,\u201d and \u201ctraffic light.\u201d Also, regarding sound recog nition, the sound is converted into a mel spectrum, and the network hsound is trained to output sound class from the spec trum. In this study, this module recognizes whether or not the sound is a car horn. We present the use of each software component for autono mous driving in Figure 6 (presented in the multimedia The static module can be applied to various motions, and is especially useful for steering wheel operation."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this study, this module recognizes whether or not the sound is a car horn.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f8f418b-5fec-4f88-b3b9-acc21e47aab9": {"__data__": {"id_": "3f8f418b-5fec-4f88-b3b9-acc21e47aab9", "embedding": null, "metadata": {"page_number": 7, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The labels mainly used in this study are \u201ccar,\u201d \u201cperson,\u201d and \u201ctraffic light.\u201d Also, regarding sound recog nition, the sound is converted into a mel spectrum, and the network hsound is trained to output sound class from the spec trum. In this study, this module recognizes whether or not the sound is a car horn. We present the use of each software component for autono mous driving in Figure 6 (presented in the multimedia The static module can be applied to various motions, and is especially useful for steering wheel operation. In Figure 6(a), we depict an experiment for operating the steer- ing wheel during online learning of the static module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We present the use of each software component for autono mous driving in Figure 6 (presented in the multimedia", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a69db45-70de-413f-8219-02bda0026c91": {"__data__": {"id_": "3a69db45-70de-413f-8219-02bda0026c91", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In this study, this module recognizes whether or not the sound is a car horn. We present the use of each software component for autono mous driving in Figure 6 (presented in the multimedia The static module can be applied to various motions, and is especially useful for steering wheel operation. In Figure 6(a), we depict an experiment for operating the steer- ing wheel during online learning of the static module. The graphs of Figure 6(a) illustrate the transition of the steering wheel angle and 10 muscle tensions of the shoulder and elbow of Musashi."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The static module can be applied to various motions, and is especially useful for steering wheel operation.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fced9a6a-753f-47fb-a9d3-da774afd7dcf": {"__data__": {"id_": "fced9a6a-753f-47fb-a9d3-da774afd7dcf", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We present the use of each software component for autono mous driving in Figure 6 (presented in the multimedia The static module can be applied to various motions, and is especially useful for steering wheel operation. In Figure 6(a), we depict an experiment for operating the steer- ing wheel during online learning of the static module. The graphs of Figure 6(a) illustrate the transition of the steering wheel angle and 10 muscle tensions of the shoulder and elbow of Musashi. At every operation, the angle gradually increases, and muscle ten- sion gradually decreases."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Figure 6(a), we depict an experiment for operating the steer- ing wheel during online learning of the static module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0977efdf-4bef-4cc1-8610-b5736a69be2d": {"__data__": {"id_": "0977efdf-4bef-4cc1-8610-b5736a69be2d", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The static module can be applied to various motions, and is especially useful for steering wheel operation. In Figure 6(a), we depict an experiment for operating the steer- ing wheel during online learning of the static module. The graphs of Figure 6(a) illustrate the transition of the steering wheel angle and 10 muscle tensions of the shoulder and elbow of Musashi. At every operation, the angle gradually increases, and muscle ten- sion gradually decreases. Thus, the relationship among f, ,i and l is cor- rectly updated using the actual robot sensor information."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The graphs of Figure 6(a) illustrate the transition of the steering wheel angle and 10 muscle tensions of the shoulder and elbow of Musashi.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a92f43ea-c6b6-452c-bff7-faad8cf52baa": {"__data__": {"id_": "a92f43ea-c6b6-452c-bff7-faad8cf52baa", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In Figure 6(a), we depict an experiment for operating the steer- ing wheel during online learning of the static module. The graphs of Figure 6(a) illustrate the transition of the steering wheel angle and 10 muscle tensions of the shoulder and elbow of Musashi. At every operation, the angle gradually increases, and muscle ten- sion gradually decreases. Thus, the relationship among f, ,i and l is cor- rectly updated using the actual robot sensor information. The dynamic module is useful for dynamic motions."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At every operation, the angle gradually increases, and muscle ten- sion gradually decreases.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc8a93b6-fa6b-4364-8c64-723c6eff37bf": {"__data__": {"id_": "fc8a93b6-fa6b-4364-8c64-723c6eff37bf", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The graphs of Figure 6(a) illustrate the transition of the steering wheel angle and 10 muscle tensions of the shoulder and elbow of Musashi. At every operation, the angle gradually increases, and muscle ten- sion gradually decreases. Thus, the relationship among f, ,i and l is cor- rectly updated using the actual robot sensor information. The dynamic module is useful for dynamic motions. The pedal opera- tion is a good example because a fast motion adaptation is necessary."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, the relationship among f, ,i and l is cor- rectly updated using the actual robot sensor information.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9d9e244-8a6b-4829-96e0-bd2a0a09b8be": {"__data__": {"id_": "a9d9e244-8a6b-4829-96e0-bd2a0a09b8be", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "At every operation, the angle gradually increases, and muscle ten- sion gradually decreases. Thus, the relationship among f, ,i and l is cor- rectly updated using the actual robot sensor information. The dynamic module is useful for dynamic motions. The pedal opera- tion is a good example because a fast motion adaptation is necessary. In Fig- ure 6(b), we trained the dynamic mod- ule using the data of random pedal operation for one minute and conduct- ed pedal operation with the trained network."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dynamic module is useful for dynamic motions.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05289fa9-f960-49e3-815f-a3e54ebc9db4": {"__data__": {"id_": "05289fa9-f960-49e3-815f-a3e54ebc9db4", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Thus, the relationship among f, ,i and l is cor- rectly updated using the actual robot sensor information. The dynamic module is useful for dynamic motions. The pedal opera- tion is a good example because a fast motion adaptation is necessary. In Fig- ure 6(b), we trained the dynamic mod- ule using the data of random pedal operation for one minute and conduct- ed pedal operation with the trained network. This experiment was con- ducted indoors, and the rear wheels were on free rollers for safety."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The pedal opera- tion is a good example because a fast motion adaptation is necessary.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9f7e3c5-87a6-4c33-a48e-ae769ce674ca": {"__data__": {"id_": "c9f7e3c5-87a6-4c33-a48e-ae769ce674ca", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The dynamic module is useful for dynamic motions. The pedal opera- tion is a good example because a fast motion adaptation is necessary. In Fig- ure 6(b), we trained the dynamic mod- ule using the data of random pedal operation for one minute and conduct- ed pedal operation with the trained network. This experiment was con- ducted indoors, and the rear wheels were on free rollers for safety. The car velocity was obtained through the con- troller area network-USB of the car."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Fig- ure 6(b), we trained the dynamic mod- ule using the data of random pedal operation for one minute and conduct- ed pedal operation with the trained network.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6514edfc-c200-4b1d-ac1e-d30cc73173a7": {"__data__": {"id_": "6514edfc-c200-4b1d-ac1e-d30cc73173a7", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The pedal opera- tion is a good example because a fast motion adaptation is necessary. In Fig- ure 6(b), we trained the dynamic mod- ule using the data of random pedal operation for one minute and conduct- ed pedal operation with the trained network. This experiment was con- ducted indoors, and the rear wheels were on free rollers for safety. The car velocity was obtained through the con- troller area network-USB of the car. We describe the car velocity as vcar and the joint angle of the right ankle pitch as i ankle , and we set i ankle as u."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This experiment was con- ducted indoors, and the rear wheels were on free rollers for safety.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fec1c7f1-7195-4ef1-87bb-e6b8d8172aed": {"__data__": {"id_": "fec1c7f1-7195-4ef1-87bb-e6b8d8172aed", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In Fig- ure 6(b), we trained the dynamic mod- ule using the data of random pedal operation for one minute and conduct- ed pedal operation with the trained network. This experiment was con- ducted indoors, and the rear wheels were on free rollers for safety. The car velocity was obtained through the con- troller area network-USB of the car. We describe the car velocity as vcar and the joint angle of the right ankle pitch as i ankle , and we set i ankle as u. We set v ref = car 5 [ /km h ] and conducted the manually tuned proportional-integral- derivative (PID) control and the pro- posed control with the dynamic module (proposed)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The car velocity was obtained through the con- troller area network-USB of the car.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd2ec5b2-4cc4-470c-85a2-920435b04fdd": {"__data__": {"id_": "fd2ec5b2-4cc4-470c-85a2-920435b04fdd", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This experiment was con- ducted indoors, and the rear wheels were on free rollers for safety. The car velocity was obtained through the con- troller area network-USB of the car. We describe the car velocity as vcar and the joint angle of the right ankle pitch as i ankle , and we set i ankle as u. We set v ref = car 5 [ /km h ] and conducted the manually tuned proportional-integral- derivative (PID) control and the pro- posed control with the dynamic module (proposed). When comparing PID and proposed, the error between ref falls within 20% starting vcar and vcar from 22.9 s and 0.9 s, respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We describe the car velocity as vcar and the joint angle of the right ankle pitch as i ankle , and we set i ankle as u.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08b10f10-0620-4815-971f-50a1953fa89e": {"__data__": {"id_": "08b10f10-0620-4815-971f-50a1953fa89e", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The car velocity was obtained through the con- troller area network-USB of the car. We describe the car velocity as vcar and the joint angle of the right ankle pitch as i ankle , and we set i ankle as u. We set v ref = car 5 [ /km h ] and conducted the manually tuned proportional-integral- derivative (PID) control and the pro- posed control with the dynamic module (proposed). When comparing PID and proposed, the error between ref falls within 20% starting vcar and vcar from 22.9 s and 0.9 s, respectively. Since the dynamic relationship between the joint angle of ankle pitch and car veloc- ity is complex, this was the limit of PID by manual tuning, and the car velocity vibrated significantly when the gain in our experiment was further increased."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We set v ref = car 5 [ /km h ] and conducted the manually tuned proportional-integral- derivative (PID) control and the pro- posed control with the dynamic module (proposed).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d575e6af-3f05-4d85-ab94-3a5f45121fb5": {"__data__": {"id_": "d575e6af-3f05-4d85-ab94-3a5f45121fb5", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We describe the car velocity as vcar and the joint angle of the right ankle pitch as i ankle , and we set i ankle as u. We set v ref = car 5 [ /km h ] and conducted the manually tuned proportional-integral- derivative (PID) control and the pro- posed control with the dynamic module (proposed). When comparing PID and proposed, the error between ref falls within 20% starting vcar and vcar from 22.9 s and 0.9 s, respectively. Since the dynamic relationship between the joint angle of ankle pitch and car veloc- ity is complex, this was the limit of PID by manual tuning, and the car velocity vibrated significantly when the gain in our experiment was further increased. Thus, by acquiring the state equation between task state and control com- mand, fast tracking is enabled."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When comparing PID and proposed, the error between ref falls within 20% starting vcar and vcar from 22.9 s and 0.9 s, respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f22f6c27-5c78-4b72-b53d-7d724480c47f": {"__data__": {"id_": "f22f6c27-5c78-4b72-b53d-7d724480c47f", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We set v ref = car 5 [ /km h ] and conducted the manually tuned proportional-integral- derivative (PID) control and the pro- posed control with the dynamic module (proposed). When comparing PID and proposed, the error between ref falls within 20% starting vcar and vcar from 22.9 s and 0.9 s, respectively. Since the dynamic relationship between the joint angle of ankle pitch and car veloc- ity is complex, this was the limit of PID by manual tuning, and the car velocity vibrated significantly when the gain in our experiment was further increased. Thus, by acquiring the state equation between task state and control com- mand, fast tracking is enabled. as shown in Figure 6(c), during the steering wheel operation, the robot does not always operate the wheel."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the dynamic relationship between the joint angle of ankle pitch and car veloc- ity is complex, this was the limit of PID by manual tuning, and the car velocity vibrated significantly when the gain in our experiment was further increased.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "633704d6-437e-45f3-b5f9-e09cb1ba9e84": {"__data__": {"id_": "633704d6-437e-45f3-b5f9-e09cb1ba9e84", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "When comparing PID and proposed, the error between ref falls within 20% starting vcar and vcar from 22.9 s and 0.9 s, respectively. Since the dynamic relationship between the joint angle of ankle pitch and car veloc- ity is complex, this was the limit of PID by manual tuning, and the car velocity vibrated significantly when the gain in our experiment was further increased. Thus, by acquiring the state equation between task state and control com- mand, fast tracking is enabled. as shown in Figure 6(c), during the steering wheel operation, the robot does not always operate the wheel. In this case, MRC gradually makes antagonist muscles elongate and internal force"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, by acquiring the state equation between task state and control com- mand, fast tracking is enabled.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6880fb2-e1d5-4e68-87b9-5706a74c3176": {"__data__": {"id_": "a6880fb2-e1d5-4e68-87b9-5706a74c3176", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Since the dynamic relationship between the joint angle of ankle pitch and car veloc- ity is complex, this was the limit of PID by manual tuning, and the car velocity vibrated significantly when the gain in our experiment was further increased. Thus, by acquiring the state equation between task state and control com- mand, fast tracking is enabled. as shown in Figure 6(c), during the steering wheel operation, the robot does not always operate the wheel. In this case, MRC gradually makes antagonist muscles elongate and internal force the current joint angle does not change signifi- cantly even when agonist muscles are elongated."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "as shown in Figure 6(c), during the steering wheel operation, the robot does not always operate the wheel.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e076ad79-38b2-4ef3-a856-18ca0b811df3": {"__data__": {"id_": "e076ad79-38b2-4ef3-a856-18ca0b811df3", "embedding": null, "metadata": {"page_number": 8, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Thus, by acquiring the state equation between task state and control com- mand, fast tracking is enabled. as shown in Figure 6(c), during the steering wheel operation, the robot does not always operate the wheel. In this case, MRC gradually makes antagonist muscles elongate and internal force the current joint angle does not change signifi- cantly even when agonist muscles are elongated. We demon- strate the transition of L2 norm of 10 muscle tensions of the elbow and shoulder in the left arm of Musashi f< < 2, with and without MRC."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, MRC gradually makes antagonist muscles elongate and internal force", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a25170d-ef62-45aa-aea2-e257ce4ce09a": {"__data__": {"id_": "0a25170d-ef62-45aa-aea2-e257ce4ce09a", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "as shown in Figure 6(c), during the steering wheel operation, the robot does not always operate the wheel. In this case, MRC gradually makes antagonist muscles elongate and internal force the current joint angle does not change signifi- cantly even when agonist muscles are elongated. We demon- strate the transition of L2 norm of 10 muscle tensions of the elbow and shoulder in the left arm of Musashi f< < 2, with and without MRC. The muscle tension is reduced in a static state with MRC compared to without MRC."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the current joint angle does not change signifi- cantly even when agonist muscles are elongated.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae197d99-74b8-413a-8959-18bd9b3213c0": {"__data__": {"id_": "ae197d99-74b8-413a-8959-18bd9b3213c0", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In this case, MRC gradually makes antagonist muscles elongate and internal force the current joint angle does not change signifi- cantly even when agonist muscles are elongated. We demon- strate the transition of L2 norm of 10 muscle tensions of the elbow and shoulder in the left arm of Musashi f< < 2, with and without MRC. The muscle tension is reduced in a static state with MRC compared to without MRC. This module can inhibit the increase of muscle temperature, and long-time operation is enabled."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We demon- strate the transition of L2 norm of 10 muscle tensions of the elbow and shoulder in the left arm of Musashi f< < 2, with and without MRC.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "811e9aa9-9c15-445e-a2f3-4657cbdd0250": {"__data__": {"id_": "811e9aa9-9c15-445e-a2f3-4657cbdd0250", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "the current joint angle does not change signifi- cantly even when agonist muscles are elongated. We demon- strate the transition of L2 norm of 10 muscle tensions of the elbow and shoulder in the left arm of Musashi f< < 2, with and without MRC. The muscle tension is reduced in a static state with MRC compared to without MRC. This module can inhibit the increase of muscle temperature, and long-time operation is enabled. The recognition module is mainly used as visual recogni- tion of \u201cperson\u201d and \u201ctraffic light,\u201d and sound recognition of the car horn."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The muscle tension is reduced in a static state with MRC compared to without MRC.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0e6b78e-6b34-4d09-b100-76909b2c81b6": {"__data__": {"id_": "c0e6b78e-6b34-4d09-b100-76909b2c81b6", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We demon- strate the transition of L2 norm of 10 muscle tensions of the elbow and shoulder in the left arm of Musashi f< < 2, with and without MRC. The muscle tension is reduced in a static state with MRC compared to without MRC. This module can inhibit the increase of muscle temperature, and long-time operation is enabled. The recognition module is mainly used as visual recogni- tion of \u201cperson\u201d and \u201ctraffic light,\u201d and sound recognition of the car horn. The left figure of Figure 6(d) depicts the recog- nition result at a crossing, and the traffic lights and a human are recognized well."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This module can inhibit the increase of muscle temperature, and long-time operation is enabled.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc150f0a-a89b-4a87-a75f-4f0800af7da3": {"__data__": {"id_": "dc150f0a-a89b-4a87-a75f-4f0800af7da3", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The muscle tension is reduced in a static state with MRC compared to without MRC. This module can inhibit the increase of muscle temperature, and long-time operation is enabled. The recognition module is mainly used as visual recogni- tion of \u201cperson\u201d and \u201ctraffic light,\u201d and sound recognition of the car horn. The left figure of Figure 6(d) depicts the recog- nition result at a crossing, and the traffic lights and a human are recognized well. Also, the right-hand side of Figure 6(d) displays the sound spectrum of a car horn, and the car horn is recognized well."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The recognition module is mainly used as visual recogni- tion of \u201cperson\u201d and \u201ctraffic light,\u201d and sound recognition of the car horn.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d2f0545-18e0-48b6-8df7-77113a884184": {"__data__": {"id_": "7d2f0545-18e0-48b6-8df7-77113a884184", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This module can inhibit the increase of muscle temperature, and long-time operation is enabled. The recognition module is mainly used as visual recogni- tion of \u201cperson\u201d and \u201ctraffic light,\u201d and sound recognition of the car horn. The left figure of Figure 6(d) depicts the recog- nition result at a crossing, and the traffic lights and a human are recognized well. Also, the right-hand side of Figure 6(d) displays the sound spectrum of a car horn, and the car horn is recognized well. The car used in this study is a B.COM Delivery of the extremely small electric vehicle COMS (Chotto Odekake Machimade Suisui) series."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left figure of Figure 6(d) depicts the recog- nition result at a crossing, and the traffic lights and a human are recognized well.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ef82538-c30b-4d39-9091-97d3e98465f4": {"__data__": {"id_": "9ef82538-c30b-4d39-9091-97d3e98465f4", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The recognition module is mainly used as visual recogni- tion of \u201cperson\u201d and \u201ctraffic light,\u201d and sound recognition of the car horn. The left figure of Figure 6(d) depicts the recog- nition result at a crossing, and the traffic lights and a human are recognized well. Also, the right-hand side of Figure 6(d) displays the sound spectrum of a car horn, and the car horn is recognized well. The car used in this study is a B.COM Delivery of the extremely small electric vehicle COMS (Chotto Odekake Machimade Suisui) series. For safety, its motor torque is limit- ed to 5 Nm, and an emergency stop button is equipped."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, the right-hand side of Figure 6(d) displays the sound spectrum of a car horn, and the car horn is recognized well.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ace16d59-b77a-4ca5-b5c0-0c85567c4e1b": {"__data__": {"id_": "ace16d59-b77a-4ca5-b5c0-0c85567c4e1b", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The left figure of Figure 6(d) depicts the recog- nition result at a crossing, and the traffic lights and a human are recognized well. Also, the right-hand side of Figure 6(d) displays the sound spectrum of a car horn, and the car horn is recognized well. The car used in this study is a B.COM Delivery of the extremely small electric vehicle COMS (Chotto Odekake Machimade Suisui) series. For safety, its motor torque is limit- ed to 5 Nm, and an emergency stop button is equipped. In the pedal operation, as indicated in Figure 7(a), the acceleration and brake pedals are operated by the right and left foot, respectively."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The car used in this study is a B.COM Delivery of the extremely small electric vehicle COMS (Chotto Odekake Machimade Suisui) series.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9667236-7e54-48cc-b16f-3261498f2313": {"__data__": {"id_": "a9667236-7e54-48cc-b16f-3261498f2313", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, the right-hand side of Figure 6(d) displays the sound spectrum of a car horn, and the car horn is recognized well. The car used in this study is a B.COM Delivery of the extremely small electric vehicle COMS (Chotto Odekake Machimade Suisui) series. For safety, its motor torque is limit- ed to 5 Nm, and an emergency stop button is equipped. In the pedal operation, as indicated in Figure 7(a), the acceleration and brake pedals are operated by the right and left foot, respectively. The experiment was conducted on the Kashiwa Campus at the University of Tokyo, as presented in Fig- ure 7(b)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For safety, its motor torque is limit- ed to 5 Nm, and an emergency stop button is equipped.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "850f5125-5527-4c5d-ad08-897d86eb4414": {"__data__": {"id_": "850f5125-5527-4c5d-ad08-897d86eb4414", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The car used in this study is a B.COM Delivery of the extremely small electric vehicle COMS (Chotto Odekake Machimade Suisui) series. For safety, its motor torque is limit- ed to 5 Nm, and an emergency stop button is equipped. In the pedal operation, as indicated in Figure 7(a), the acceleration and brake pedals are operated by the right and left foot, respectively. The experiment was conducted on the Kashiwa Campus at the University of Tokyo, as presented in Fig- ure 7(b). As shown in Figure 7(c), the COMS is equipped with a battery, logic power supply, servo power supply, Wi-Fi router, and PC for the recognition module in the trunk."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the pedal operation, as indicated in Figure 7(a), the acceleration and brake pedals are operated by the right and left foot, respectively.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b4e2fd1-1c43-48b0-bb1e-c4a6e2f5619e": {"__data__": {"id_": "5b4e2fd1-1c43-48b0-bb1e-c4a6e2f5619e", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "For safety, its motor torque is limit- ed to 5 Nm, and an emergency stop button is equipped. In the pedal operation, as indicated in Figure 7(a), the acceleration and brake pedals are operated by the right and left foot, respectively. The experiment was conducted on the Kashiwa Campus at the University of Tokyo, as presented in Fig- ure 7(b). As shown in Figure 7(c), the COMS is equipped with a battery, logic power supply, servo power supply, Wi-Fi router, and PC for the recognition module in the trunk. The COMS is an electric vehicle, and all of the electric power for the robot can be obtained from COMS models in the future."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experiment was conducted on the Kashiwa Campus at the University of Tokyo, as presented in Fig- ure 7(b).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d94aec2d-1086-41e5-b6ad-3b397204e236": {"__data__": {"id_": "d94aec2d-1086-41e5-b6ad-3b397204e236", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In the pedal operation, as indicated in Figure 7(a), the acceleration and brake pedals are operated by the right and left foot, respectively. The experiment was conducted on the Kashiwa Campus at the University of Tokyo, as presented in Fig- ure 7(b). As shown in Figure 7(c), the COMS is equipped with a battery, logic power supply, servo power supply, Wi-Fi router, and PC for the recognition module in the trunk. The COMS is an electric vehicle, and all of the electric power for the robot can be obtained from COMS models in the future. The experimental setup of autonomous driving by the musculoskeletal humanoid Musashi: the (a) pedal operation configuration, (b) experimental environment, (c) experimental configuration in the COMS , and (d) configuration of the PCs."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As shown in Figure 7(c), the COMS is equipped with a battery, logic power supply, servo power supply, Wi-Fi router, and PC for the recognition module in the trunk.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11dde415-511f-4ba3-94a4-f7e9c51ffb3c": {"__data__": {"id_": "11dde415-511f-4ba3-94a4-f7e9c51ffb3c", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The experiment was conducted on the Kashiwa Campus at the University of Tokyo, as presented in Fig- ure 7(b). As shown in Figure 7(c), the COMS is equipped with a battery, logic power supply, servo power supply, Wi-Fi router, and PC for the recognition module in the trunk. The COMS is an electric vehicle, and all of the electric power for the robot can be obtained from COMS models in the future. The experimental setup of autonomous driving by the musculoskeletal humanoid Musashi: the (a) pedal operation configuration, (b) experimental environment, (c) experimental configuration in the COMS , and (d) configuration of the PCs. As depicted in Figure 7(d), PCs (Intel NUC, Intel, Inc.) for motor control and image/sound are in the head of Musashi; the recognition is executed on the PC (ZOTAC VR GO, ZOTAC, Inc.) in the COMS, and the other processes are exe- cuted on PC4 outside."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The COMS is an electric vehicle, and all of the electric power for the robot can be obtained from COMS models in the future.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79788e03-56d9-480c-8696-e6c3886e235f": {"__data__": {"id_": "79788e03-56d9-480c-8696-e6c3886e235f", "embedding": null, "metadata": {"page_number": 9, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "As shown in Figure 7(c), the COMS is equipped with a battery, logic power supply, servo power supply, Wi-Fi router, and PC for the recognition module in the trunk. The COMS is an electric vehicle, and all of the electric power for the robot can be obtained from COMS models in the future. The experimental setup of autonomous driving by the musculoskeletal humanoid Musashi: the (a) pedal operation configuration, (b) experimental environment, (c) experimental configuration in the COMS , and (d) configuration of the PCs. As depicted in Figure 7(d), PCs (Intel NUC, Intel, Inc.) for motor control and image/sound are in the head of Musashi; the recognition is executed on the PC (ZOTAC VR GO, ZOTAC, Inc.) in the COMS, and the other processes are exe- cuted on PC4 outside. We conducted an experiment integrating the pedal operation and recognition (Figure 8; presented in the multimedia mate- rial)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experimental setup of autonomous driving by the musculoskeletal humanoid Musashi: the (a) pedal operation configuration, (b) experimental environment, (c) experimental configuration in the COMS , and (d) configuration of the PCs.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f533b87-a7b5-4299-b0bf-79f0d252cc3d": {"__data__": {"id_": "1f533b87-a7b5-4299-b0bf-79f0d252cc3d", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The COMS is an electric vehicle, and all of the electric power for the robot can be obtained from COMS models in the future. The experimental setup of autonomous driving by the musculoskeletal humanoid Musashi: the (a) pedal operation configuration, (b) experimental environment, (c) experimental configuration in the COMS , and (d) configuration of the PCs. As depicted in Figure 7(d), PCs (Intel NUC, Intel, Inc.) for motor control and image/sound are in the head of Musashi; the recognition is executed on the PC (ZOTAC VR GO, ZOTAC, Inc.) in the COMS, and the other processes are exe- cuted on PC4 outside. We conducted an experiment integrating the pedal operation and recognition (Figure 8; presented in the multimedia mate- rial). We display the experimental motion flow in Figure 8(a)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As depicted in Figure 7(d), PCs (Intel NUC, Intel, Inc.) for motor control and image/sound are in the head of Musashi; the recognition is executed on the PC (ZOTAC VR GO, ZOTAC, Inc.) in the COMS, and the other processes are exe- cuted on PC4 outside.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30c7b2b8-a6bb-42d6-8fcb-94947d308de2": {"__data__": {"id_": "30c7b2b8-a6bb-42d6-8fcb-94947d308de2", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The experimental setup of autonomous driving by the musculoskeletal humanoid Musashi: the (a) pedal operation configuration, (b) experimental environment, (c) experimental configuration in the COMS , and (d) configuration of the PCs. As depicted in Figure 7(d), PCs (Intel NUC, Intel, Inc.) for motor control and image/sound are in the head of Musashi; the recognition is executed on the PC (ZOTAC VR GO, ZOTAC, Inc.) in the COMS, and the other processes are exe- cuted on PC4 outside. We conducted an experiment integrating the pedal operation and recognition (Figure 8; presented in the multimedia mate- rial). We display the experimental motion flow in Figure 8(a). First, the robot operates the acceleration pedal using the trained dynamic module, steps on the brake pedal if it recog- nizes a human, restarts the acceleration pedal operation, and steps on the brake pedal again if it recognizes a car horn."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We conducted an experiment integrating the pedal operation and recognition (Figure 8; presented in the multimedia mate- rial).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "395236c4-7b9f-4318-ab2d-7e307f23677b": {"__data__": {"id_": "395236c4-7b9f-4318-ab2d-7e307f23677b", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "As depicted in Figure 7(d), PCs (Intel NUC, Intel, Inc.) for motor control and image/sound are in the head of Musashi; the recognition is executed on the PC (ZOTAC VR GO, ZOTAC, Inc.) in the COMS, and the other processes are exe- cuted on PC4 outside. We conducted an experiment integrating the pedal operation and recognition (Figure 8; presented in the multimedia mate- rial). We display the experimental motion flow in Figure 8(a). First, the robot operates the acceleration pedal using the trained dynamic module, steps on the brake pedal if it recog- nizes a human, restarts the acceleration pedal operation, and steps on the brake pedal again if it recognizes a car horn. The motion sequence is depicted in Figure 8(b), and we can see that it succeeded."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We display the experimental motion flow in Figure 8(a).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3537e75f-aaaa-4258-b0a8-42fde60f8f37": {"__data__": {"id_": "3537e75f-aaaa-4258-b0a8-42fde60f8f37", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We conducted an experiment integrating the pedal operation and recognition (Figure 8; presented in the multimedia mate- rial). We display the experimental motion flow in Figure 8(a). First, the robot operates the acceleration pedal using the trained dynamic module, steps on the brake pedal if it recog- nizes a human, restarts the acceleration pedal operation, and steps on the brake pedal again if it recognizes a car horn. The motion sequence is depicted in Figure 8(b), and we can see that it succeeded. The transition of car velocity (upper graph) and joint angle of the right ankle pitch (lower graph) is pre- sented in Figure 8(c), while Figure 8(d) demonstrates the rec- ognition result of a human."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, the robot operates the acceleration pedal using the trained dynamic module, steps on the brake pedal if it recog- nizes a human, restarts the acceleration pedal operation, and steps on the brake pedal again if it recognizes a car horn.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a388d8bd-95ea-4dc2-a210-ad79f0894221": {"__data__": {"id_": "a388d8bd-95ea-4dc2-a210-ad79f0894221", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We display the experimental motion flow in Figure 8(a). First, the robot operates the acceleration pedal using the trained dynamic module, steps on the brake pedal if it recog- nizes a human, restarts the acceleration pedal operation, and steps on the brake pedal again if it recognizes a car horn. The motion sequence is depicted in Figure 8(b), and we can see that it succeeded. The transition of car velocity (upper graph) and joint angle of the right ankle pitch (lower graph) is pre- sented in Figure 8(c), while Figure 8(d) demonstrates the rec- ognition result of a human. The robot recognizes a human when its bounding box is larger than a certain threshold and its center coordinate is around the center of the image."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The motion sequence is depicted in Figure 8(b), and we can see that it succeeded.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cce7fc22-bdab-4322-b73f-d458035d6367": {"__data__": {"id_": "cce7fc22-bdab-4322-b73f-d458035d6367", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "First, the robot operates the acceleration pedal using the trained dynamic module, steps on the brake pedal if it recog- nizes a human, restarts the acceleration pedal operation, and steps on the brake pedal again if it recognizes a car horn. The motion sequence is depicted in Figure 8(b), and we can see that it succeeded. The transition of car velocity (upper graph) and joint angle of the right ankle pitch (lower graph) is pre- sented in Figure 8(c), while Figure 8(d) demonstrates the rec- ognition result of a human. The robot recognizes a human when its bounding box is larger than a certain threshold and its center coordinate is around the center of the image. When a human [Figure 8(c)] or car horn [Figure 8(d)] was detected, the brake pedal was stepped on, and we can see that the car"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The transition of car velocity (upper graph) and joint angle of the right ankle pitch (lower graph) is pre- sented in Figure 8(c), while Figure 8(d) demonstrates the rec- ognition result of a human.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebfcebbf-2884-4e0c-b8a9-ad96361062b1": {"__data__": {"id_": "ebfcebbf-2884-4e0c-b8a9-ad96361062b1", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The motion sequence is depicted in Figure 8(b), and we can see that it succeeded. The transition of car velocity (upper graph) and joint angle of the right ankle pitch (lower graph) is pre- sented in Figure 8(c), while Figure 8(d) demonstrates the rec- ognition result of a human. The robot recognizes a human when its bounding box is larger than a certain threshold and its center coordinate is around the center of the image. When a human [Figure 8(c)] or car horn [Figure 8(d)] was detected, the brake pedal was stepped on, and we can see that the car The big problem here is the poor tracking of the target car velocity as illustrated in Figure 8(c)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot recognizes a human when its bounding box is larger than a certain threshold and its center coordinate is around the center of the image.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65462fca-f4d4-4325-bc55-cf80555e8360": {"__data__": {"id_": "65462fca-f4d4-4325-bc55-cf80555e8360", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The transition of car velocity (upper graph) and joint angle of the right ankle pitch (lower graph) is pre- sented in Figure 8(c), while Figure 8(d) demonstrates the rec- ognition result of a human. The robot recognizes a human when its bounding box is larger than a certain threshold and its center coordinate is around the center of the image. When a human [Figure 8(c)] or car horn [Figure 8(d)] was detected, the brake pedal was stepped on, and we can see that the car The big problem here is the poor tracking of the target car velocity as illustrated in Figure 8(c). This is because of the difference between the environment that the network is trained in and the experimental environment."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When a human [Figure 8(c)] or car horn [Figure 8(d)] was detected, the brake pedal was stepped on, and we can see that the car", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a57bc87a-f5b3-45b1-bd91-dc79dafbd6bb": {"__data__": {"id_": "a57bc87a-f5b3-45b1-bd91-dc79dafbd6bb", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The robot recognizes a human when its bounding box is larger than a certain threshold and its center coordinate is around the center of the image. When a human [Figure 8(c)] or car horn [Figure 8(d)] was detected, the brake pedal was stepped on, and we can see that the car The big problem here is the poor tracking of the target car velocity as illustrated in Figure 8(c). This is because of the difference between the environment that the network is trained in and the experimental environment. When t = [ , 0 60 ] [sec], the car velocity decreased because the road friction was high, and when t = [ 90 110 , ] [sec], the car velocity increased because the road was downhill."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The big problem here is the poor tracking of the target car velocity as illustrated in Figure 8(c).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20c20c23-2d45-4a00-8fe3-eab25e652b14": {"__data__": {"id_": "20c20c23-2d45-4a00-8fe3-eab25e652b14", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "When a human [Figure 8(c)] or car horn [Figure 8(d)] was detected, the brake pedal was stepped on, and we can see that the car The big problem here is the poor tracking of the target car velocity as illustrated in Figure 8(c). This is because of the difference between the environment that the network is trained in and the experimental environment. When t = [ , 0 60 ] [sec], the car velocity decreased because the road friction was high, and when t = [ 90 110 , ] [sec], the car velocity increased because the road was downhill. This was not a problem in the experiment on the free roller, but the differ- ence of the actual environment was difficult to handle."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is because of the difference between the environment that the network is trained in and the experimental environment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1eaaa90-3e4d-4b55-ac8b-d680014ec418": {"__data__": {"id_": "f1eaaa90-3e4d-4b55-ac8b-d680014ec418", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The big problem here is the poor tracking of the target car velocity as illustrated in Figure 8(c). This is because of the difference between the environment that the network is trained in and the experimental environment. When t = [ , 0 60 ] [sec], the car velocity decreased because the road friction was high, and when t = [ 90 110 , ] [sec], the car velocity increased because the road was downhill. This was not a problem in the experiment on the free roller, but the differ- ence of the actual environment was difficult to handle. Because the learning-based controls depend on the data used for train- ing, we must continue to conduct experiments outside, obtain data, and solve this problem."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When t = [ , 0 60 ] [sec], the car velocity decreased because the road friction was high, and when t = [ 90 110 , ] [sec], the car velocity increased because the road was downhill.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ae2ad54-7d2f-40e6-8a36-87915760edc8": {"__data__": {"id_": "9ae2ad54-7d2f-40e6-8a36-87915760edc8", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This is because of the difference between the environment that the network is trained in and the experimental environment. When t = [ , 0 60 ] [sec], the car velocity decreased because the road friction was high, and when t = [ 90 110 , ] [sec], the car velocity increased because the road was downhill. This was not a problem in the experiment on the free roller, but the differ- ence of the actual environment was difficult to handle. Because the learning-based controls depend on the data used for train- ing, we must continue to conduct experiments outside, obtain data, and solve this problem. We conducted the steering wheel operation with recognition (Figure 9; presented in multimedia material)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This was not a problem in the experiment on the free roller, but the differ- ence of the actual environment was difficult to handle.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74fa7e51-f00d-407b-a747-fd00e4275539": {"__data__": {"id_": "74fa7e51-f00d-407b-a747-fd00e4275539", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "When t = [ , 0 60 ] [sec], the car velocity decreased because the road friction was high, and when t = [ 90 110 , ] [sec], the car velocity increased because the road was downhill. This was not a problem in the experiment on the free roller, but the differ- ence of the actual environment was difficult to handle. Because the learning-based controls depend on the data used for train- ing, we must continue to conduct experiments outside, obtain data, and solve this problem. We conducted the steering wheel operation with recognition (Figure 9; presented in multimedia material). We demonstrate the motion flow in Figure 9(a)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because the learning-based controls depend on the data used for train- ing, we must continue to conduct experiments outside, obtain data, and solve this problem.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf8287da-c587-4944-95e8-521b99170af5": {"__data__": {"id_": "cf8287da-c587-4944-95e8-521b99170af5", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "This was not a problem in the experiment on the free roller, but the differ- ence of the actual environment was difficult to handle. Because the learning-based controls depend on the data used for train- ing, we must continue to conduct experiments outside, obtain data, and solve this problem. We conducted the steering wheel operation with recognition (Figure 9; presented in multimedia material). We demonstrate the motion flow in Figure 9(a). The robot steps on the brake pedal when the traffic light is red, releases the brake pedal when the traffic light turns blue, and turns right at the cross- ing by the steering wheel operation with the static module."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We conducted the steering wheel operation with recognition (Figure 9; presented in multimedia material).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41b95b6d-9cf0-46ee-b991-b3410fab7c51": {"__data__": {"id_": "41b95b6d-9cf0-46ee-b991-b3410fab7c51", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Because the learning-based controls depend on the data used for train- ing, we must continue to conduct experiments outside, obtain data, and solve this problem. We conducted the steering wheel operation with recognition (Figure 9; presented in multimedia material). We demonstrate the motion flow in Figure 9(a). The robot steps on the brake pedal when the traffic light is red, releases the brake pedal when the traffic light turns blue, and turns right at the cross- ing by the steering wheel operation with the static module. In this experiment, because turning at a curve is difficult when the car velocity is fast, we use creep velocity just by releasing the brake pedal."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We demonstrate the motion flow in Figure 9(a).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f08adc1-4d10-414e-979e-36d2da82ef72": {"__data__": {"id_": "3f08adc1-4d10-414e-979e-36d2da82ef72", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We conducted the steering wheel operation with recognition (Figure 9; presented in multimedia material). We demonstrate the motion flow in Figure 9(a). The robot steps on the brake pedal when the traffic light is red, releases the brake pedal when the traffic light turns blue, and turns right at the cross- ing by the steering wheel operation with the static module. In this experiment, because turning at a curve is difficult when the car velocity is fast, we use creep velocity just by releasing the brake pedal. Also, the steering wheel is turned to the right first and then is turned to the left by a human command."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot steps on the brake pedal when the traffic light is red, releases the brake pedal when the traffic light turns blue, and turns right at the cross- ing by the steering wheel operation with the static module.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3f12a07-6fbf-4472-8988-99fc57900e40": {"__data__": {"id_": "c3f12a07-6fbf-4472-8988-99fc57900e40", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We demonstrate the motion flow in Figure 9(a). The robot steps on the brake pedal when the traffic light is red, releases the brake pedal when the traffic light turns blue, and turns right at the cross- ing by the steering wheel operation with the static module. In this experiment, because turning at a curve is difficult when the car velocity is fast, we use creep velocity just by releasing the brake pedal. Also, the steering wheel is turned to the right first and then is turned to the left by a human command. and we must make the motions faster and smoother."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this experiment, because turning at a curve is difficult when the car velocity is fast, we use creep velocity just by releasing the brake pedal.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b139b657-d47b-400f-a386-bab5d78c5915": {"__data__": {"id_": "b139b657-d47b-400f-a386-bab5d78c5915", "embedding": null, "metadata": {"page_number": 10, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The robot steps on the brake pedal when the traffic light is red, releases the brake pedal when the traffic light turns blue, and turns right at the cross- ing by the steering wheel operation with the static module. In this experiment, because turning at a curve is difficult when the car velocity is fast, we use creep velocity just by releasing the brake pedal. Also, the steering wheel is turned to the right first and then is turned to the left by a human command. and we must make the motions faster and smoother. we succeeded in the steering wheel operation with both arms and human recognition in the side mirror."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, the steering wheel is turned to the right first and then is turned to the left by a human command.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55591b36-c407-4e38-901c-132fe10ee533": {"__data__": {"id_": "55591b36-c407-4e38-901c-132fe10ee533", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In this experiment, because turning at a curve is difficult when the car velocity is fast, we use creep velocity just by releasing the brake pedal. Also, the steering wheel is turned to the right first and then is turned to the left by a human command. and we must make the motions faster and smoother. we succeeded in the steering wheel operation with both arms and human recognition in the side mirror. wheel operation is dis played in Figure 9(c)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and we must make the motions faster and smoother.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f48cdfc2-da08-4fa1-a27e-68e6f8cf7467": {"__data__": {"id_": "f48cdfc2-da08-4fa1-a27e-68e6f8cf7467", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, the steering wheel is turned to the right first and then is turned to the left by a human command. and we must make the motions faster and smoother. we succeeded in the steering wheel operation with both arms and human recognition in the side mirror. wheel operation is dis played in Figure 9(c). The sequence is rotating the steering wheel by both arms as much as possible, releasing the left hand, returning the left hand to the original position, releasing the right hand, returning the right hand to the original position, and rotating again."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we succeeded in the steering wheel operation with both arms and human recognition in the side mirror.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0e920b9-ac05-46de-bc13-50d31e5ec672": {"__data__": {"id_": "c0e920b9-ac05-46de-bc13-50d31e5ec672", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "and we must make the motions faster and smoother. we succeeded in the steering wheel operation with both arms and human recognition in the side mirror. wheel operation is dis played in Figure 9(c). The sequence is rotating the steering wheel by both arms as much as possible, releasing the left hand, returning the left hand to the original position, releasing the right hand, returning the right hand to the original position, and rotating again. The transi tion of the steering angle is presented in Figure 9(d)."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "wheel operation is dis played in Figure 9(c).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "286b4c6f-2595-483a-8b86-08949afd5ea7": {"__data__": {"id_": "286b4c6f-2595-483a-8b86-08949afd5ea7", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we succeeded in the steering wheel operation with both arms and human recognition in the side mirror. wheel operation is dis played in Figure 9(c). The sequence is rotating the steering wheel by both arms as much as possible, releasing the left hand, returning the left hand to the original position, releasing the right hand, returning the right hand to the original position, and rotating again. The transi tion of the steering angle is presented in Figure 9(d). The robot could turn the steering wheel by about 180\u00b0 in 70 s. Figure 9(e) shows the recognition result of the traffic light."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sequence is rotating the steering wheel by both arms as much as possible, releasing the left hand, returning the left hand to the original position, releasing the right hand, returning the right hand to the original position, and rotating again.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54389c3f-930e-419a-8a77-7ade862972d2": {"__data__": {"id_": "54389c3f-930e-419a-8a77-7ade862972d2", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "wheel operation is dis played in Figure 9(c). The sequence is rotating the steering wheel by both arms as much as possible, releasing the left hand, returning the left hand to the original position, releasing the right hand, returning the right hand to the original position, and rotating again. The transi tion of the steering angle is presented in Figure 9(d). The robot could turn the steering wheel by about 180\u00b0 in 70 s. Figure 9(e) shows the recognition result of the traffic light. as shown in Figure 9(f), the left ankle pitch joint moved to release the brake pedal, and the car began to move."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The transi tion of the steering angle is presented in Figure 9(d).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f015b22-f635-4edc-95c4-c795b88d9191": {"__data__": {"id_": "0f015b22-f635-4edc-95c4-c795b88d9191", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The sequence is rotating the steering wheel by both arms as much as possible, releasing the left hand, returning the left hand to the original position, releasing the right hand, returning the right hand to the original position, and rotating again. The transi tion of the steering angle is presented in Figure 9(d). The robot could turn the steering wheel by about 180\u00b0 in 70 s. Figure 9(e) shows the recognition result of the traffic light. as shown in Figure 9(f), the left ankle pitch joint moved to release the brake pedal, and the car began to move. The prob lem of this experiment is the slowness of the steering wheel"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The robot could turn the steering wheel by about 180\u00b0 in 70 s. Figure 9(e) shows the recognition result of the traffic light.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca1d83ce-5662-4569-a8d7-d422951de7c1": {"__data__": {"id_": "ca1d83ce-5662-4569-a8d7-d422951de7c1", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The transi tion of the steering angle is presented in Figure 9(d). The robot could turn the steering wheel by about 180\u00b0 in 70 s. Figure 9(e) shows the recognition result of the traffic light. as shown in Figure 9(f), the left ankle pitch joint moved to release the brake pedal, and the car began to move. The prob lem of this experiment is the slowness of the steering wheel the road is not smooth; the ground rises at a crossing, and the road is some times uphill or downhill."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "as shown in Figure 9(f), the left ankle pitch joint moved to release the brake pedal, and the car began to move.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "266b066b-d0c3-4118-a36c-75aac5beeec1": {"__data__": {"id_": "266b066b-d0c3-4118-a36c-75aac5beeec1", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The robot could turn the steering wheel by about 180\u00b0 in 70 s. Figure 9(e) shows the recognition result of the traffic light. as shown in Figure 9(f), the left ankle pitch joint moved to release the brake pedal, and the car began to move. The prob lem of this experiment is the slowness of the steering wheel the road is not smooth; the ground rises at a crossing, and the road is some times uphill or downhill. In those cases, the state equation trained at a flat road is different from that of the actual envi ronment, and the robot cannot adjust the car velocity well."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The prob lem of this experiment is the slowness of the steering wheel", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "845db20d-9c24-4acb-91fd-ea0d0a301266": {"__data__": {"id_": "845db20d-9c24-4acb-91fd-ea0d0a301266", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "as shown in Figure 9(f), the left ankle pitch joint moved to release the brake pedal, and the car began to move. The prob lem of this experiment is the slowness of the steering wheel the road is not smooth; the ground rises at a crossing, and the road is some times uphill or downhill. In those cases, the state equation trained at a flat road is different from that of the actual envi ronment, and the robot cannot adjust the car velocity well. To solve this problem, we need to conduct online learning or add the image of road condition and IMU information in the staskl However, online learning body to the initial task state ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the road is not smooth; the ground rises at a crossing, and the road is some times uphill or downhill.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a3987c2-5d30-40d0-8169-c7a24d729c13": {"__data__": {"id_": "5a3987c2-5d30-40d0-8169-c7a24d729c13", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "The prob lem of this experiment is the slowness of the steering wheel the road is not smooth; the ground rises at a crossing, and the road is some times uphill or downhill. In those cases, the state equation trained at a flat road is different from that of the actual envi ronment, and the robot cannot adjust the car velocity well. To solve this problem, we need to conduct online learning or add the image of road condition and IMU information in the staskl However, online learning body to the initial task state . becomes difficult with additional network input, and an effi cient learning system with only a few data is desired."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In those cases, the state equation trained at a flat road is different from that of the actual envi ronment, and the robot cannot adjust the car velocity well.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0048af3b-0665-4f3f-bd98-2eca311c2b3f": {"__data__": {"id_": "0048af3b-0665-4f3f-bd98-2eca311c2b3f", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "the road is not smooth; the ground rises at a crossing, and the road is some times uphill or downhill. In those cases, the state equation trained at a flat road is different from that of the actual envi ronment, and the robot cannot adjust the car velocity well. To solve this problem, we need to conduct online learning or add the image of road condition and IMU information in the staskl However, online learning body to the initial task state . becomes difficult with additional network input, and an effi cient learning system with only a few data is desired. the robot needs to acquire how to smooth ly step on the brake pedal as the car velocity becomes fast."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To solve this problem, we need to conduct online learning or add the image of road condition and IMU information in the staskl However, online learning body to the initial task state .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43049c07-8d46-4664-bf63-0102c0cb2cd4": {"__data__": {"id_": "43049c07-8d46-4664-bf63-0102c0cb2cd4", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In those cases, the state equation trained at a flat road is different from that of the actual envi ronment, and the robot cannot adjust the car velocity well. To solve this problem, we need to conduct online learning or add the image of road condition and IMU information in the staskl However, online learning body to the initial task state . becomes difficult with additional network input, and an effi cient learning system with only a few data is desired. the robot needs to acquire how to smooth ly step on the brake pedal as the car velocity becomes fast. Since the car veloci ty obtained from visual odometry is too noisy for pedal"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "becomes difficult with additional network input, and an effi cient learning system with only a few data is desired.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84e54cd6-7a51-4181-b63f-f9e3767fc9c2": {"__data__": {"id_": "84e54cd6-7a51-4181-b63f-f9e3767fc9c2", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "To solve this problem, we need to conduct online learning or add the image of road condition and IMU information in the staskl However, online learning body to the initial task state . becomes difficult with additional network input, and an effi cient learning system with only a few data is desired. the robot needs to acquire how to smooth ly step on the brake pedal as the car velocity becomes fast. Since the car veloci ty obtained from visual odometry is too noisy for pedal we need to develop a new estimation algorithm."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot needs to acquire how to smooth ly step on the brake pedal as the car velocity becomes fast.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf3bf93f-737f-4c7d-bfb2-a4420474f988": {"__data__": {"id_": "cf3bf93f-737f-4c7d-bfb2-a4420474f988", "embedding": null, "metadata": {"page_number": 11, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "becomes difficult with additional network input, and an effi cient learning system with only a few data is desired. the robot needs to acquire how to smooth ly step on the brake pedal as the car velocity becomes fast. Since the car veloci ty obtained from visual odometry is too noisy for pedal we need to develop a new estimation algorithm. the robot needs to identify from sen sors the direction its hand was caught and restore itself."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the car veloci ty obtained from visual odometry is too noisy for pedal", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3fbf10e6-7a4e-4a3a-818b-b7d29166e940": {"__data__": {"id_": "3fbf10e6-7a4e-4a3a-818b-b7d29166e940", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "the robot needs to acquire how to smooth ly step on the brake pedal as the car velocity becomes fast. Since the car veloci ty obtained from visual odometry is too noisy for pedal we need to develop a new estimation algorithm. the robot needs to identify from sen sors the direction its hand was caught and restore itself. the robot operated the steering wheel by a push\u2013pull steering method, and it is not the usual human motion of the crossarm steering."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we need to develop a new estimation algorithm.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c4f5faab-6c94-41c6-999a-2d6d714e24e1": {"__data__": {"id_": "c4f5faab-6c94-41c6-999a-2d6d714e24e1", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Since the car veloci ty obtained from visual odometry is too noisy for pedal we need to develop a new estimation algorithm. the robot needs to identify from sen sors the direction its hand was caught and restore itself. the robot operated the steering wheel by a push\u2013pull steering method, and it is not the usual human motion of the crossarm steering. In the crossarm steering, the arms move while interfering with each other, and the inverse kinematics must be solved in a wide range."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot needs to identify from sen sors the direction its hand was caught and restore itself.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18c264fb-ad4d-4968-8ee2-fbc615850f54": {"__data__": {"id_": "18c264fb-ad4d-4968-8ee2-fbc615850f54", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we need to develop a new estimation algorithm. the robot needs to identify from sen sors the direction its hand was caught and restore itself. the robot operated the steering wheel by a push\u2013pull steering method, and it is not the usual human motion of the crossarm steering. In the crossarm steering, the arms move while interfering with each other, and the inverse kinematics must be solved in a wide range. We experi enced that large internal force among the arms sometimes emerges and parts of the hands were caught by each other."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the robot operated the steering wheel by a push\u2013pull steering method, and it is not the usual human motion of the crossarm steering.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0515afcf-1d44-4ba1-8189-83324576b7af": {"__data__": {"id_": "0515afcf-1d44-4ba1-8189-83324576b7af", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "the robot needs to identify from sen sors the direction its hand was caught and restore itself. the robot operated the steering wheel by a push\u2013pull steering method, and it is not the usual human motion of the crossarm steering. In the crossarm steering, the arms move while interfering with each other, and the inverse kinematics must be solved in a wide range. We experi enced that large internal force among the arms sometimes emerges and parts of the hands were caught by each other. Also, we found that the robot must have the scapula joint to solve the inverse kinematics in a wider range."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the crossarm steering, the arms move while interfering with each other, and the inverse kinematics must be solved in a wide range.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "753eb80c-222e-4a13-b23d-27a7839615d6": {"__data__": {"id_": "753eb80c-222e-4a13-b23d-27a7839615d6", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "the robot operated the steering wheel by a push\u2013pull steering method, and it is not the usual human motion of the crossarm steering. In the crossarm steering, the arms move while interfering with each other, and the inverse kinematics must be solved in a wide range. We experi enced that large internal force among the arms sometimes emerges and parts of the hands were caught by each other. Also, we found that the robot must have the scapula joint to solve the inverse kinematics in a wider range. We used the pretrained model of Yolo v3, but the model does not use timeseries transition of image and is not trained using the dataset at the night time."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We experi enced that large internal force among the arms sometimes emerges and parts of the hands were caught by each other.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc07e778-ea39-436e-892b-a0b3aac4abf6": {"__data__": {"id_": "fc07e778-ea39-436e-892b-a0b3aac4abf6", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "In the crossarm steering, the arms move while interfering with each other, and the inverse kinematics must be solved in a wide range. We experi enced that large internal force among the arms sometimes emerges and parts of the hands were caught by each other. Also, we found that the robot must have the scapula joint to solve the inverse kinematics in a wider range. We used the pretrained model of Yolo v3, but the model does not use timeseries transition of image and is not trained using the dataset at the night time. Therefore, we need to train another model using video and dataset at night."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, we found that the robot must have the scapula joint to solve the inverse kinematics in a wider range.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8080ae8-8c43-410f-b530-ad44d4d53848": {"__data__": {"id_": "c8080ae8-8c43-410f-b530-ad44d4d53848", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We experi enced that large internal force among the arms sometimes emerges and parts of the hands were caught by each other. Also, we found that the robot must have the scapula joint to solve the inverse kinematics in a wider range. We used the pretrained model of Yolo v3, but the model does not use timeseries transition of image and is not trained using the dataset at the night time. Therefore, we need to train another model using video and dataset at night. we can detect anomalies; even if the muscle is ruptured, by"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We used the pretrained model of Yolo v3, but the model does not use timeseries transition of image and is not trained using the dataset at the night time.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c3ba16b-4b60-4ab8-a4a7-c8acf2083f24": {"__data__": {"id_": "9c3ba16b-4b60-4ab8-a4a7-c8acf2083f24", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Also, we found that the robot must have the scapula joint to solve the inverse kinematics in a wider range. We used the pretrained model of Yolo v3, but the model does not use timeseries transition of image and is not trained using the dataset at the night time. Therefore, we need to train another model using video and dataset at night. we can detect anomalies; even if the muscle is ruptured, by we can obtain the intersensory network in the broken state as in ."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, we need to train another model using video and dataset at night.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "685d34a4-d052-4778-bdb6-9f5b3323a863": {"__data__": {"id_": "685d34a4-d052-4778-bdb6-9f5b3323a863", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "We used the pretrained model of Yolo v3, but the model does not use timeseries transition of image and is not trained using the dataset at the night time. Therefore, we need to train another model using video and dataset at night. we can detect anomalies; even if the muscle is ruptured, by we can obtain the intersensory network in the broken state as in . However, the actual task of autonomous driving requires quicker adaptation and operation in dangerous situations, and the detection of anom alies in hardware components and their handling should be considered more deeply for safety."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can detect anomalies; even if the muscle is ruptured, by", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3358de9-b476-4275-8295-4cf7fece261f": {"__data__": {"id_": "a3358de9-b476-4275-8295-4cf7fece261f", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "Therefore, we need to train another model using video and dataset at night. we can detect anomalies; even if the muscle is ruptured, by we can obtain the intersensory network in the broken state as in . However, the actual task of autonomous driving requires quicker adaptation and operation in dangerous situations, and the detection of anom alies in hardware components and their handling should be considered more deeply for safety. we need to improve the hardware for the scapula with wide range motion and the body shape with smooth humanlike skin."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can obtain the intersensory network in the broken state as in .", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78fb3ff0-813c-4c35-a685-ce42fbc9ffa4": {"__data__": {"id_": "78fb3ff0-813c-4c35-a685-ce42fbc9ffa4", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we can detect anomalies; even if the muscle is ruptured, by we can obtain the intersensory network in the broken state as in . However, the actual task of autonomous driving requires quicker adaptation and operation in dangerous situations, and the detection of anom alies in hardware components and their handling should be considered more deeply for safety. we need to improve the hardware for the scapula with wide range motion and the body shape with smooth humanlike skin. we would like to develop the next hardware and soft ware using the obtained knowledge."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the actual task of autonomous driving requires quicker adaptation and operation in dangerous situations, and the detection of anom alies in hardware components and their handling should be considered more deeply for safety.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4a67ec8-0fd5-4479-ab86-6ea9cd4a8f50": {"__data__": {"id_": "f4a67ec8-0fd5-4479-ab86-6ea9cd4a8f50", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "we can obtain the intersensory network in the broken state as in . However, the actual task of autonomous driving requires quicker adaptation and operation in dangerous situations, and the detection of anom alies in hardware components and their handling should be considered more deeply for safety. we need to improve the hardware for the scapula with wide range motion and the body shape with smooth humanlike skin. we would like to develop the next hardware and soft ware using the obtained knowledge."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we need to improve the hardware for the scapula with wide range motion and the body shape with smooth humanlike skin.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bb56d8e-8212-4c2f-8854-761f0ecd1a47": {"__data__": {"id_": "8bb56d8e-8212-4c2f-8854-761f0ecd1a47", "embedding": null, "metadata": {"page_number": 12, "source": "Toward_Autonomous_Driving_by_Musculoskeletal_Humanoids_A_Study_of_Developed_Hardware_and_Learning-Based_Software.pdf", "authors": ["A Study of Developed Hardware and Learning-Based Software Toward Autonomous Driving by Musculoskeletal Humanoids T his article summarizes an autonomous driving project by musculoskeletal humanoids. The musculoskeletal humanoid", "which mimics the human body in detail", "has redundant sensors and a flexible body structure. These characteristics are suitable for motions with complex environmental contact", "and the robot is expected to sit down on the car seat", "step on the acceleration and brake pedals"], "doi": "10.1109/MRA.2020.2987805", "window": "However, the actual task of autonomous driving requires quicker adaptation and operation in dangerous situations, and the detection of anom alies in hardware components and their handling should be considered more deeply for safety. we need to improve the hardware for the scapula with wide range motion and the body shape with smooth humanlike skin. we would like to develop the next hardware and soft ware using the obtained knowledge."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we would like to develop the next hardware and soft ware using the obtained knowledge.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"f6dc0d69-8440-4c68-835f-74254d4b651e": {"doc_hash": "6754dca1febbcf12ef68b1e42d8e9f2c2290ff89de6181e37dd678e00c0c6866"}, "88bfb23e-6c24-466d-ab81-7778e80ca621": {"doc_hash": "8884e9ad09712b5924ded742df2a484c24ee7c6f1b3f03fa7ec6dc48d3c45e03"}, "f64eefdd-b82a-4a10-b1e8-bc9c32c5acd1": {"doc_hash": "2d0e200964d38801e2b9e82de0215e0ed33bdfba5f1e251d625cb147897b463e"}, "c2152666-8be9-4f3b-aa52-d3b2163488b7": {"doc_hash": "babce12a673fc4e28e874771ddb3f2dfe438d91b64ce8a6623e6e409f037d730"}, "d9d9dbbb-1016-40b9-9e36-a7d9646054d3": {"doc_hash": "854dbec9bea994d920cbc55f938d8332d134d6bc9e8412db0155e0524afa339c"}, "d6c4b0c6-dbc0-41ec-be49-81fa30ffa287": {"doc_hash": "cb6d6ca6e66c42376740669570c70f322d7fa6f3c39b4884c5190ac2e7559177"}, "be294e0d-dbff-4cf5-8a2b-dff8c8097317": {"doc_hash": "f4dbd4242cefff265edb8e356089743888af8ed1f37029d815231d1646f63980"}, "8f8a061d-e9ce-4da7-a390-971cb1a014a2": {"doc_hash": "aa2f394bbcfb2454307afeff8eebf1b1dddea8ff68489343d6707900ebe3e599"}, "0d6455ad-2fbb-45da-bfc6-dc8def2f2daf": {"doc_hash": "36239e0be1c0397d212af128b09d1ec11b0f467b9b90f78a27522adb76f3d3f8"}, "9039f1c0-47f9-4036-a949-36fafb8ccde1": {"doc_hash": "37f3ec8dc7646f054faae55f37a75e87bcc4784cd4acbfbd22a13c32906522f6"}, "6a12fd39-c638-4341-b431-b09a9a9a6782": {"doc_hash": "59503c42891c4bbe7c235bb1e20c7be8e6f362076ab998e0071c51257315173b"}, "2c3ac841-db74-4055-b006-fc873742585f": {"doc_hash": "1bd28d26e11230965a008495f5e03e75571ab8b17e31b783da5ae7d5e1e8e4c0"}, "363a2dd1-14e2-4ee1-ae09-62504c12bccf": {"doc_hash": "c97d601f728ff99f2c1b42524aaa3c3739d88913f35efe5c75457e5d265b7050"}, "0db7847d-61c1-4b0b-b6a6-13a65bf670a5": {"doc_hash": "980a2021a9db7ec07606aacf56aa3cb39c13681837d72ea6b195180204d28bd2"}, "d2028aac-fbfe-4356-af18-da154acd9abf": {"doc_hash": "9dd5aee253f191d94ccd78dead32ceffd87804060a9570ebadf64547e17a7ec9"}, "75a679f9-1655-4937-b4ed-96bdce2938d9": {"doc_hash": "a8770ca4343b1b1cd4b70b8aeea33318a87f3bc5bfb021b31837e87ad236b5ad"}, "a1d64d23-288b-49fe-8e37-ce0a976beb10": {"doc_hash": "ea3652d29fdfb52260ae20eb98894c3b431305e192da07e0261bbd064f5eac46"}, "1d4adca8-a6dd-4112-a493-03625cc604ee": {"doc_hash": "28937443607790c79684213a288c4fce24a43efc3e7fd90cd7abbde1792c5307"}, "b279fb08-b10f-45bb-b8be-a88b827c0f8d": {"doc_hash": "843bfde64f60ebeeb08a628b129c5507cf118002b96cabf7a713d9cf68638c1b"}, "c2593068-93fe-4698-a6a3-82356cdca07f": {"doc_hash": "af8d886795469bfb556ac2508c34b1d26cc97f338bd9857f3df19b64b2df93fe"}, "cfd3bd4c-7ff6-48ee-ad49-7e8ab3814c27": {"doc_hash": "142334c93b60c06e5267a3af42dc234b767a843f8d933fdb859bded24704b24a"}, "48ea08b0-0a2f-42e9-947e-77786c8de115": {"doc_hash": "dad4f3b6b9ac18cf9ea3f38ad0342c3351567725101c4226796c49eef7f5d68b"}, "89bb0f48-9076-44e8-b19c-b7102ac8ebe4": {"doc_hash": "b8f5608b473f2e41b5127fb17c0e6dae008f929776629432d493b7d4f04c5922"}, "98fd1e3b-539a-4cdb-bf83-e95f8b052821": {"doc_hash": "b3595f9e34f77a37f2b49913795f809c21f51c3b8f3734a6dab2abc44b7c65c1"}, "278e2590-1d61-434a-828d-d30c4e7b2455": {"doc_hash": "4eed8721111e20c3a6057fb9faac3179097f80b0065eecbb3cdffc344e5bade3"}, "18940f95-2f09-4d1d-9c82-c1a73c428175": {"doc_hash": "683dcb809d13bb636f311188f7742f47674188003e29083a49b4c0ea24fc6ba0"}, "ca9890a8-37b0-4b9b-8873-6841964bbd78": {"doc_hash": "4f6f55d787a308495e252a7c863dff73d8418f388f1e8d84eb89d4ff679cde2a"}, "856d71d7-c37b-4c2d-a108-62c483a01ab8": {"doc_hash": "e671b34d5faa635f08fa4eb076b362d23e9116bdb5b2c17f4350aace8877a75f"}, "68214f7d-328f-4264-933c-3c92eeed24c9": {"doc_hash": "5159e49f0b70b6a10a48e1807fc90f26202ef836635b6e31b00e9f594bff3cc8"}, "a85ad1de-ad4f-464e-9282-a83338e38b11": {"doc_hash": "e1c723d2025adb7c1b2c86f076a13bd64ee0f119cd231f2fd5db7aa72fa7acd9"}, "dc8b7639-2253-4428-8ccc-1151dc73e0d2": {"doc_hash": "ebdcfba215a3719a45cdf7b24d2672455f209dcd8d5fce999334139db194e036"}, "1b77035f-a928-42cb-a234-96d1aa50b62a": {"doc_hash": "8f0454d0b913b54beb8c3eef7d1de0a1bd027d070378687344fc8035fb92cb4a"}, "933fd4b4-eabf-4e89-8501-f4270bfe4173": {"doc_hash": "fe59b0ec9b65711a2727f10d90cdedfc0aab7deffd5c4a48d1e624d5e3d7f50b"}, "97cd37fe-bdf5-42cb-a657-725a2d8dd514": {"doc_hash": "ee00657d4005aeeee2a469c7e74d8570e2512786d71dfd31edac992972f0e00e"}, "780761c1-b488-4361-94da-2b65f120b510": {"doc_hash": "05d1c63468494c62093abd791c50272afc6b9d0a5a0777f1b34600106cfc9e3f"}, "b591cf49-ca56-491a-9c2c-ed0b0ded3a10": {"doc_hash": "9bdaaa5bbe8ce0021c6f491eee528be973b5c8c69e8196b02576fcd1d5114ded"}, "87015095-e5f0-4a75-ae52-6edddc84dbb1": {"doc_hash": "04fb2a77b42aceccdab8d288a25c879c481a0092016d13f4e64eb43e476e4fd9"}, "ad4700e8-fff8-411e-9340-87efde9d0d24": {"doc_hash": "522259d69421b8b8513372e1bc0080a1d69b8bc624901f826d70937f415196e8"}, "413eac7c-ab10-43e1-8b7f-84bb3ac68183": {"doc_hash": "762be5970fda786e54b9a7301bfbc0cae5aa6bf86f1f78819e13e2a1ef94d167"}, "ea5332d4-9b07-4329-ae4f-55e3c5eada91": {"doc_hash": "4624b8fba9a11c489cbd0acbf77b3324589d4f28ed78819621d2f48ed20925df"}, "12a77b7e-c039-4e5d-a378-f9c4ccf7eab6": {"doc_hash": "3770db5ba9955e39c8998743cc00e68f55636414345c4ea02722ea9e3f615aca"}, "2ffe3ff3-814d-47a7-99ac-0b86ba11cfdb": {"doc_hash": "772cd68ad05edd4814fc98e6b0c8130076241117b48afd8ff431d9e8d739ac0e"}, "f4910b06-8db6-4528-ae8f-eae4d3efde4e": {"doc_hash": "a9c91f0a42251ed7ee603bceebba7c17cd472192733643a2a27928d2d4839a60"}, "daedd081-874c-48cb-a20d-2b5829596e2b": {"doc_hash": "c6893db0313b704baaf117b690296312301e393da7a4d9d5d1d930c8111f4349"}, "a53e43cd-c675-4623-bf04-19ccaa9c8747": {"doc_hash": "0947ace8e79654959945a60039d62a05020e5b07f2bbfd61f487f14d8bf35a3d"}, "08d36cb6-535d-4ec0-8d6d-903d1473798f": {"doc_hash": "67583bc998fbbf6d15ff32b14084f363bb5415e6618c7036b61310a3b5cbdb76"}, "827d6bf7-9fd7-4a23-8e8d-f6232a0fb3d1": {"doc_hash": "cf1db3f000ababc4fa993a38aa217a6d1fcfc005e7860cfcb6f02cbb7893f070"}, "815bc4bb-13ad-4bd9-9e9a-fe7ae468f55f": {"doc_hash": "f44d7be627dadc7c9b496e0a2613fcfd36a5f3bce171b9876aef327951da4d60"}, "1253bc63-093a-4a5d-9acc-53d26f6423db": {"doc_hash": "21156989b0a9d99a88ecf2f94f10f7ab55232bfb7b4da92f5eaf419a27cb3eaa"}, "0e711a01-4ddd-4e61-b7f9-4a97d829107e": {"doc_hash": "ff4dd1938759311e126d9e04068f32981739a0017282a6ebe3084ae1c885691f"}, "825b5cf5-19da-4989-b553-3efa54d20e0f": {"doc_hash": "df275d8e47ee71ec89434849320fbbed60a573a9064b4f8428fc1c417a1fe929"}, "b2e86d3e-36ca-4a8d-9ed9-5b45bfb87aec": {"doc_hash": "75818d167617363a862ae175edc7f2ff8de515f20b3fe413fa9c0f55d1f82f95"}, "98e4587f-91e6-452d-a81d-ed352b14f226": {"doc_hash": "0e9a956f91523260f225aff59347448c0c6c227e358b94665a1a2451b963922d"}, "d252ca11-80c8-43e1-bef5-406fd9f4f772": {"doc_hash": "34362db00b490bb61f05e77285e9ef4d9cfdc4e9ec90f0e7a59878cdbee8f24f"}, "0f771aaf-8c76-494e-8ad1-55da40ae2b25": {"doc_hash": "21b665437431eb484edfea109bafa2f7295ce0c881a62d4632d663c48b0ee60d"}, "20e0a454-1cba-4306-90ed-c0a7b59be9fe": {"doc_hash": "9e27a10919de67b559300db85331f21bc7f8f0dd80dd6c5a21fba7e3627410d5"}, "2713e43e-5259-40e8-8dc1-83e5e7f77a11": {"doc_hash": "b780f6fc757aa1a42c9933844b490f6fef3ae329022678126d60fb2877e572d0"}, "82da8de4-ad60-4617-a276-3881f8c84ef6": {"doc_hash": "56cd534f5dd185c53a21bade8a9b45ae7ec82753d2f4501b8d89036dedeea5b6"}, "926c6aad-fdf9-4ffe-ac17-0fbf4a30fc46": {"doc_hash": "2884034719e7591ab8926440732d4e86a5cc6eb6d37573e3285a8f6aad2c7807"}, "3daa29bb-a333-475e-9f45-1bbe27124460": {"doc_hash": "e172254d3e229411e4e5d060c652d05a6bb0f101feddb1a2b568180206b866ea"}, "e39b995f-29d9-499e-9b71-2e9e69a4d244": {"doc_hash": "a800e42ced98ce46b7cc6cdf46b34cc011f5a3c5df750f9af8339a06e9aafddc"}, "3ce19e1c-4c3f-46ff-b72a-7b1ef13ce89e": {"doc_hash": "4306ecfc79bb41416f175866f31ccaed0933f26a43c83b48471e2ebae3d3057e"}, "6c48f863-f5e0-4f41-8734-726e8ccc453f": {"doc_hash": "4522eea70be4224b09e715a239a9e9ef22b09eb3827f503d124c989e24f8759c"}, "eba74659-908d-4adf-8e20-b7868a627b75": {"doc_hash": "bdf9b3f04b6f80f8dbc42b2d0390bb3a2ef9869f78aefc2ff1b1e33fe94a0c11"}, "22bada2e-8356-4835-84a0-1cb03918c3a5": {"doc_hash": "00999592f67a7cfa3281cf4cb81b9657cdc18a603ddb651b83401b3664f7a673"}, "5d05b455-b713-445e-8417-ecb91539e024": {"doc_hash": "6d9c4c5e2a1db14a5f4e44198b7c2e604f872487aa5d5255a8c6c76261f94077"}, "e838d63f-8158-4948-a137-218eebcab509": {"doc_hash": "b4a17378a342981138e85eb56c3598fcb1bdc30967893d73ba64269f02ccdfe3"}, "5dce6f08-a652-4ffd-94dd-9ffb508bc475": {"doc_hash": "7b82203e9d3711a9ebc79417cec05968ccc368ef72c50f1c371ecf30f3c5d574"}, "68bfe79c-a812-4407-b799-f7819210ceed": {"doc_hash": "8b8d15121b8b241a8d8f374594625df3fe781f61b57afcd7eac9449d0172fe27"}, "c8674cb5-c87c-42f5-98da-f70f0eb165c2": {"doc_hash": "58f320969657b0206a216391575470fc2cfbf9ca0a0c643f9b86db1523ab19ad"}, "73f091c7-3074-4914-b7da-180871e54ae2": {"doc_hash": "12ac3c3ce415b16418a52c4cbca96f87db09b9fe58eda99e70a2502b4c089f31"}, "969f0d38-bd83-4bd6-b2c6-a6d5a97fc6f9": {"doc_hash": "b366ecebeba2a56647b985a1fa62e8b757ca91e96b0bba2bb62e139dfb259ad7"}, "c53d4a2e-faef-46b8-bc4c-e70fe83d78e5": {"doc_hash": "32ec6d0743feeda867ff48bd0f4706f3100af5afb8af0edda44567218750957f"}, "7fa7b121-5f8f-4540-bd18-2e1fa78680b4": {"doc_hash": "9ccfeaee42754053a314fed63af01774811f1742208a392a02253059286120b1"}, "59f4d0da-b045-4901-a4b8-d3ae9572702e": {"doc_hash": "c31ab21614e7196bdb943033ebfebbc5880f47de8d3bcd15a07196b9dab066eb"}, "12c0734a-f277-46ae-a474-e14922e6954e": {"doc_hash": "0f95a0fce2168b1da1f0d4fb270102c60403ee977b1f1af1d570487a62ff82a1"}, "08afdb8c-4dc8-4cd2-8f06-485de55272e9": {"doc_hash": "6fa3b8b4380226dad966cfa5afa62ea1f2a02b4992eb38e831a43f127b60f2de"}, "8449f6ee-42e8-48b4-a7e8-fd8635ad1ade": {"doc_hash": "85aacce7d3b913f1998cd0e99182ad30d08e90e03ce56544cfb56056ed2afac5"}, "c2bbe6df-a25f-41e6-8f33-a702cc9e71ac": {"doc_hash": "2a8a9baabacd40b902312a3a568911179deac75249c58fd4e6df21f614cccf68"}, "4d3454ee-8ec2-431f-8a69-8b516495f1fe": {"doc_hash": "646c6b511ad6fde1b9d2f6da2c08274a233b4440c2aee1d4da61bab8f0d78e8e"}, "1792b58b-7730-4e70-87f7-97ad67dacb87": {"doc_hash": "457bdc2bfef19a04f94dc25e93b75fa7f5475dea715ca019bc7f78abe018c35b"}, "2865cfc3-509d-4f99-890c-8deb73445df1": {"doc_hash": "f35b9ea651d7be7976ced32ce0f1a5b7ab42c7b6d4dea509f79bdaf4f2b5d5a9"}, "10ba885a-9575-4557-96ec-865b9e91c8e6": {"doc_hash": "55b58c6dfa3535a169783e73a1b321d75f5a0a9aa26efd7cddb8caaf6a11f7aa"}, "a1b1e2a8-aaeb-41a6-a9b4-dcdf2fb0fe1f": {"doc_hash": "7526882dd87a336b6bc375f40bf15c04498b6fdbbde69aedb626de91f0a17469"}, "a7701903-29e4-49cb-b229-8bc35e1f88fc": {"doc_hash": "1677d63a139bf0c7dd81eccbc1c559b3b4c56769f1e990f7e2e6e92f506f459c"}, "2232864b-eae9-404a-83d5-ee7fdba1c3f6": {"doc_hash": "2d29a29afc8d96ad310faa3da7e121d5ee7cc4eb5d0243a8da0719e7941eeb87"}, "7d1944e5-d3de-4aba-a440-9382d33e01c8": {"doc_hash": "fc25a10843e1b02b5b3127aed0d8d14a424445a42232700da83a3f9990aa32ae"}, "d1ce9816-7b01-4d48-8537-0e8b142e078b": {"doc_hash": "886c76388c87f1d0718be05e5a67ded11a6b9fb31adb365f4f4d6159dcae51f3"}, "01a8509f-ba7c-40bc-b97d-7b1f7b4b1cb0": {"doc_hash": "1a5ba730e4fce4a51028f8de7f8c5c4338799484294ac5d743ddb8839594d1f0"}, "12d780a3-35b5-46c5-98e9-ec2e25abc362": {"doc_hash": "82ec41bd9945cd1284311ca5e5201b5568794404eeb2e34e646246ac418ae598"}, "469504e5-0497-4857-9bfc-88e568d45ba2": {"doc_hash": "85e27392446c22976de8cbecf3625a377a48d4fae21f5b2b9832e8a321df7c1e"}, "a086fc32-218f-42ae-88f4-f7a6735b99eb": {"doc_hash": "1534744d5af4523bf32db714a1521571d6fe04dfbb63afeef4b90c315c8032e6"}, "2787a4eb-d9be-4993-845d-909ec48bc01b": {"doc_hash": "60b8f69283c4c6eaf9dea85f51193a5e5715a19fabac2801754631f7b56d658e"}, "193db374-b10c-479c-b61e-8cf390c96217": {"doc_hash": "e4524e5753b031f6cafd7df938572e2086cf5533e4134446737b2acfefa5ca9f"}, "72c75b8e-b856-43ef-b8fa-dbf161e0ab93": {"doc_hash": "8e716b81cb989eda153219316df2e7fc68f102ce18054627812ba40031d1759a"}, "a98604bb-944a-4d6d-9971-ab49a8a604ca": {"doc_hash": "98ff18eacf6c85c386a98c7675393f9df7616054f7141d2f5c3b14a40c5924b8"}, "ad3149a7-3647-4fd1-a9e6-70f01aa40662": {"doc_hash": "cf660a8bf5602005f0bdf10bcb3320ae373c24766e87c63e2f9ae706eb072def"}, "8629a9fd-2c10-4648-b5d6-c5a26f59aa12": {"doc_hash": "e5817a6af4b63b3f57eb60760a6ae020c7dd620a2a845795055526dc814ddd71"}, "d266206a-7ca6-48a4-8082-f8abc33673ee": {"doc_hash": "72a90fbc0bfbd4049085324289e917d0143b529e6a8e89d33130de112a82ec63"}, "1ff945c6-d5d6-4b01-b4e7-da2e5ea3ea46": {"doc_hash": "eb78b638e39f1714927b954cfa879ce67e371f733ef77575dc8198b9470323dd"}, "9b3e3c96-ec00-4dd0-be78-0c879081f4ef": {"doc_hash": "a9e81b311e182cadbf2031f8359c1b97f0f57c202b07267452d0aa8ad5146e4c"}, "1166019f-9ca7-41f7-99db-e032c048c7c9": {"doc_hash": "c283da4b3d1edc3bfd01bda5f994e9eabbbb4321495f499bee53fc0b33edf7a5"}, "6ed1b753-f45a-4b1d-84b8-d58dc498856d": {"doc_hash": "cb2e814a951740b53a24d3bef40f43421817ef841e90b6cb65541663bb1f07de"}, "4a0cc928-3b50-45cb-b9ca-9098b081d4a2": {"doc_hash": "0d549bb2ffb8e063586924ba80008ce12489270cfee9b7304e562701e475caee"}, "5e353e85-484a-4fef-b386-be41dacb5efc": {"doc_hash": "bc94b7915cc800ed6f00e2f8bd90ba491e687df827d6464cd969c4ba39834b73"}, "eaf33d80-d583-47b1-8249-d4bee3324231": {"doc_hash": "0de3d5081b1604e2c4247634a428cd6946ebc72cce71c9f6228315ec957deb6d"}, "c28825f8-0ade-4bbc-b0da-09891da199f0": {"doc_hash": "b5f06406651e54b391dcc774ba05fd074fb83b7ca32f1541eb7f63ec9974f335"}, "5e537a22-9251-4f30-a93f-34626165089c": {"doc_hash": "ab3295f453fe1f83c2b125d2fce559177811fc893046046379d689803ec2bb16"}, "72ecdd6c-737d-4cd2-a5bb-29374433dd54": {"doc_hash": "ecdfcfc11861162c01ad6fc86dfb3b641fea0ad8c8785c0b845418a884f0a2cc"}, "2a38e4d6-b2cc-438b-bde4-f73c85117410": {"doc_hash": "4f4cff90e7fe692d16dca074e8b8ae96f64352e7d3d20de13eb34b30ee2951fc"}, "9f7d4a8b-c353-437e-8455-27adfe9182b0": {"doc_hash": "67a1d788c037c47c10f9b9fa4b27ee182ada03b1096dbe1e11fb440dbdd594e2"}, "967f6fe5-3f71-4e52-a4c3-2ff78acd5b39": {"doc_hash": "4800d6fe9faf4de0a8c2cd2dc3e2713d4080fffdb481415d5e30681e85faa5e2"}, "8b6c9b64-df71-4778-8b08-aa8a647f4751": {"doc_hash": "95152736c95072cdf2516365d579ecd3a78be953d5f911a075037d702518957b"}, "0096cdb0-e4f5-4b34-b322-cd36c7a3969c": {"doc_hash": "6f10007622947ca85c7139eb27db53f595b632b30d97af9d5fe056ad13a33105"}, "e4fa28c3-a558-4e0c-ba30-c38c41bb876c": {"doc_hash": "b8448fe4b28fbf62c7cbd33dad927c013f0c193617597ae09852d8e186938614"}, "50cd0038-399c-47fe-b3d6-d43d1eb45635": {"doc_hash": "4fb074d5cf394b1cb79502a551231c4461f29b53b8c4c26e301e651898acbc20"}, "772616ed-69a7-421a-87d8-ce757d9fb282": {"doc_hash": "4fe1b25571af5caa586df1911f87e6201e68b4f2ec3f0496589552470791b880"}, "1c969d3b-5305-4adc-b263-149ccedc57bd": {"doc_hash": "0911d349932a88c8c85a757e051c6ab767250a944015f34bab2d3733c5a84a54"}, "b6a417a7-f5e4-4449-b01d-368a37c13bc0": {"doc_hash": "8cbb730580be30dc8d8679833f3ddbf9224b573508262b5fb685c70e4a2d532f"}, "8ed98fa3-a9ba-4ac6-82b4-c680ab94eda4": {"doc_hash": "636092fd78d75c5070d9030738eff0e920df09d9e3efa380f2bfe60b98d9bbbc"}, "d2b2007f-156c-44c4-bdfa-3a1e8b2f1863": {"doc_hash": "0a1fa0b1fbf0ca7f84e96a377a9133b18eb7f87ade9c044b2b4ef844e88a4f60"}, "e08a0dd1-0358-424e-a718-aba261e28efa": {"doc_hash": "b632ba69cf6b8054546e335af963c64f6e4d406585e8f8532ca5f1c55dc41873"}, "e6936279-6bb3-41c1-8440-2afccd0719ee": {"doc_hash": "ac3acfc0e478bb2c24ec5f9fee99a6b352e8257859c1f334b412176c9c39069d"}, "8dbebd5b-af6b-4b07-9e0c-eaf66d730507": {"doc_hash": "947be8829ab919fa6074f1207670c02c63a8b1955872798306a592fa7a716581"}, "3d917202-dda6-4179-9bf2-764a4ac5181f": {"doc_hash": "df3370c8c5c647a9311aede7754010c31491709480bcd812b16f946a3361e477"}, "493a31d2-f938-48b5-9d39-6bfef4e7f300": {"doc_hash": "d06d463742b9e072fb21364ee30a0f8dc12721a852e882924a48980e42859ca7"}, "a2e8911f-a156-4f6c-8a0e-c9ba93a1ecf3": {"doc_hash": "f7cfb1ece918d1c9b0b482b1dcc21cbf83fd6ab0db293856b8c9b74f184f5efd"}, "b157d691-f164-4442-bc9c-0805c1d0653a": {"doc_hash": "8b70a534b1fed59e5e69a58a262f564d7542126995614f7e593ab94303a2d8f1"}, "fde62790-a231-4123-a4ec-a7681cf332ad": {"doc_hash": "b6e5bf7ea99290e312941fa6226a9f5d89512b9fc2bc8d0257979e3562581757"}, "26fae75b-b907-437b-af2e-6dc9e82ab20c": {"doc_hash": "3b746a2a9f1c2c69eb8c55115b2bef6c5d1a435e7d2f520d8077f9f9324d99cc"}, "3f8abb0f-28bf-4adb-a846-0985087b36e1": {"doc_hash": "6331abef21dfb24700d46ccd08ad9d9d4e12769266bf9befb22effa38d2e8fb5"}, "885fa230-71eb-48cb-a42e-5eb05a7d8a00": {"doc_hash": "441b3bdc4b2823964c400d04963ec84d93e56c34c6d4f4492b41cf4c654252ee"}, "f18998dd-5e73-4091-b5aa-48fdac623325": {"doc_hash": "488d1c0d877135124455fd4ce116d0c7d4b47d14cc31e7022a2a9a252ec953aa"}, "fdd81865-5400-40d8-aaa2-80b69b937cab": {"doc_hash": "89a85d4ae3de1acf103620bbffcd989b9ecf57e98a44e5074b1da0e3028c799a"}, "90d2702b-2082-49ca-96da-07eb81dd8a2a": {"doc_hash": "9d9b9eef7988a4422ca937be97182997ddbb6a86a798af1db97068ab0617696d"}, "d9a73a75-7026-4f8b-a381-a73fea148ff7": {"doc_hash": "b1cfd5e811720bb5e78f387ad53fecf95bc5f561b5188a148849710d9b28cf52"}, "629c9f22-9103-4376-8e72-2d009041f51b": {"doc_hash": "7bb4a2b995c2bb1fa15c05010f13c7fbf4a001ccb4523365dfc5acd72d74c2b1"}, "00622f2c-aaf3-4b21-a290-d44cdf507307": {"doc_hash": "20e03e59938faa9e312d84e0ebc5d810aaacc93a1ddee9d6083aa2d09932e642"}, "1e783a99-9d00-4522-901e-daaa708c8536": {"doc_hash": "373bbe5ae5d481fb8094509f203dd50577d5746a161df114c837e17e39e38658"}, "02c0a05b-8e21-4a3f-a17f-ffad3d02be9c": {"doc_hash": "6e4de4bcc046c4e5e1ba34515408324c706daaf6b6ae8ffd66c1ce6b12f1ff4c"}, "5d988e54-6aa7-4631-83bd-26b8cdaa97e3": {"doc_hash": "74a58dd3302c5d494400db7fb28e51474eaf4f1c871d6a72113dc8949d0f4b47"}, "cb5c2262-b8eb-47a5-b383-fc754589d405": {"doc_hash": "6c74e080d48ad33d3e4b03d24e66036abd085e76c009e9380a0095d436bbf4f7"}, "ab2530f6-7064-411e-b125-49fd53572aa3": {"doc_hash": "28f79fd5df767481beae4c679ef43087245f5fd182dab9fa9c7a034827db0e45"}, "b395b144-5be5-4998-99bb-bc08615eea67": {"doc_hash": "3cb28404cbcae499236d865cf9e9deb0d07d2a5fa9ab22dff4cad715b0b0b779"}, "5ad570a6-5ce1-4351-aca5-e05dd72399e6": {"doc_hash": "f63758bbd89a953702e813f91e7ab76039cbff4b9e98b4f639c55263c25e24ff"}, "95e9ad27-16ad-492c-9903-31a080e397b1": {"doc_hash": "bf5d2e29df8c5ccbfd22ca226fa86597135448190c4f180b0e0b016f74915ec8"}, "6ded8ebe-fd88-4b6d-a761-60762ef7f877": {"doc_hash": "f07297423996035b223d05f4e1e547e7c4594c8668a8b495c347c44c2290f1b1"}, "2f99361b-12bd-4b30-9dd2-268aaacde7a5": {"doc_hash": "e48cb058b8ba64cc3ceca7e5414cbe8c120b7648d3c1c8f123f908a81894620a"}, "bc71e666-8c96-4fc5-b80b-87c8457d4d14": {"doc_hash": "b9a5382dfeafd6cba61e00c5ee0faec8729c31b3dacaf7ae4d2fca272f5b140e"}, "0a35f4cf-1a26-47bd-be85-14f77c3827c1": {"doc_hash": "9490076691bf150734aa1b36170f4dfd6daec5f143a9c0e4b8c2adba39491e94"}, "49d27d87-60de-4ce0-ac91-1726d6d1e8d5": {"doc_hash": "af7141fd4383676b6aec2672ca0fd65209b9f20975cbc91d6bfa645fe6ee9d1b"}, "59a04999-801b-45e2-a472-b6c764916e4b": {"doc_hash": "488ebcc11dcb660975210ef992673ea2a03fc412a97c5e3f8bb5a2a8abe68cb0"}, "5c436dc4-31af-4141-82d1-b8478e195a70": {"doc_hash": "43bf1c2b5fee0024b071acbdb031ed575eb947236aec5b7b183cc5b6c483fc52"}, "b179f188-71de-4267-aab2-3cb7a8b5907c": {"doc_hash": "cf033c9365b8352bcf0012cede0f8f219425a3601406464ba2d61d204c2da15e"}, "8b2eeb60-8af2-4b57-883c-9a05ab81ec96": {"doc_hash": "736305347a4bd5242d968a64c98d0687c54f0bd091089cf426c2190d7ce3e04d"}, "eaa94b16-3c77-4bb5-9484-2b642782c6e7": {"doc_hash": "41da8a1c9c06e26bcf6b4b52ea896d0cf6849ebedf84f18890753f50ab28c8f9"}, "1be9bdd2-687b-4aaa-b26c-32d043deaa80": {"doc_hash": "e601d9be2050c433e32ea334bc653cb0d9f6ce44c70fa6b2d65583fe14a1e065"}, "d94bcb8a-b9ad-4b78-aaf9-8ddea38a5609": {"doc_hash": "4cb570d48cfaddd3eb7fcd9dc326ffc946e23a3081d1166f2ce3865e327f0a2e"}, "d47b84d3-53a0-4cdc-ae9d-d523a71e5458": {"doc_hash": "69d77458893809e4b941d6fccb84b35a6c704b9f078caa4f716339d1e5d480a6"}, "e1354447-033b-48b0-ae07-451df4fbcc9f": {"doc_hash": "0f441db05ca16cab56793f6333f4267d7783d94bdd2c1e78f28dbb160d1169c3"}, "05b9c90f-5522-41b6-bd43-26aab5ab4552": {"doc_hash": "2f7ef4a90475d9b888198777d56658b3b0edf2b752774857c2db1f8a80c89280"}, "d48815b8-f544-49f4-8390-6d3f399100b8": {"doc_hash": "7de085b36149eed329989f4187c3a50066bf86a65b6ccc3125c9b4cfefd02ee9"}, "6d993373-e1f1-4b35-816c-e921782aba31": {"doc_hash": "d1f90abbac2c4ff62443586d8ead0ec4959976357badbcbd0e4f9ac04e59c88c"}, "1d118efe-4cee-4fa8-a2ad-44d925eebca0": {"doc_hash": "0e637dfa9654c8a39ef51372846f7cea2747eff842b8cd6b12531bad52156772"}, "a7a4ea5e-4b70-47b0-91f8-8ebef28b2150": {"doc_hash": "a57d1e4a1b17a8968a3cd5de93d443a00761be95f381ac7b9a190241edac0502"}, "b3f7ac13-8c91-487f-9a50-d6eb01e48090": {"doc_hash": "66723c44310e87703183d1046fffd87931ba87bff7a294db911530af93555eda"}, "e83fef94-f233-4970-b3fe-477d05e0dbe7": {"doc_hash": "1d5f5e040ca88f478bffefd23cc795dca1ba4146711899167a929465b2f60b17"}, "d15c3e00-b33b-4140-a7df-b9f3fb53ce65": {"doc_hash": "7e2a918d476f61b351a165bcfcfd7eb4c99db4342ef49f2d712a65057279a57b"}, "632c1702-d43f-4e4c-972e-6c313e1c9835": {"doc_hash": "4eb172acb697872c0fe18814d241d135206b5a74e8ba3e15c94779a46bbcbd9d"}, "c4c1f171-2eb7-4406-afec-edc3c91c811d": {"doc_hash": "ecdf7bc735d7296b809407e7e39189736493055de22a798ef0fa2a77959ad5d4"}, "54d5c17d-1d46-4add-b595-915eba71feb0": {"doc_hash": "2077c614c21a7916421b3908100bf5ccc7ab8837ab95ff2476cfb830171f5fb0"}, "a3a708fc-4da6-4e97-ab1e-8a8358a60cf8": {"doc_hash": "609ae2f2a9d18982927641148c09adc8cc3a1c0a867e72ea8f1c4e72a45b046f"}, "ac0feaec-8d7a-492b-bcb6-300c42e81199": {"doc_hash": "e2fbf7bdb0360b0a88a79747d67a32f57aa7f22abea7cc23ce1f720cfec80be8"}, "c5fedd20-9c78-4792-a5e2-3d7936a673ac": {"doc_hash": "40ef8a19cd58f9d952a79597ad6a0f980d5240f372cedf4b810edbf0e38ee6c4"}, "ca0b644c-5d0c-4da0-9967-59ab175eb51b": {"doc_hash": "42d3b0a59947877d0af998fbccb82755b7c3998dbcec8121deb8273e1cb7f0e9"}, "7936823e-03e5-4173-9553-0efe86bab6f1": {"doc_hash": "4acb8e7356d30611d9eabac5400c1ea9b40b223e12c1a3a6bad49aabc0c41c2d"}, "a6014149-c447-4863-9db1-b0b4746510c0": {"doc_hash": "eb79d90d7dbf0d23eb59953c49d4e7b1976066df99a55f3ee9e05bdd3bcaeb60"}, "382dd6d4-31e9-4692-80fa-e168455bd91e": {"doc_hash": "0bf0a7c68f6040923295b499028dec05da23131350f5f0fb315ce84143dbbc1a"}, "ec2d215e-8b40-4387-838e-f85ea1c2c34c": {"doc_hash": "755ec21379e8e4dded278e8a2128b62da36995953a275f57077cbdc4114b74df"}, "6b4e7f18-8a1e-4dd0-907d-a1bbb7b8aa91": {"doc_hash": "5c991135b5e48ee1dfa56c1131ca3cae1c0005757f969452a8a3e114cdfa35ec"}, "6ebbe811-be80-4785-ae3c-f50a1d82b761": {"doc_hash": "fa9eb846eebf8abd41ed7e4c3c714389fa6d0723122d28e088d93baebdd3256f"}, "a0bbebdd-7faa-4009-ad04-dc51a502832e": {"doc_hash": "4dc58d942e39b5998c50f9ba6af8238693bcc95b77186e69ec98e15d28165d2a"}, "0da114be-0755-4206-8239-46fd644b8fde": {"doc_hash": "f128922e451791127831e226a1cd72e62fc33bb4cd706ad8c60da71d7550b82e"}, "57042e45-0596-4fcc-8ba9-2eb45f9cf693": {"doc_hash": "1e6eb3b6324467e3626be5a57b1b3e23ade8cee43105f72c84029bc265067a38"}, "7141f8d0-27db-4144-9473-227327c9ee54": {"doc_hash": "f5aa42d590ac0b104f2ca6943ec6b8679bdb41daccf3d0ddbb0ae2ce362ccfcd"}, "89e9060b-6249-4639-928f-fabc3900663a": {"doc_hash": "d7cace76a91bd596e1851e1743151423083bc70abc2e94f462e45686801eb848"}, "fea33201-2eac-4cbc-95e4-175b60f09fdb": {"doc_hash": "eb78dad7f5773733c3ffe80387edab19b278d0f209546d273203a3d7d45de450"}, "56944e45-6aab-4f43-b579-e41229097380": {"doc_hash": "f7cdfdc0a5bc5a3aa5caa4e338a826923e7946f0b4b139458d657f40e99f7b4a"}, "87987577-35a9-4135-87d8-64e484e7d842": {"doc_hash": "e2afcbe59af9c77bf9adaa57317ac46968ebcb05de2276eae0741c41ec796762"}, "397e2e26-fbf8-40fa-81f9-dcb507486f38": {"doc_hash": "efeee04de13a89a8b4d86aa9d1288aa63c16747f45f8cd15faf7567fb126157d"}, "8be3f661-f6b3-4ddc-8b23-3af31a692129": {"doc_hash": "757a79676e4ce89ce6aaeb639fabef18391ccbbd66093851bb4b91963979bd01"}, "2e85b00d-9045-44a9-be7f-71c586c52c9d": {"doc_hash": "4824182cf5c72eec55b9e906f2321e7f8ed65a6f22c50f00749572cc44fb76dd"}, "e249e863-94b8-4de5-a947-6429cb03d698": {"doc_hash": "14fd62510ed8424187bf25f38ecfc6c6cf7d9b0acda5714900982c16c9ae7db3"}, "6e52e6af-3397-4d0e-96f9-7b75f5cdea4e": {"doc_hash": "a2bd70d5fa42b9a0b94921841b9acd8a0adfb219a9860d56f545907501db7690"}, "b8ade469-80db-48bb-b180-67e0b744b18c": {"doc_hash": "faea60f849111d357546d92c0bceebdb9dd96f1ea61bb2fbdee6b9e193fae141"}, "0b7270b2-ef2b-4688-858e-bc40c97f810c": {"doc_hash": "e9f41750bb11d8dbe2aaeb2ee7b8924d9d72ef26563c7a6e7133c294223d8f58"}, "86c361b6-fab9-42f9-ad44-cc362baf0c4c": {"doc_hash": "8b2db52c600b9afe24302885e4c47486082b90c1ee139a2654d283961651ab43"}, "aed2c5c9-07ab-4e95-af9b-d6a843b88369": {"doc_hash": "ea41f1396ec18c3bd9ccac01c485c128f201f12171542ca536f9c6b530a041fa"}, "cef55299-7a81-4da8-b929-3c0812844529": {"doc_hash": "29d2ed82d6e912eb79d6f4792f5616a05395c6c71b5dae74094987b01236c6a4"}, "c9670508-3ad2-42d6-a5d2-bb3e3bbda5d2": {"doc_hash": "b0bcfc221d45ccbd5658750852574a960860c72c056009d9396a2e579c42397a"}, "964e0e81-35ad-448a-bcb3-279a45c96279": {"doc_hash": "ec495c74bfe309deb5350866365e4fc5425cc23433ea7baa3a0b9d1abd658ccc"}, "49369c25-b556-4dc1-b342-ec57e326aa56": {"doc_hash": "ec733b98abee1b850ab6814326e7d086ab3ea4d7c2def080df8d7f60e5a36c44"}, "2b1243a7-e923-40a7-9424-0146dc865843": {"doc_hash": "74a7422063b8677e5fc736b968b7ff712df70c046d5028fe741de44005897a28"}, "e6537f27-53e1-4bad-abb7-8a5b85da1766": {"doc_hash": "6a7bfb38827bcd9600e627b6e3d13b874e54240b2d00c3019108f5168baff8e0"}, "44044170-3cfd-4b0b-8b7d-5bca19871389": {"doc_hash": "4f13259f308d531980c9a39abfcc400c7f6fa75e59d5e6734fd0140e07ac959b"}, "cb349c86-c539-4371-8419-fd719cbed565": {"doc_hash": "6a1fc549d55a803a4c5f8da1630e8b7cbe15671348ec7f1a2122c9551d54492c"}, "2419ddd2-6b75-4664-9685-c73a6de49113": {"doc_hash": "4d61c02bfa6b334c24145be2bedb836dc2c19e65beae529c11fad9378b41eb38"}, "9e2b4fdd-8cdf-4a6d-8584-9b72f228f8f2": {"doc_hash": "ccdf6c8787cae7bc718f02d2ad5efa7d11abf91855334cf92fb4193bf6dda19f"}, "9d146f9d-5f5f-4ade-98db-e0e9bf65b443": {"doc_hash": "808e60864ce617d548f2c4287d77147aa4415634ed263996ac85b18dcfe66967"}, "a19fe5de-b74f-4703-a629-ed8ba2ee5d22": {"doc_hash": "b23260a432b3a089d62082328acb8897487a6cc4e5461f82e8cacd31862aa0f4"}, "9083feef-9822-449f-a969-0c749060fb62": {"doc_hash": "b02a5e18537bd0f810ea9906a5dff74f7861faaea74df8cebe3f399f8a4792a6"}, "6866a6cc-060b-45bd-a3e3-e6d04dd71bef": {"doc_hash": "881a98845bf51e2046382d7eab3ab3186973146b8b5bf5a4ab475152f4665c62"}, "3573fdea-2fa4-44a4-aa12-2a8a58779da2": {"doc_hash": "78051634c3c201df4c2850ee1ec741ba46ab6c8539c624c34809a762de074439"}, "bdb65a88-e09a-439a-b019-b7ba6cd45b35": {"doc_hash": "4efe156649062bdd6e139fe1c2b6602b42846eebd8cd856cc8e4079065c61a46"}, "6858b31e-c16e-4082-afee-e29801ed4a60": {"doc_hash": "46fa422fd7560d73dd2e12ff0a03725775d248b17cf73f29a3605bcf3fc2be82"}, "668539ba-989b-4e33-86ed-8a6d93abde41": {"doc_hash": "76c4f6e84514d048e63ba3f5a7f246ce5dbb3d555410d97a5e963329ce0bf0c1"}, "61541ddb-c964-4fda-bdec-7c87130ba617": {"doc_hash": "29372ca75a9545bb9e51f707cf96c1dc0560d08a36ee3529f72f05300116a757"}, "858c47ea-f1db-4538-88f0-09ba1b8113ae": {"doc_hash": "67e829e4f09e4f4b38b22e54a7261547c159be75ec9563f42ab4a77fc8e709ab"}, "ece23999-0cb5-43c8-a9b3-8a6cd1a16beb": {"doc_hash": "9166ef38eda8e4d667804459b789ed289d0721d5cd995a13767336f788682dac"}, "3a888623-54da-4293-92f8-741accd4c161": {"doc_hash": "b3562ab0f07c4f04f0eb3ebe98c50e161fc3e8f5992ad9ce9a0049ed3971dc9f"}, "da52540a-5ccf-4db4-8f6f-022502024328": {"doc_hash": "f3d0928e08d6cd28ae68bbcd18282c97ad4f4a96907419f0a6f0502d02deca0b"}, "bbbb75b2-f363-4937-b5c4-11b541cf3a33": {"doc_hash": "feeed4806b337ad3a34af9c91d4dbf285db895697d2d6b9dad95fd04b093b61b"}, "efab3ed5-b83e-491e-a616-20e35d588633": {"doc_hash": "0ab0a521e77fc5a711aca5b69632a4fb443bbbc99728a8f1b7124a12118b5095"}, "f7f454f5-364d-4a83-9bef-f27015435587": {"doc_hash": "1779cb6519b1cb052cedb09e6787e8feab5b1fadc6b8e6d326fcb2bc1aa6b579"}, "9ec63400-f511-42cd-96ed-c17525e3ffd0": {"doc_hash": "f52a168e220339ad139255b60359200d27adf81380496e019802bf5e3fa504eb"}, "b4287913-94b4-45a8-8ae2-bdf3893d3256": {"doc_hash": "ef32407effc5387f74ba7bd5de65c4eecabac05ef66ef378834eb04602ea9000"}, "0bbaee7b-fb09-4aa7-a87e-921152aecdec": {"doc_hash": "7fcbe46e36a9261af7926e0c7fb6b6eada60994839376c9185ea81ba9f6df8c8"}, "65d0fdad-6670-4310-aa0f-4e2a50171db8": {"doc_hash": "25ab8eb2c0690a9f9e84785d27e9384b565bfd0c2a78ec56a3428392ae122b07"}, "7ba19d05-0439-4d0a-a877-140bdde9d36f": {"doc_hash": "c7be1bae2b1b8e775b7a777c3cfabe3474382706228af472294e78c4b01c6e72"}, "63c47e38-d519-4aa6-abc7-eb56ac59db94": {"doc_hash": "7b334780f264cbf3cd7aef98fd45510f63fba0c206a57782503b6d597897eacf"}, "dbf8f52f-0da0-4377-91a0-04329bcedc7b": {"doc_hash": "47ea8e852f0119160fe8732bf4bdc60e4f0e6f890f09f367ddb4f4bf9583483b"}, "4fa50f28-461c-4b89-b4b7-3a6e799a0fb0": {"doc_hash": "25f82ce27b6fdbf2ad4e6690e3952614ee87f77640aa90c2e5bd0efef6891cc6"}, "54b8c599-8116-462a-9cae-7320968c40b4": {"doc_hash": "e0e26b68711bce94782754eaef02e3a98bdbf10223bca1ac16682d4fe94aaee0"}, "3329fdb8-f99a-42db-b056-189fb1d354a0": {"doc_hash": "3d450dbeb94ab2e00a9cae7a924e66c8b1db4fe54e601b526f5293c0ec5521ff"}, "0e6c2cdc-8ed5-49c1-8e5d-1f7c4199831c": {"doc_hash": "4c6acfd1dee3f8ea1aefdc7a0de7ba67e589fd8352b89e83dfe9dac123a9e508"}, "e44ef91c-a872-42c9-b632-9402ac5d34d2": {"doc_hash": "e654b40ce6de0dfb99e8ccb00c9161bd31d012188594d6371fbe594ac567b0c8"}, "b3bc85fd-de3f-49bd-a68c-81da11120f08": {"doc_hash": "5d7bf7b19b5e40e71df2d381ecaf9d371b81dd50ade7d6d0cdd7b6edeb430a36"}, "0e88fd4e-8a32-49fc-b4b5-ea574ebcd425": {"doc_hash": "3b4c58e06afcfd4214af9a8103f7a0b2630ea8304e213e3d3b497f8f82f270dc"}, "aaa824e3-2702-4d4e-afc3-aa25f2a3c6ac": {"doc_hash": "673fb29facee094a4694712f87330b79117388142d946d5221e3d84500012195"}, "3c81c6e2-85e1-4474-9e7b-909a34c96aed": {"doc_hash": "a7354e9903bbe1d7db8c2b547522ef3eae1fd3b98276a40f8347fb64bc810657"}, "c29b878d-0c41-4be7-a96c-20c344097d3b": {"doc_hash": "d29c2324955efc0dab57854976952db5d6fd5b302253704fda5aeaee725de33c"}, "d5c81d68-109c-432f-a125-e927a9151787": {"doc_hash": "63187f9c2a297cadb54aea8500e263de88ce143f5ed7933f856e81b2fcc482f6"}, "b1c837f5-d983-4e02-acae-fa6cef336a5d": {"doc_hash": "4795a53f88c42f49c5e6fab05f2001729424fb7a17e79622dcc24d45f2f5958f"}, "8f4a7f6c-9b3a-4cd0-af60-c7cc271cea10": {"doc_hash": "47a18a01ac820d33d4a9a8549227d8452f9577f7df38784a64f626e84c332ef0"}, "cba3e429-938e-48c9-8e25-7f9d9647e03b": {"doc_hash": "ff2114bfea9a6e6ffa5012d1b7b7a6904dceb54ff110ef22c5406fb8165c29df"}, "ac3b0665-f62d-473d-bf4f-891cf31e44d0": {"doc_hash": "274f082fa7f460dac2220b28fcedbb40c6a9a0b651a28ff1fa6cbcd5ea982be0"}, "8d3e5dde-545b-41e5-90fe-80ca9ade55c1": {"doc_hash": "7252b0c76f51a3132136016759ac73a2511fa22c5db7b6591e42fd5537e837e0"}, "7455ba65-e26c-4ff3-99e8-0ca866f2840a": {"doc_hash": "4bc9cfbc6a6f36534dc0c343533eecc0434ccbcf2dddd49ca3bda766e6292d2a"}, "f64d9c32-bb21-4486-8a2f-4adaf7acbd9f": {"doc_hash": "7508cdd620cc1fd945902c97b0efd28a5efe32ccc914533fae90ece41c307911"}, "52e9727c-5971-41e8-a5b4-85bc52398287": {"doc_hash": "937aa649f6c37a48aa0c2ed8d105d1068e8b63d2349bbf3f0a59a26cf8ca892f"}, "fd0032d2-e316-4110-9b80-fc900de7562b": {"doc_hash": "c1209b86911c44f7e681d954ae942c409c81a1a29c5c9043de0d2ad50ae7a4e5"}, "cb8bf0c1-1c92-4de7-a793-b69da6e0b3d2": {"doc_hash": "286633c5b9656641cddb3105c4ff4b6fba1daa2b0d482182e62a8c2688e604f8"}, "660b086f-4187-49a0-9392-971d279c4737": {"doc_hash": "ff6411c14bd3e55e75d836a98f1d320a7fb7c4948b454c352e44c9a249acea39"}, "a8fa835a-0a79-4924-8da3-ae4ed5149512": {"doc_hash": "2935ad17d8fd1d8952345377080564c0d2b31c865ba46653142ff3f7cf576705"}, "0ee43df9-67e5-4b98-b19e-bbc4a3bf6878": {"doc_hash": "abc6381ba67d8e68cfc667f73ec7cfe9cebf49e3642f305aebac9aae5f237468"}, "7f2bf6a3-209a-49c8-b26f-1f887ccb6449": {"doc_hash": "9b9124214705b320e35833ae38d7fd86a5311ef97c9fc8c1383ee594c41d208f"}, "62981326-88f6-47d7-822c-ca4147a7bdd0": {"doc_hash": "94ad90d539d1651e7fccf1efb78014889f4ec765b6b43e508590358f8a530f74"}, "a8913fbf-4f2c-4cf2-a84d-3b28b71d0369": {"doc_hash": "60f38de9e09cf5bbc92f388dd63da900015404cd91f3edf5c6f86dddd7912276"}, "2207b831-b3bd-47ab-b2a6-98cb28612181": {"doc_hash": "486b1add95071241ef0db41e74c8a90ea53f30b469e4b9f3f46431bfa81f39e5"}, "3bef7599-9bb5-4b57-b44f-21df3160d0f0": {"doc_hash": "bc0d86a25c6f2f9ac6c755295d8a75ac3b28d18cb650c040adb40157004c6c9c"}, "bc3b3376-fe75-4d4c-8b9c-eb23cd817639": {"doc_hash": "4b37a96f9ebbfc9f4a3b6acbf5ae7fcca8d8bed48a80eb8e9c3f3f830f7be468"}, "3564d56b-33eb-488a-a9ca-717ab6a6f971": {"doc_hash": "c893598eceec444a18d1d27c49706002fd936b06b8519d591b0aafe2cde072e7"}, "82976e87-3c86-4eb6-b59d-69e2a88ccb66": {"doc_hash": "1432bd52fc8d9801101ec7b5130712203ce8f7921cafcdae441ed4dc201b0340"}, "c0397d77-e454-4f58-b3ca-0d43f9acedcc": {"doc_hash": "7f94ba7ef0706166a57c2609dba85e31736f28dea141fd950c7925993a67084b"}, "1b073eec-1bb8-42c7-aca2-511152d9a5b8": {"doc_hash": "6c33de53b043d2276e34badd81dca71e9bed055d520ae5ae608f4e2ad316084d"}, "24a863c1-34a8-4062-a11a-09220a0e0068": {"doc_hash": "d7807bbe28be10047c3cb7a5ca78119dffa22888fa3e16ae8428f74b64fc3111"}, "eb30c30c-de17-4f6f-9244-43ae169794f2": {"doc_hash": "77edba96949f6da2109f6feea4fc7cf5bff28a21629878229c70e2286b02ee02"}, "7a0f5758-324a-4162-a640-2ff9ffe82daf": {"doc_hash": "9596d6a811129ebc29c25fe328c2b5e4a27aec255d242b9b069aa94b29d351d6"}, "1c09089c-638c-4cca-9e05-8270767285ac": {"doc_hash": "a91d398397ffc5d843ad557c7827627ca7eb28ba7e776511c41eb022f3896957"}, "0754997f-2f92-4755-97c5-28d2bd78e6ef": {"doc_hash": "015040f23859018d6b25e027238dfbdac10d33508d4c636a71ba06260cee1778"}, "e29dc5e8-9d27-4358-a41b-ed0437e9cc00": {"doc_hash": "5ef824369bbfe6396051dc52d14609590ed00a1aa60304503fa95f20eb63c6fd"}, "4d3e1973-7606-4925-8473-54100070756a": {"doc_hash": "1760b6a39db2cf9f6e317f8848cd9ecb987e48bfbd61ab8d55676515dca45bbc"}, "bb8aa0ec-5f8a-406e-8c07-79687bedeec3": {"doc_hash": "967da3e13f0ce69b140f5d497f4a6bf4ba752f8a053b5e7f02c979dbb7eefe3d"}, "f10340a6-1610-4d25-b9e5-fda75e92afd2": {"doc_hash": "8c32ca18c9be5d0e77ea737ecad672d3bebae6d6616a71dfd2ab2b924d1cf954"}, "df75f5af-260f-4938-98a4-4ad1e0740de9": {"doc_hash": "10813ed28cd40892b52547eba77574914efbe9d9a4237d024fdfdde64cd5ff92"}, "76e92a0f-0517-454d-8b2e-12a3207250be": {"doc_hash": "dfb660a94c54ffa808a9cf83ce7dda7b330fd04bac2e6a8b1ed33731e15c6378"}, "c4de356d-14e3-457e-b57c-a235293d106a": {"doc_hash": "2daee2a6ad68858601ffe63d2a6aeb480b6a2408d21b52fd28a6ba5917e071a8"}, "0f955a54-2b47-41e3-9c29-96e1404e413a": {"doc_hash": "5da2121de3a115af78219a5b463cbb0f4dd2d52864fa358bf50088311455db9e"}, "2a0070d5-44ff-4da9-88e0-57c3f852ebbb": {"doc_hash": "2c12fd86eb3c90bf5f00b72e8ee9e42cc4a5b21745a5a07a6179ebf63e85669d"}, "b2687904-e25f-4264-9896-8eb4e04c7e09": {"doc_hash": "58cbec8c41c70deed4dbb794134627ff482b026ec41b3436b596e748f63fe6d0"}, "e286df0d-9b0c-45cc-a3d6-8587c244a9c1": {"doc_hash": "6728ed9316e30d4f9f4dd0fe44bde12602586ca4e932c626cd22aaa5f23a4e40"}, "b4bc6fee-a95d-4dbd-ae62-b643d63b590a": {"doc_hash": "61bba209f6a1271fea713d70bf418070662b4520e41813da57936050c23f0cad"}, "79ea31ab-c6a9-4bf4-b032-7799953c5198": {"doc_hash": "992e466c97689196ea68b8115fe83c9efe1263f9a41ddba9173568a7acdff2f0"}, "d97a7ab6-2671-4af6-9073-761bfff34fe0": {"doc_hash": "2172af23db893c71bca981c6b0cb379264b03b8cb2d014e2ac6436469303cb3a"}, "ceb016dd-6799-4b65-984f-be29a622babc": {"doc_hash": "e8b250407e2e2bde57cfe76ebb8453594febf2bc6fb39d1331db1d1d21deb089"}, "6850cb54-0f3d-4073-b184-ef2aabfe0f79": {"doc_hash": "af917fdc3f0e1a66f627e5a58ababb25cf201dfd03034194a44f364a3297f61c"}, "72733cd3-72d5-4452-880e-b613a76f269c": {"doc_hash": "256831703f8f2a0d808832ab75a2f5a219208bca79b376978e070b066cc88424"}, "8cf47e23-5175-49c7-bab8-547667d8ed67": {"doc_hash": "7fc04144bf36c976e4337888150ea03ceea220cb353ba3ec7a2962636826af1c"}, "fa4fea19-6a41-460e-93d2-4d3843f595d0": {"doc_hash": "b4799214ab82f7fce1dc0572c30893472bc37f4e3ad448c45bd916f553d3dce5"}, "51cf25f2-a8a5-4c6d-aa4a-4645955e5a91": {"doc_hash": "c8358d1910ad42245e4312e868e134e02364d0df3d30e6cdbcc3e937a89389a6"}, "82512e86-d83c-423c-96fe-f45e8eb17969": {"doc_hash": "1fecf4cc0d2241617eab678efb6a2b00d2bf839848e77a831fe7312108f2cba5"}, "ba0ddf5e-1ccc-4c5a-a37b-c6dd84b9ff82": {"doc_hash": "6dbc046649d2a55724fc38bfcffee28a09a3ac0708b7f3c9ed4d0a63889a2af7"}, "b2a780b2-67c8-473a-b434-5578a5fbbdb4": {"doc_hash": "838649348172007643b92136f315518803b042ded052ee7a037d2f2c2cbd245c"}, "e2fc74fc-f684-4ace-831c-bbc1e1ea3bde": {"doc_hash": "340372255d879981a351bc54002969b9c104fe6e1a953249af0f522921f00acb"}, "647c33f9-296d-4f01-b5d6-0faf922a68b2": {"doc_hash": "7c659ed4bb99bbe10dc478a9dd13fa97b44d78856c4164da7e6ec12e7c38e561"}, "d7674dbc-3c9f-490a-86ed-d48fae4c4e6c": {"doc_hash": "74d272a476250bc7c6456dc81b11ca4fa32e4f9893c73690bf2be07d5196e600"}, "a9e30a0d-7c49-4289-95d9-99030ba34fe0": {"doc_hash": "160046e2008bd886124a863206063852433b59f69f447a461cb50532ff4f4ead"}, "096020da-fc4f-40a4-bae0-9d5b6477bb48": {"doc_hash": "e624b59b0f9a4821b1575e90cbfd48d357c9c4adb36d9e9f1809faa510afc9b7"}, "37426114-fb64-4769-90bd-1b286ab23183": {"doc_hash": "1b3bc814a3102ff79a6bb3c02a7868597835874f0c1eec04686f8fbe96d366b4"}, "794aece4-9924-42f2-bf63-d966d7fdb76c": {"doc_hash": "4137df75648c597d5d7fd1ec145a1364ce6171eb5a6886a7c3edae6ca8b42cfe"}, "c44ed328-e42a-4ec3-b291-8867685eac7e": {"doc_hash": "2efd048874ee2846282c61cfe335fbb048acf105fb67342683fae4a1d846a027"}, "20cf7e9b-ef05-41a2-a576-67d82427c8d5": {"doc_hash": "2e3c9b00abbb05e40738c65ce9cb9a211032feb8f4d758e4833aa8214efde0b1"}, "d1f5c754-8a61-4cb0-b780-9f7d53e6e03e": {"doc_hash": "da3857f896902bf5c2a813fe902905872432ab450792717c41eb5133ec812c3f"}, "37d0ee96-5989-4d5f-852b-f5cd31d80c50": {"doc_hash": "b2a808cc05a72a740a46c5bca579bbf273c9225945c57522bc51c2961d20bf6f"}, "c0239874-dc57-401a-a5cf-7d43084a9682": {"doc_hash": "9fe87337569d144067c03f1c8b7a100329234256bb62780fcec94495b67a3bc6"}, "190b1579-9f63-43b6-b503-43aea6b81946": {"doc_hash": "37ba92609426da57e0c40ab8cfdcb5e01d75691ff749964ba5ba4e2151d5f9e1"}, "d509f709-546e-43d6-beaf-5b3fa41db220": {"doc_hash": "83bf117f43c693c1193238f99274ce1fdd22cc7ba7fa9947a21aeb992218aef7"}, "d25b3644-e682-4350-b141-6a1b75315211": {"doc_hash": "18f44d38ed559a21bd6a00d6292f29dc3992b9f5b8cee370d37fdbf9bc3e80e0"}, "0318ba3b-80c6-4a6c-bec0-b76619502727": {"doc_hash": "417bababde99452812e2988b80a56f2afad584e67b985bb075e2a33eb4f49d52"}, "a765570b-784f-48da-94dc-0ec4acfd8912": {"doc_hash": "0a492172e35e468b4285fc5ef858080687c1c35329df39862dc100ec26a35e37"}, "dd6d482d-16a3-4257-b881-f56e90763856": {"doc_hash": "662f069acce969fbf40d870d78b46159168d0e9de45751102d1ae53ddc66ba72"}, "7e115ada-9c54-439e-bfe3-b2e1b1e307ca": {"doc_hash": "ea61b5dfaac7bafe47fab4e78d699d055f0d184dd6b73bb58c190975931286a2"}, "32a65d81-984b-49a5-8acd-84e3ee54e9de": {"doc_hash": "caa44ad9e0be9e56deb80101a89ba4f891a826972d2dfef7c22e9d25a9cd6da5"}, "78b73576-715c-4a2e-9850-39ab2b6d8f0c": {"doc_hash": "2667e6894889649882b3c35ae8ca91b04f16af3cff41d1d8bed18f20b6d202e1"}, "11c9839f-70d5-4ebe-b64a-9b95ae790d84": {"doc_hash": "ade5efdd2ee5f760950870049246992a975b9a66f302a524682ebd763885ea72"}, "835f285f-037f-4740-965b-23b60dd9c1c4": {"doc_hash": "9a371552a8eea276280da27ed8c4bfd41ce1d489b47f7c54069de0d88c9fcd17"}, "7e210df4-e1f9-49d8-917b-18b29cb2db56": {"doc_hash": "c35c7706ba08ee88889bc73032f2cb43751e33c81ae25ea468b33d652c8ae6f7"}, "67f01724-d1ef-448e-9c09-89050724321b": {"doc_hash": "da078b7a72ae0ae14026da37e6d81967f679dd372d9721e65a6fecff2c66afb8"}, "30a8d2ee-6bf1-4f5c-9775-1c7988795c59": {"doc_hash": "f9247939826c54278921c13ec8dd825e4001847939adcacf14fe81208ab93f41"}, "34b8e486-53cb-4297-a38f-652b39b70f41": {"doc_hash": "40c4f71498371a65b40f8242d43df94577cdb5d7a30ff323eb5e841bd461f1f5"}, "d0f6f059-cb4f-40d4-acd7-b8e498938666": {"doc_hash": "bc7e595790113f68afdb881d161f06d6fe4775e9e051c33cd33209fd5cb8cfe6"}, "3bfcaf9a-9d99-4943-b1d3-6f7f79de10d7": {"doc_hash": "69604af2bdae6f4722744e90709ce6430230b6a036a61d1d2aaf4c0d520723ce"}, "207cbd31-efc9-4ad4-a64f-3e1ec209d6c7": {"doc_hash": "d6d9ccd5330fbf3d9e8731f3a12583b5f302d0685a291c57271f7a564b719cff"}, "4d0633a1-973e-4192-8dd3-c5cee737cb6b": {"doc_hash": "4de69586c4f6134979706ae1924a0cd52702f02599f7cad3e4afcc3f6a62e8b6"}, "76c8b93a-a8aa-4eab-b7de-63332b877649": {"doc_hash": "41f138864e1deeb4100f875c6efa495cd9afaca9611d08fbdc0d0a7e84d76993"}, "3c47351b-a548-4736-8caa-a477ab4f8dc0": {"doc_hash": "660f597f22277f79c032f2e854e04123c02413971bc746647f741c0a68cc0790"}, "343bbbe2-b8d6-42ff-9bb7-0318d02bd4f7": {"doc_hash": "82444f60a4e85ae805bb8c3d425c0431256bd0903bf267023b8ce678c740ed8b"}, "ac2c1ab9-44c0-47bf-8577-491d44cdc39e": {"doc_hash": "c4b5cede969414f9bb4d7616e416dbf5909df894d7e89e2624f1c12b5c645ab3"}, "8698582d-8c1f-45af-9797-04872b28ded7": {"doc_hash": "e7febc5d24e028764fa9f888edd55330ddab4a8b10e1cbafd7858dbbae7b37cc"}, "23700668-70dd-4b5d-a4c3-1f16a863e212": {"doc_hash": "224cfbf6d242a5aab212c8973ae5e0fd92f7c79bb4cc08b046d8ac46d98003f0"}, "e7b2a662-9a86-44b2-94ea-d3e7ae9c650c": {"doc_hash": "854b5a1e920853df7e26d1bf8bc5ac77f4efd10059bc9e23818801ad61b85714"}, "f3b9f378-7cdb-44ae-9118-b36a0696a1da": {"doc_hash": "16ab0b0b09e8a5f1816bc4239b93a5b51f0c52724c8dfe96def4375d8c3e6f1c"}, "df4bccf5-c030-4e74-9fd2-96973157d186": {"doc_hash": "8ccecd4c03ffd47e85fc2e4b074cfce115a7d29c93d1801b80dda69ac38e6491"}, "194b9cf6-6db2-4640-a962-a18d5c0e92b4": {"doc_hash": "1c750e1d6a6c4e90bc5f342459e37463efb26931de18cb2d5a3ff65771634789"}, "89dbce7d-8794-4deb-b404-4875bbab8a03": {"doc_hash": "446df3ecd6d32ed4d58b78972641667a417c8e2466394f245d0298d037de7127"}, "cbc34bfc-e78b-4045-a8a3-2da30cc53cc9": {"doc_hash": "02aa6de646d2485ce2552450967934edc0eb8d9d8d107764635777de32cefc50"}, "94cd962f-738e-4fce-b29d-0eecfe11904c": {"doc_hash": "f7ef70c944ddeaab1a54db0eeb3d151d081c70ddf101da6f10fc269b34a276c6"}, "948e37ac-67b2-4281-8ed0-eecf860a3480": {"doc_hash": "ecafa4499ad6e5f5c83c2482e9b7d1a96efa7fc8fcabc0fe709da22e50ffe8d6"}, "981d7e86-b614-4576-9136-c3a7ac444e6e": {"doc_hash": "c848300086170d248594bb80b2023bc9d691ef0264f23db617b923b9f6e9d13e"}, "20557f7e-7de3-4e05-aa98-cc590ac7849e": {"doc_hash": "5f245f434ce0678698b51c1564573ee137ca11f48942a0d1fd7ace2278fe4942"}, "07acf763-22fe-4698-9353-c8343da7c08b": {"doc_hash": "dc21c4ac573f9dd3550c84ab0ca60db87d1623cbdbc554522160b03c881926fd"}, "df02f0d6-303a-467e-8873-709d585c5a35": {"doc_hash": "2ad0e482b01a706cee98edaa68c7e389111942f746a5242d134bff60bed5c8fe"}, "8278dca8-920b-4722-8907-921d82fb1f36": {"doc_hash": "f2cbe736b6608e51de7da35fe138eabbcdfd206574bde300ae25dc42815a2d8f"}, "bfa2610b-e202-4511-bc05-ee03c184a5c4": {"doc_hash": "fde47783a26212d56114f5a0b3b95a5b5d80427e6ab8dd363a578d30b7f52f47"}, "a366d406-d83c-446b-aaf8-d88d01516925": {"doc_hash": "f70d04b0fe8aa8d3e9cd80aec9e9f80a6f8e8ea0b3a8669e5f72e519cd914b33"}, "993ee037-9883-4cda-8406-5866a66cacf5": {"doc_hash": "370ae596400a2b6f9f3feee09bb3789ffc32b844111697338ec91096180ff40f"}, "df84b4b6-0024-4524-94df-e87ea5def2f4": {"doc_hash": "6820b3e4cc45cb1b84d2c7e5631d65110f45129c6260bb7cb27d6c09d4a28f3f"}, "1af094a0-3b5c-4c54-89d5-6f9a954c0a28": {"doc_hash": "6f32c22200527919cbd25a0295e6b35c263971a86ef68a42bed8fcd4811dae2a"}, "fb6a4f43-98db-4f66-9275-afe97f3d36b1": {"doc_hash": "ea6e5bdac6bcee0869281cd8118df340fae643299c2f7ef543758a16b68ad13a"}, "0a5cd0aa-c8a4-4679-8cc6-f2c4db7a7a10": {"doc_hash": "ee0deffee61bbd57181b83b3bdc7cc530602d25cb9fff674948bd15b1de719fc"}, "6d816f1e-4736-4fc3-9a64-2cadd922ea34": {"doc_hash": "d438ca42969dd3a18bb1b249b9458c393f5c4ea0df336c135a7681c39cb4ecff"}, "c6720815-519c-46a9-9eb8-d57576617a97": {"doc_hash": "68af100aa9a154de91b690b59099e2d3f850ab6f08a4cb77a65e3a077058caa1"}, "3f8bb31c-dd0d-479e-8a6d-b2f3cc3f0ade": {"doc_hash": "06d0c1bde46fbefc4b15d3856132e545e65ed88dc735f226f1e9c8a30f2b4588"}, "3aa03d24-2c7e-4870-96f0-d6131be653be": {"doc_hash": "f38c570caec9474c781fba15eb58f89c0dee03eeff575bb1bcbed87505a13742"}, "68365893-73a4-47c2-af43-9a72d1c9b479": {"doc_hash": "bb2cd5f560b50f0ca441680d1411f4542a888689070b4c48f659b757d0c24eb0"}, "fed331cc-a5f7-42a2-8df0-6f21e35c3b28": {"doc_hash": "e9f768a763cdc5481108cbe378fd7de78bc3ee77220b29d35a19f3a9c911b772"}, "f77f31e2-3e86-4ae9-9d19-3e0934090a5e": {"doc_hash": "04fc06fdf0836b0fae9bb6720bd0516fcad65dad0d9adc4f75e22273c6a39eba"}, "1a3d0aad-170e-4a1c-b64e-a9adbaab2b03": {"doc_hash": "9f2cd03ba07d49f962bd15b14e645586501015706aa6016f9e2b8cff6fe64212"}, "3134533b-e744-470b-a885-edaad3796eb5": {"doc_hash": "b1bfebbb139b2dcf74ec3769d8ffa0e52f060dbe2ff3f66ce15a8e0d1c302e7a"}, "a0abcac9-682a-4167-9896-b406be59cd76": {"doc_hash": "b5d0ca762d395fecea3409fc7130c59f9b9bf1256fd56cb0088bfa3f1d4c01a0"}, "f8dbd3f8-9c91-43f5-ad11-711ea18b490e": {"doc_hash": "4fc2188273199f54adeb069f03491daf78be24af344fb36bd04de2124ca879be"}, "3166d3b2-2e64-49a6-99a1-c66364fedb2e": {"doc_hash": "5c149abf0aabe2bcaef8c6366c7c34304918cf8e32df87bb40cf991a87ac04d7"}, "2a5dd044-c79e-4340-8805-149de4931cc6": {"doc_hash": "75c3791c968ffb68b50bfd815735b73f07ff6625972acbdccc7da9ec6e943157"}, "c2bcc3fb-509c-4974-b649-0318ea6f4f1c": {"doc_hash": "1e8f118a725b0dd473b7e2bb3b5c2c12a34ad1219c8fcefbbe0a6426f86aec65"}, "dde99584-3064-400c-8441-a1107e95deaf": {"doc_hash": "bae6fe5129f1d508d5b1901b0c4e89989c13c0b052bf78baa1c11e496eb3ef78"}, "907af906-f5c6-49b7-94f7-587098942720": {"doc_hash": "b18fc11aa25b98b72d318a69b0ebf677883ba7f91d7542b3460acf5ab2430ee0"}, "10d49a32-245a-4f73-8089-bd15fabbbf17": {"doc_hash": "9f7d77ec88410aa18555f81e88cb0f488bbce503afdeec01eec66a5732208ca8"}, "342dee3e-4641-4d14-a6b4-c5180a1267b4": {"doc_hash": "322585c76829355fc7b470e5e23e6907e2668bb0d424983345141ff1531f2fe2"}, "a6917501-72f7-4fc2-a11c-0b435414ab5b": {"doc_hash": "de3f11841f6dc277adf87a71796a5746099d92557165e95f106c0df7f112ad9a"}, "2be34c1a-c24c-4c51-96a2-32682e75ace8": {"doc_hash": "f08ffe7001a948e44c3d1e47cd0db872e851e6c60198ef4f4c319a63bcd1869a"}, "10c4c332-2fc0-4ef6-b6e5-6e86468746c0": {"doc_hash": "b71a49ae356d4e968ab6ed9c7aeed7dfcd49931f464742912270d72755af097f"}, "88ea6a80-89b8-42fe-b2b9-87d0f92a6535": {"doc_hash": "bd7571ae26ed2b80fa3fd0a390368239c3cee573f390a66ecf72e204a8152533"}, "8be564f9-d6c6-4bec-8408-4c7246a0b61a": {"doc_hash": "bfd26b187eb90bc87e9906209ff944d5bb765fe525ee4b339ca5f35f48042b7d"}, "fa617d7e-fb23-44db-8244-291e878e6ab4": {"doc_hash": "82d72163654baeb4a5b17e26534222bebaa5ac5eb40ccc78ea993f7d988be5ff"}, "3a7cec06-b46c-45f7-a71f-d49169ef346b": {"doc_hash": "ab16a715989c9e4d63ccce48707395d67f67ca29b0216c9d8f642f509c936d52"}, "a475c164-42c8-46d7-976a-053ddbe1c0d7": {"doc_hash": "7f1e6e89948dcb7a0a81bd8dc551f471d3fae0a6ac9e4fbb22a35ec2cea94961"}, "7638b625-7381-49b9-9cdb-18dd09efc0ab": {"doc_hash": "dec0dce5f904a927d880946ceb8f2caae6329b1c4a50f3528657dd1d1e1bc5d0"}, "c2c459da-eec2-45a1-9e7a-26d670512ad9": {"doc_hash": "92f999b489f998eaf32d0cfe2009d30817683d5b46ba17a2373084dabf209301"}, "8d7bd5c4-a0de-4fc3-adb5-92401e7b19ff": {"doc_hash": "997f0b43adce071eca0c6b678d5c485a16471df0878b2b278777617258d9764b"}, "8e3ced3c-a7ee-4b99-96ef-af77f1dbffa1": {"doc_hash": "9cdfb0fbe53edb012e7031b07f454f92429e211443eab5dbc5065b2a3af0adc6"}, "8528c256-20ff-41bb-922e-c7e9140c7f39": {"doc_hash": "5cbd420b4b160327c76564b6875bc7ada769b177c19b377018e0dc2ecf2d90e9"}, "50cb3538-642f-4348-8472-fd50d271ce7d": {"doc_hash": "9f7e64837a90db6e51524a2eb7aee62e4ace3a2d9d6ce4011c5e6daad748fdc4"}, "7fe58327-f295-4fe4-8d2a-cc73d8fae2f1": {"doc_hash": "5898ef84addb301d9d0660573da3848bd928d67ba172dacb1407aeefb745ac14"}, "d082cfee-086c-4f44-9f06-54da45f7b972": {"doc_hash": "b1df8784a73920d189cdfd1d6b3b04182c85651d1815009dddd113e9ce516189"}, "25bd4cb6-7097-4f25-abb2-add10d1989c8": {"doc_hash": "559b1edd7f626305c1c8b10d1b779154ec8b069bffa1911414c1ecfc1e0163d0"}, "517b1098-0f21-4ff1-b485-e1964a456608": {"doc_hash": "a2631fedf66d0f3c0e4952268d170b3987b14e6760c096bf4aef7efe08d02628"}, "57f88eb9-19b6-4860-a651-4917a74eda25": {"doc_hash": "d6b23036b5fb335633f8b9dc1f25565eab189c0c9ff656b385e910de03e590fa"}, "5d072507-1f97-4778-b0cd-c866d4a89786": {"doc_hash": "b4460113c0ed92a41040ef258eb9f765cadd847a6fc5eaf4fd371953bcf5bdf8"}, "f8b07c68-10ea-4a7b-9f66-088f0da238a0": {"doc_hash": "5390d5eb6fe31bf6b1a84271b29e634bcca41ffa72bb6062e623f7c06baf7227"}, "6aac6622-fa0a-4638-b6fe-1d1ab1074411": {"doc_hash": "b33d9a4db298b68a2c9a1cc7e4e17ea624b673baa52641f86c856a6e99d18034"}, "27e94a23-744a-4c84-baca-41087f7f3bc4": {"doc_hash": "a37b18e768655423b57e82dced1952e69d8f47a2f7e08e9d3f4127724bde388e"}, "b174ecfc-7311-43c7-856c-6fd9ffe328bd": {"doc_hash": "ba29da34a0d139da6e7fe7a381850608503ee05a2c5b618f87c37bba601d3dfd"}, "8ea12de1-743e-4b48-8176-f0095af07924": {"doc_hash": "f2e052efe65e3028d146b0b8d6b3d1f9c4dfc92c75fb9a5a2e0211df99c3fe15"}, "eb56df6c-51fe-4ef4-a4a5-77731ee9f7d1": {"doc_hash": "f5ab4a1861c069896438978dd9c6524524036eeba87a677307362db7226af3c4"}, "4ee3ef64-1f84-4368-82a8-a888ca3a2e97": {"doc_hash": "e26f178ad6f85df2b6246489681801eccae3c341771ef7589d0a6be97acbc85b"}, "54b8bbcf-9eae-4cb0-976d-015dc6878057": {"doc_hash": "df0c379cf6c22635c72d0d01734d97af77c644dded6d4cabb4f9f2cef5ef5bdb"}, "61bf5286-7530-433e-a692-176b74cdda00": {"doc_hash": "7dcce0ae2a931df79f2725849fd38e6368a47a62cf6c87d12bfd52d5ea0ac201"}, "81f4e997-2885-482d-8a4b-5b12b6943b5e": {"doc_hash": "4ce90ef4aa467301e8baf3d73b39614b26f153fc18e7273effc5f04e562a7e9c"}, "4c396f98-3064-42c9-9db0-8b3749ade0d5": {"doc_hash": "d93e0b98cc12a6a6612c5c736d283925fd55244f5e2446ccab6aa17ad676f0ef"}, "c7aea38d-e548-4b5f-b80f-1723bd840021": {"doc_hash": "16297ea6d8ea38ec0cf84216c939253a7fce23b4a6d0280271fe17dc45eb256e"}, "2f2d0bd2-b916-4222-ac4e-6397a68f3ad9": {"doc_hash": "9435e97939838a0801aaaa12252825faaa0912a46dd38fce4561188c74f5ca69"}, "e9643de6-e07c-4a03-8fbb-6642b8ce0c95": {"doc_hash": "17b557c165707a2b3e5d147e0ff98f7607410c68d01dbc213f8c5583b7c10d17"}, "106c24d5-d466-4657-8e43-2cf29526aeb5": {"doc_hash": "0fbe1ddbe11c30328e398ef30c6d659a1f4b09553fc6bda2559b5244fcdf35d6"}, "e02bd80e-d9ce-4757-810e-73bb19175399": {"doc_hash": "86b8ed527f97370b9d2cfc20f2a1c3f4a23422dfeba5b63f2dbef8054936d93c"}, "1f0e32de-7f5e-42ab-9b0d-80c691163f4b": {"doc_hash": "06eb7009ccfe9fe028ab2ee64ad1fe8efcc9464a874469aac8c1d6ff65ba3e20"}, "84f3be36-4936-4ff6-893b-69a6bc648cf1": {"doc_hash": "6b134fa872772f32a77777020d050e30514318708b99b88067070184cb3e101e"}, "35b7fb4d-e33a-4b26-aa80-c8800d2c43b9": {"doc_hash": "84197dbbdf389629e1567831b6c6899f43586eaec9bdac95d6f42f90b7aba0b8"}, "1e162de4-4807-421a-bae4-15123a2b4352": {"doc_hash": "4a81c771807f51267dac28e87442b58e228f849025934ad99aec66d0a68e8a79"}, "fcb446b2-c4a3-4492-b569-524a87e949fd": {"doc_hash": "2d017d07cd9bc310c5a3f2860f5180fbcc1cb98f130ee42cbb84ad7d35e3f2e0"}, "46c250ae-ea27-4e5b-8a29-77585af21cf3": {"doc_hash": "f079c4c3d0678e3236ae59b9116242187a2e6cda3dc81d3643f19c6c55ef6816"}, "a72ace99-cfd9-4184-900e-5b1d40d829b7": {"doc_hash": "5b2a792f65335974465177405d9e1c76d91ae9967798d6b87ab1ef83ad8e75ca"}, "09636e0f-a68f-416f-92e7-f0d60ba3fe73": {"doc_hash": "e5168f3649866a7e0fb91641aeac54d62870f85566c92badc307f10461205d1d"}, "d5a677c4-31e5-4029-b071-5c26cb51ea95": {"doc_hash": "b713cf91307e87a36dd7938c3f66e5b6ff0a437120e693cd50f458cfef91b33e"}, "ffe6cecf-9077-4691-81ca-e69434661773": {"doc_hash": "6034e44a9e9bbcd354fb260ed80e2afefd07576be0ee881f326da59ff7a3f157"}, "62572afb-f057-42c4-82a3-a3a894d1e6dd": {"doc_hash": "5b5dbd74bb77d0b189454d45c05ff6fe5cbe5525f3e5f0811755e60c116f20ff"}, "e77d5652-e7f3-4714-a80f-d0a6b3e289d5": {"doc_hash": "8ddb93a883e648395d0d77a33e84119f297350c3c1ecb59e12b4a4ff9ec67320"}, "a7a585f4-fddd-4bc0-a378-d1e0b708076a": {"doc_hash": "d695446239ddaa7d3415b8fdea553cfce411f0146cf476af6a12e9467a52773e"}, "0b7ec44e-89f8-48db-a85a-eea397103770": {"doc_hash": "78874ad9b22118fe1c366a0b4a398a4507ee6a4e95ae2ecf22e921485ba3af5e"}, "cdafc79b-f932-4c64-8467-9286fc9f7d1a": {"doc_hash": "af91bb04d7c33ca6c66778056f56995b9b2698a0df012cf51bf8b3f833872614"}, "277b97fe-66c1-46c9-b801-86248e74f8bd": {"doc_hash": "d6f1d8760b96e3e42effdccc976ed5a0d962508e29de153d609aeb58bfaae75e"}, "1da8e5c3-1cd6-4b97-a8bb-125e775be3c0": {"doc_hash": "abea97c39e7c2d88b540958970da81f6eec09325e4c96870e7fde029f5719cb6"}, "8f9594e7-7437-47b3-9a14-40e926f96cb4": {"doc_hash": "7defd6a066e055ec9503916a87b7f729309ded47374d99186423e99430ce19bf"}, "dfaa9501-85dd-443c-88c0-768f67ee16c6": {"doc_hash": "771d12460470d46b16743ac093d5abd038616132e61b1260f30d98cae212dc3b"}, "21d39b5f-e045-49e6-a01f-8b5c1569f201": {"doc_hash": "97cb4aead8b9d45c262c0fde6dae1ec9152a47abbc0cd490e3bf0310f0f2636b"}, "5514903f-b12d-4e31-9812-bd224ee1d5ff": {"doc_hash": "495b31dbfaad504eeb8aa3dc55e5bf1952cd9078d697a84b9462bbcdfdcc4477"}, "37833400-2515-490c-9ae7-387a331571a7": {"doc_hash": "7dd589c761ff0a385f4639f0b7feb4c2d183382220f417a209a555fedbaf0672"}, "9f9e7e34-e0cb-43a0-b863-a8bd2a70acd6": {"doc_hash": "22720d6a68749851c0068a286ca1d8791efc795fd6a5bebbe52be056efe2ac6f"}, "743e0625-9e34-4bf4-9a2d-87416caf41a5": {"doc_hash": "06e0b7367264c58b94ed01cf0577267b9bd105e703cd665df40f798f96a26d53"}, "ab7e35f0-8034-4a8c-8410-31d435c7e7e4": {"doc_hash": "cfd121610185c9081b5c17fd72bfe1e3bdd50e5e80e69e6166e06c7ec3b5d1af"}, "0ae7c766-9c40-4db8-a5a3-1d98f626bbb2": {"doc_hash": "a103be3262d0fa61c6b6cf54b13c2b41718de328bc7eac4af37d237163316bcc"}, "b4951d43-33c8-4790-88b7-47c2afba07a9": {"doc_hash": "7072ad8ba5be6b7e7684cb8144634f09a52ed875710287edf57447234e75482c"}, "9027d42d-9546-4a13-9286-8abb936ff526": {"doc_hash": "ad9989bf3759532de9c6f0a1cedd3783f4d3a3f3289186343f3ec0abf102ffba"}, "5f886487-719f-4070-8d64-7d1eb61a8f38": {"doc_hash": "2baeaa060834b9533dfb9b73f9e728fe47976efd7054af70fee319cc06925ccb"}, "fa96f28e-b170-4afd-8d05-fda959122ef5": {"doc_hash": "d9c46ca66fa3a039c676702d38ffa12dba7a29d9477bb5a3d6ee57889ee75185"}, "eb0a2ade-e53f-485c-9de7-2884df038302": {"doc_hash": "77124057ffb14ecc4db7942dda408ed0512376a14d068cab05bdb475239a19ee"}, "19212ae0-b8f9-45ac-b06f-a528562d85ba": {"doc_hash": "a54ed45f91b7a4cd7835bab2fb5083fa195bc1e23aeafe34a11c71347b7fd15d"}, "f5799a40-9301-409c-9769-fd68c5ea3aea": {"doc_hash": "5ee5c44252b1805cec1067010f274f7157084f0f87062ac1d0ac3bcdba88012a"}, "252beae0-e031-4073-be3f-31897ef7a51d": {"doc_hash": "0105bbe875d0774538f09bdbc2f42e6e4269d60bd32a04502a69570feba3f0ce"}, "55799fe2-c798-4fb0-933f-6d6d992d4292": {"doc_hash": "baafed8f42713dd992695a1a9db0a08deaa48ea3053852493acab740e1ab3fd6"}, "04bee71c-d5fd-4d15-9019-9df7fde7bcda": {"doc_hash": "5d84b1a35788074f4d450f9b0fdaa3f091d7fef72168453904349702544589d1"}, "ac669a46-09bf-466f-95a7-dbb5cd9e7975": {"doc_hash": "154c909aad8e144cf2d63c609fa746dd11ec299cf835be609e703ed7cf7293ba"}, "16eceee8-6f7d-4c2e-9d0d-d0761d744722": {"doc_hash": "0e09b6afb063dcdffb2b78febc721867cb64626d64116a56bedcf3cff564f495"}, "b1fee925-bf34-4834-b356-bf0bea9f4aa4": {"doc_hash": "39dac67f28646a3bbe8f41ad0b8e2bf5f9972d468aba5b39359ba5a0d8d69e71"}, "bfdc197e-4e28-4577-b534-af3696543e18": {"doc_hash": "2cd43d1d169a668a62e0f710a9f24bcc921a9d46c0a46a5ed89d7d12f30ecf5f"}, "82edaa0d-6009-418b-a99f-3c85d9df9c65": {"doc_hash": "2c400741cc9fdd0e23527c804412c42f9a88490f768f3b321aa03f9c2f970d34"}, "bf7d2f04-9c65-48ae-8e2d-0d479a80ea25": {"doc_hash": "2afe6819102e21cb6530d84223b8bcae6ded904664d771a2213bd7e7558ef845"}, "1ab936fd-0670-4b21-b3e8-9f82edb1250d": {"doc_hash": "434f4245e438fdd6c962d017e47409f9645726801bc54f75dfcb04dc04703945"}, "27ff4e30-59db-4d48-8cb8-905ecc536cd7": {"doc_hash": "07af997ffc445d4799e06b1528c4681fbd73f9358953a43c9b48e1c6d183dfe4"}, "ce67012c-58a1-4b0a-bcd7-97cd2bd66044": {"doc_hash": "fe3f396985ee59cbaf5a6af513bbe9d75a67ec3d4b9871a0e287c0e445bd9c7d"}, "f4c8f9c6-8130-4f8d-96df-119ae5b7a550": {"doc_hash": "a3199c81d5410e868c45566334724563694748e322f73c6f044df61b2eeba3c3"}, "459cd465-f5d3-40d5-b68c-c6996dea8c9e": {"doc_hash": "c396d2b8006a712061a9591479c75704da4ffaa223d8571e788a278ebee9f69e"}, "2c06b267-333b-4551-bb5a-8cde9ee67f9e": {"doc_hash": "b82822ab5ee32d97913c922bdaa9636005948c100cca95c21728c5744810f96d"}, "bb70b768-f63f-4b78-9d08-ab445e61aa19": {"doc_hash": "57de7c500c0d9720a3e90939a2722b5dfc9040429f18a7f83341ba298a62bad2"}, "25e61af9-90ad-40d9-86e8-72d1d884ed30": {"doc_hash": "edb4733d9d160cd3ac5a8fb20bfe1915d76704fa7726a16253f9c0bfeec351d7"}, "2232d54b-fe21-409d-b988-884df8cd7a3f": {"doc_hash": "1b13897ad00bff979468595fb86a1b5e46689e3ecb99295e1e7d22edb9c5d47f"}, "b6c57d80-c96a-41d4-a91b-b76179187713": {"doc_hash": "645ba5ef3b1aa7e338572c531a342b57fe7df03d5e547672867b8e738e7c5346"}, "c2b93a86-5f4e-4e8c-ac6a-ee38c4d97f7c": {"doc_hash": "c2dde0c2a59046567c0e83f062c1b594dd5be316f4eade0cf774e933ebc66570"}, "788dc869-d6aa-4ab6-ace3-c8ba84c86628": {"doc_hash": "607ef9392b7b12c8a05dc23e8ba4110d057448db7af94619cc2b811327250122"}, "4894eab3-11c3-47be-a904-d8d576752c2c": {"doc_hash": "f252cc5f1aa121543b96ce42a8cec5ecbd63d227a3b1c4e7796633acb640afdb"}, "cf06a922-5712-4070-93c8-01929b0ac542": {"doc_hash": "486300cecfdba15da25c051ec580fce76610c38cbdc7611c418d9809d427c75d"}, "b45d38aa-bb2c-496c-99ef-f8457871658f": {"doc_hash": "60d83e31f78a1d863733c72eb9875cb4f6e3a91e10e5358a898461fb1612400d"}, "9bd22418-6b1c-47f4-8d08-28ef7c53fc93": {"doc_hash": "d4a69c9fc8bbf58ec1ed2c5aa3009e31891a71a1c13017ace9678c9da6967de3"}, "e6ab48ef-d664-4c32-9e95-765106d49757": {"doc_hash": "9d9b2a2df683a0837299cf12902d9703aefb56b6c6aed52f1e6a327b3a581f8c"}, "4ca78c9d-1da0-452c-988f-e7e66b316c43": {"doc_hash": "f7531311410b2e23b4ef2ab2aff52baaff8de0a9d7fd86db1376caa08798f33c"}, "870976d9-1def-4482-83b9-a22f0aaa212e": {"doc_hash": "57a1df08b0017aad8a186a6d22c745ac2ee6c9a11c77afc36cd73df98d75d16e"}, "910fe6ff-0263-4c65-ba79-8d21492f4d4f": {"doc_hash": "851ed5ef5f5086bc68e3272fd134e48b708ccdd61ebba53ce7580f4631ab3eeb"}, "9ab728cd-d07c-4ef0-80ea-a47c7abc688b": {"doc_hash": "f5f36ad90a79d3650171afc11f31e4037122fa298e006c7f9d38dc5e852cf4a2"}, "e0f24310-b527-428e-9b5e-8fea01aaf08d": {"doc_hash": "86b7071d477715575927e72679de6dc0080f743c18eef2a634cb327be8edc3a1"}, "f9b30c28-545f-4a11-a698-207e0fcc3ef7": {"doc_hash": "a034a11aed9eb33bb590bf294730ef8dc2c2fd6d18674e4e176cdcc3efc8b2d1"}, "b427bfcb-7e64-4592-9e42-cd8605e9f5ea": {"doc_hash": "0cc7c4bc57f01694303387bdfccd3f5fda415adbfb8c23a34d231cb080bd2841"}, "63b7e32e-3e71-4f5b-ae8d-6099fb19c4be": {"doc_hash": "b28d80b92793797c8672b5de24709a2a2c9598d340cc448e336229ec3ea6ae0d"}, "a7af9eae-680b-4350-b5cd-0ac616752a4e": {"doc_hash": "6f9e6f526c59ed36efefb4b21be2bbee470cdfe5a0da3e76827bf97656c74c2c"}, "3ae38392-e74c-4f7d-8d91-31e8ccc7d517": {"doc_hash": "b6acb83a6d45a71af12bffd48f47d27b0964e34ed81bc11e704012356b4187cc"}, "2cc5d03c-7358-4b2b-98a9-604513671303": {"doc_hash": "86d1e61dd536897326b537e8fbaa6ceebd54c2f80932d7a1e5fe2f5378af939a"}, "3b9222ee-558c-42d2-8143-d3ea59607c98": {"doc_hash": "f81e56f9f463c59caceacd3bc12bc8e964edf806a0ec81b89db4be0908111ba7"}, "fed68388-0d03-4158-b3b9-9266133e9da3": {"doc_hash": "98af57bb1355c9d9aab1520e38a5c333e77ada4e91c336c7f643c691e628fc27"}, "048449a5-ed5a-4028-8250-10d77b9354bd": {"doc_hash": "87a516201bca384458f8f605a1762af7fd677e9f14798bb803f22f303af73143"}, "604ccbee-f179-47b5-ac0c-c9e8f5311933": {"doc_hash": "d288a25da330d2fadfb324ea2a54518f144d6f34daa1d26b30778d969338e405"}, "453475c1-22ab-4079-9fd7-698e6aa1f40b": {"doc_hash": "b7c89dcaa55240629d4855d1ed86dc1198e419be446d48690d85b420c5bc0deb"}, "9d26a983-7037-4c0f-a42f-9f7419a3419d": {"doc_hash": "d29a7137a303d4081e1b9f933edc2264adec75621ea30f50901d3ebe3c350586"}, "7f8b43b7-d4ba-4216-b925-ec9c5e18cdd8": {"doc_hash": "55e6558eb682d8883330bf4ff512bd859af44837a5954c508ad86a68ba6770ff"}, "cca1a4c7-5523-4cbf-b7f4-10c30536e55f": {"doc_hash": "3bddc71db07b6df07650b714fa8bc6798b09df91565368758c49ab43ea661db7"}, "f51afaff-8205-4a60-8242-e7f60ecb32a1": {"doc_hash": "2a9621d282103cb70d2bb9be229bb21243101021ec6da0d09f75ff2572bf242e"}, "c6c3e174-7c16-4d87-8e5f-fcd8243a3214": {"doc_hash": "b98743a4b4a9925593f8c968747beb46b7400e8d97872f06d8129aaf561bf108"}, "cc30f599-d922-4d87-81c1-2b2ebbbe6b10": {"doc_hash": "9cbf7de81e151d9c55ba62bee94d7e6323a4d54a8ac71e2237c0b3d4423f49e8"}, "a4a02a88-3804-44d6-a57e-bfe35fb02f38": {"doc_hash": "75333149ae60cbd551928582454fb3ac0aca5d161937ea78816b65a59e41a06b"}, "2c9629ff-7849-47d8-b660-2e2d8dc78fff": {"doc_hash": "ebc5a818190cff43ee18570e6875bd602475c97ce60ad3f7a55014ef542eee69"}, "7b7a60df-ecc6-4e1b-9a1b-8f69fc5afbbd": {"doc_hash": "50a022987c249358efcf74032377e784e3dde38ff9ad7cf3b135e1def2783443"}, "f57265bd-f095-4865-b256-6e435c3a1d94": {"doc_hash": "c6e0e3d71f93122b14bd4eb31e440f34a43d25f9dac8fd592b54616fa99117b4"}, "eef3d8f8-57f7-4d61-bb31-5ded24e2deda": {"doc_hash": "e584b3d9a686ad871bcb8127b7c261a475099d4eb2d11e59f5969a57dcc08846"}, "fa478122-32ec-4142-9162-58bc685e1031": {"doc_hash": "eef217024f4d209cc0a48f2c0f1e45e5c201f5eb0e44f1fe26d6b0d7c81238f9"}, "a97979af-a588-4488-882c-e6842ac8e646": {"doc_hash": "080f312a372e0523f97480a0d26848c6b9b013b39e740c6e8ff500816368ad47"}, "e7cacc65-f415-428f-bd14-9c463d5f2f8f": {"doc_hash": "f2ff3b31ac5350a0b293f0c2cda26bdeec321e10d5561f9940913db6ef0acd37"}, "50f6d20c-f310-4ea7-a8ef-e7e9537c9ced": {"doc_hash": "9938755407e8a0d371c9cf6f8e390b5dfab32c3d6c89b228363eee32ff6aa902"}, "bb95c842-46f5-46d1-82c5-4152f7a84997": {"doc_hash": "7f10152f665e1ae7a158992833b3ded490a41e4d6c616fff2e45b5d594fb4816"}, "fb3222c7-efdc-47de-ae3f-454b32a89ca9": {"doc_hash": "652e3d8391d420f90852351b089fde3fdba5b3f37e6c0c3d8e1674e3720f5bd8"}, "571cdb73-7846-42d4-a51d-86602250cfe9": {"doc_hash": "46a026adb52619d7908712cbe174b16c7c36e9d41380026ef38f179b1585fbd0"}, "d192ee94-a50e-4a0a-baa1-089be360b61e": {"doc_hash": "347a81de4d5f1db0cbec4208808a89be47c71d4e547d28387653f517fad2c6e6"}, "acea8140-f6de-477d-8176-a946e6c14a08": {"doc_hash": "8beb104004a243236ea73074dfe0e2ecd9d213695a34b0c9820c0298b0174e0a"}, "f89473e1-eb77-465e-848e-56496b77dbb6": {"doc_hash": "2f1da5183f5f70506ef391b988fa3ee58b84fb1de99c3cb2b7360c955cec6cd7"}, "f8203ba5-86df-47f5-9785-620c0c5926b8": {"doc_hash": "ed584180641c086eefb54a07bda6131ff1a162b618222bffdeb0c7f280af928d"}, "a553f9ca-c533-47ed-aaa5-9eae015a2073": {"doc_hash": "06234ecfab5c4870c2e0c2e846b558818ac5b25526e53961d195af3dff99b397"}, "9d3b8054-119c-4fb9-804c-66d996f5c156": {"doc_hash": "ff20f537894ddd4ae53bb808dd16aa3a88b3aa19948de38c545e2b7f567543d7"}, "8a64aba1-b43b-4e69-83ac-2217c2f5390d": {"doc_hash": "861907d4fbea353a9102b8b9c8cb0a897af89e3a4f0048fdba21aaa33e7c9b6c"}, "be388ea6-f290-4e47-a484-c3b084d24f56": {"doc_hash": "d24117ec5da5fc0805b858f5d24bfb69bfbb480741b7378f059fe35d56a8cf5d"}, "525316b8-d3a2-4784-999f-20efcad1dffd": {"doc_hash": "75a20a4e32d576b704fe09e2099ce3fea06eeb80118e3b234a4548260f482861"}, "7867b23d-7c43-4a11-a119-3a385e296338": {"doc_hash": "ec6b94d3a894a84d015f49bed477d046faf2462e13a456e0c963783bc9b5536b"}, "4c6e20e3-26e5-475b-a5c7-56a3fccff15e": {"doc_hash": "61eb92871cba03950a9259a5fb16515f8b8b78f8cac1e3fc2920404fb9eb501d"}, "f35fc6cb-c645-4a7a-8b48-fa299a8673b5": {"doc_hash": "5a65084904d23fe0bdb70f19ed6d18f3b78459a7ca695f5b516c79340eb97ff9"}, "82b9c6c7-1985-4bd6-81c1-3cccfddadb88": {"doc_hash": "c3af3c120e6258c8c85e9c27482398341d7777e1b00c46dbb383da732f660bcf"}, "92871670-81c2-4ed7-8537-c1199f8dc0a7": {"doc_hash": "9ec0a3fa1c2eaa657557e047df9739906c2b803aef194d263e5916be568eb864"}, "3b0cfc57-2ee3-4ac8-9169-0c4a3c7e02ba": {"doc_hash": "04b3f434cd30c93bbc1b11565538f613daf27ddbf525cf30cdde76493e37f1c0"}, "310ac328-33f6-4167-a0a0-acc3e45618cb": {"doc_hash": "9eda48c7285f3ec35cfbf87b64076052cd02443fc792f3de9627318df38ff196"}, "54b67631-f5a9-44a4-a73a-e07e2b549190": {"doc_hash": "ca7e1173a62bad26cb095a10b135244865df73096a1e9ade13d8b644a3f5a9eb"}, "15d2b3cb-1bed-4a17-9e80-8d72d5117695": {"doc_hash": "90831275093af62e0d80935d5d25a7d98ef5215118295498d22895ab1fc0046a"}, "7348f286-aac9-4e72-bdc9-180bf667e6c5": {"doc_hash": "fd84e4f3b8af0018db1e429312640bdd3a39d652d0ff128ff4ec12bb84c0f53c"}, "d24a2c54-6d7f-4ff7-b61b-868608d64163": {"doc_hash": "4da577eafb924fdcc55ab3ec61aa560e10c219dbcd3ea3c432a0e2cd30fde1a7"}, "6ab7ea70-e095-4b30-880a-6c6c63d2f041": {"doc_hash": "ae4094f07af51cf3100f7a9256eb44746f1d2ecfa13611101594a4d2a4d72c18"}, "797252a2-808e-46d5-84d7-4da42069113b": {"doc_hash": "c41db57e75ab84bcafb187dc523e16c8d9e8abac64ba825fdcdccec2371bf8e5"}, "1d86a95d-7932-4aa1-bef4-043f3d100f8b": {"doc_hash": "74ff6a7904c368656ef4ac0a2cab248ce70bdc2bdc66e2ea880a1704dfb0e772"}, "67e07487-7e89-43fb-ad89-676aa08cf609": {"doc_hash": "c4d0866e4e9d9371a5e1192dd15035ce89d34b6094af2b9f942e2727ebd0fd0e"}, "49f9aade-bc2b-4a28-8e00-8ab132d641a4": {"doc_hash": "67d6359db0fb471c51e113f7eb4c4cd3443ed398ee9883b337a084eda6dbce8d"}, "0af0ab50-4188-48c9-ae0d-cba6495d22fe": {"doc_hash": "118e1847c4cefbd6aa3e79ef48432610316161cf7edb5647f3df37b2854777a6"}, "2ea0e623-a281-4f09-a476-dd3dd05ca5ad": {"doc_hash": "eee4307177b7ef32bd3312963ef48490c588b2a475a023b60f9bd5f2e65c5128"}, "8c63038a-9d5d-4808-bd72-f0ef8fef0ed0": {"doc_hash": "8b06ce89bc841ac368be8157a241763066236ddd2c00062047583f8efc5e9e46"}, "39bc86ea-2e13-49dd-8280-334798d72301": {"doc_hash": "6c420fc2c62e602e9b5a1c57d4e6b993e28ccbb7e1e2e49b0adcf17ccd0d6111"}, "f9440350-11a9-4ae2-81b8-20a22e6264a1": {"doc_hash": "3ed4ef723bc20fec26f36e7977530264759078c613008cc7d5ece4b48b8a76a7"}, "5f98632d-9383-4894-9b61-367ff42b621f": {"doc_hash": "76899365a1142dd99ceef818b028cb2f3c12a2546f63c5d80301f483a5ad6d1e"}, "6bba819c-7663-42be-9d31-6302e7461906": {"doc_hash": "49f67c45a2ce792b23b12d05ac2075700d3fb865cb1c6eba2933a63cc904a9b9"}, "d1d97c15-e4e1-499f-bcfc-e252ea18fc42": {"doc_hash": "bb97cbcb33a965efc77cce511302431e3f5f987fe798da966923f0534b3561c2"}, "1bc7d3b1-9cea-452b-9108-0f19b8f9c984": {"doc_hash": "a5d8e9b9f62079141a6c152dc10b8477c7c38efc2ffd3714472ed1ab0f289215"}, "3021c7ad-f786-4f4b-9efd-2ad39a8ad5c8": {"doc_hash": "e93c28e33979ee2e6e5f78f41a97ec79e5930c022da4193043a068b9bd2bda20"}, "d1f57fc1-b274-4e45-b85c-3fb4c8ab6fed": {"doc_hash": "db875792654c9dc8b94a5d2444738bdf78bf61ed63434d593f520a93a37c6198"}, "2e6be556-9731-4511-be0d-2e036e00d46f": {"doc_hash": "661a9d6cdc1cde3e79b4b3d2bbfac2c70cd4273d7aa69e69534d87114dd9546c"}, "b01394e3-d6f6-4907-8acf-1abb4a4f4a97": {"doc_hash": "ed7b230fe550617a362616604456006286721012c67aa6b5c233f2c516a546bf"}, "9e42d731-4f6a-4033-b084-ea5ec14e608b": {"doc_hash": "013aee13102fb8d032a5d45b5435c62624b04a3258a39bad29df554f50f146a6"}, "d7e18ab4-97dd-449c-8cd6-e6d5b1bcf2d0": {"doc_hash": "aa281eaab3722f1f4966890a1f9d30e4441b8c24e38af72b1fc1927ecf776de1"}, "dfa72433-4f4f-4454-a4b2-dc8d1bfd2da3": {"doc_hash": "02975d48bb954a4177825018f5d49d6a9b26f3b70c4465e7081c80f1219a7da6"}, "dea59d27-5283-4e0e-acd7-011a195c0784": {"doc_hash": "6b5a21eb9ecaedc13db73bc203f0938b5616356836fe271c361d9a58d992a6f1"}, "391f486f-4bdc-4f78-9d89-7e3ff97ecdfe": {"doc_hash": "086141d472abf1c3c007eb509a15cf32dd19b4d57552461f5ed2c3afd287adad"}, "b1e65a6f-8a8a-42b3-a672-11e866dd044e": {"doc_hash": "b2e0587f182dca2144d6adcb22b92cd17d4dfb1e5b8caff16d3fcb349ed65121"}, "5606388d-5825-4663-bf8f-9053ec9133a0": {"doc_hash": "c2998b9d69cfb8dec4dff9b41b7a3ad5732364a2c6cb46f849cb62c802eceba1"}, "6abd5b79-8093-4011-a3b0-55fd79453203": {"doc_hash": "31de7a26bbc16300ed802e5de557a581fc3747c7c5c2a370aa4c522005d7e287"}, "28dcef94-1e8e-47e7-9f1b-9f3a27e125a7": {"doc_hash": "799c7d95b43ebceaf6c4778717bfe2a53ceaee3a0a1d95a8b945fe1d662121e9"}, "87591350-f022-4ac0-81ef-60e14c428448": {"doc_hash": "a90ee77a48935cc8998b2dcbb373392c884000b5164efb46ef09ec7a95b352d5"}, "c6dc95d3-f41f-4d28-80cc-13a9f4010cb4": {"doc_hash": "40a7c830d180a65087b1900ef760ddc54936034bf3cce085b4890ee7c47803d1"}, "45802596-7803-49ab-9917-3972aecce78a": {"doc_hash": "f34e1a20167f2b3561d538ec42f6f3745d1c1ac0588084797f0fb4efbfb1098e"}, "a74ea41a-c46c-4bf2-8cc8-c3b8bb9d801c": {"doc_hash": "e0b5d03e59e4433e469723f49f8fc8c2219187ea5d1300b346839c47abac2bc3"}, "0e81483a-099d-4bdc-b6fd-7ca72394a087": {"doc_hash": "d97b6af7d2ef27d8a4acfee4af945925734b4ffa1434c2332da50497a3ee4115"}, "aaf83d6f-9ba2-4f42-8aee-585cc5d3381e": {"doc_hash": "882f9fbef875424c23c38d2d5bca8d233aa83b75e5a645f53b3a845b4b565357"}, "6f9b80c0-fff6-4485-bb1e-9bc366bd326b": {"doc_hash": "7228e9ab30f49a70d326075eba382f9596b358af1c127c3bf0e9b5613bd3e771"}, "384d1a5e-209a-4203-a45a-6d8102a80f0c": {"doc_hash": "985ca5894395706fe98ffa9a1eb0713c37fe10715d5f51df5473445a34e1ee6d"}, "d0c6389f-8f28-4210-9f91-ebacd98c4073": {"doc_hash": "494f9bb647e14c537a9b40e6e1642e2a68d2497480fbb41e70dcfe01ba0ca9a4"}, "393b5045-8bbb-45fc-bbe7-058ff8097ebf": {"doc_hash": "0748733db0370a33a941df227ccdfddc2b8eb8da266833cc602da29b9db23373"}, "ce334202-b69d-4c8d-b0cc-c420f56e325f": {"doc_hash": "149965cd3b10d28d56a7ee85d2f98c37e1801bd05cd67cc0d684a0fc9f523ca6"}, "1b28642d-651b-4ff8-b654-21df7998ee86": {"doc_hash": "1fd91ccbdf28fc18b704cfd4cba8d6528258cfe4d94016fe92deccdff53d7f6d"}, "e853b76d-e531-4658-9042-3bc62e84c159": {"doc_hash": "88a90c33eef5956da913ed1d3d3bd0d4d967e15589097e987a2dc3e142d5ed77"}, "95cdc0cb-6a39-49d2-abb6-39025cffd91f": {"doc_hash": "9c5788c356270283f380accceb7532bcffff6ec234660f4657ba9d80971cba00"}, "eebeae87-784b-4417-92b2-72fef430691b": {"doc_hash": "9dab37e20d8d5798d8a39a6b19a7383defe29c7420985a0cbe23c4f73288c959"}, "627a2819-f2dd-446e-aae7-2590b6005596": {"doc_hash": "bdfac669901594f21e8da02a600e38323dcb213005e9f7ca307215a15ec0e448"}, "559cda98-c4b3-4c57-85e7-45b0c63acab3": {"doc_hash": "16db2bcf39a6072e08234f8e8d6df5e10d28d4c4a067487fdffb3deda6f093e9"}, "899df535-f2cc-4bd4-bb96-f800107bd934": {"doc_hash": "11267abcbe4d69241f76ff3ffad850d1cb601d36db4f6326c9b22df6ec424669"}, "43b73e6e-bbfe-4172-9226-41d015ffedb6": {"doc_hash": "bc88e9b47c7fd0b7e1866f84a107f9f0ac18fa0a900e22f48e67c01f1d220c91"}, "5d317982-2573-4c8c-85ab-cc7193a0af4b": {"doc_hash": "1ffae7ef6ac6590ea63e3cc791aacfa0dcb25e0fe2d1ebf4543588dd47a4c116"}, "22766c9f-4191-4a86-9c01-4e623ffadd0f": {"doc_hash": "119373dccde9cecd43e5a5e77c4021a26653b95d2070bc3f93392e6d28d0cd6a"}, "d6cd180a-02ff-49a9-8fd0-ed74b0ae6fac": {"doc_hash": "43dc4945f433b23d7a547f70a4b14297ed738b7aad12e3b6c311ac5363d8f19c"}, "ba193267-8eee-4536-999f-018b85402fb9": {"doc_hash": "6749b1338a3cdf20c4dd833423adbb130f7bb63fc64baac41cea43ad38a1fd6e"}, "a3237c1f-0c80-4540-be94-ddb060d208b9": {"doc_hash": "e6af475adcfbc744051af98bf8257ce99dd90ba3be048f3c2a21cbb9876149dd"}, "68b36f79-3a04-4d14-8b3b-c2ae6cc43890": {"doc_hash": "5ba058a65659d15111c62b6c9dfac1857d35c6913ff03aac9ca8d3bbb3ef3f60"}, "6d01ff0a-9927-4409-a95a-7008d4c8970d": {"doc_hash": "6770ca991efcee6f4404e077aed63a3fdd9b415f3faadeded3b416f3691447a5"}, "5c96bf38-0baf-480e-84b2-64e5921bf87b": {"doc_hash": "8e374263962213dfc3e685c0da2375b4809123ee6e93c49b58748d8c782ce05a"}, "be20e1c2-b065-452b-bc38-73337825c9f0": {"doc_hash": "22932e205eab1d4a917ba51d623e4e61f3bcf769cfc9933298ec20bab0be5ce2"}, "9b8fdb3e-2131-4c70-bc05-e2ab86b043c6": {"doc_hash": "66a6af5c23ee3a451343d885ccca11bf4ddc4fedae4728be4c7c27d3f8932287"}, "867955a9-d1e4-49aa-8099-a83f4a130e1b": {"doc_hash": "48d9027a9ae71a3c6026a7ef8db7ad3f2a1dc72420d7b32b849b5570292c73fc"}, "8ff9db89-cba8-45c7-ae8a-3e5898aa11c3": {"doc_hash": "a7a8b41728baa5f799ddf6bd573edd5d204ba428201c5ad00e89743d1c195c7b"}, "a17ca399-8663-4ad4-9245-e7ed399e0793": {"doc_hash": "90556177233a816f87593102d8840303d1991bcf76bfc56ac397aac051ead0e1"}, "d5a497ec-6dbb-4c22-bac6-6892c3cef477": {"doc_hash": "71fe12453f1aa9058e26f9ea3059d24f2a8e475adcf3f1b1f714707a8ad44fba"}, "18d5e59f-93fc-4ed8-a2c8-27e5d969c8e5": {"doc_hash": "5ebd67b936f3c9819523311ca6a78c20ccd5a6946ff5d25ae69d87b99be5fbc1"}, "6f91d370-9e7d-4c64-8a09-92cf9ac83588": {"doc_hash": "4cac0c659b6de4e1c598fd2df1c556d6da40951c3ad61f728b49b298a26b34f1"}, "56b2b81a-e814-492c-8985-3d067685017e": {"doc_hash": "7ff0d3dd64ea0a6b55c4e947102749cb342912c9405c71c91e2df87ae987ba61"}, "36dea8ed-8681-4cf0-a1c0-b32acb56589c": {"doc_hash": "c5ec01c843e30be79660d9187a86234da77ddfafbd7d49537bf66559d8470be1"}, "f71883b5-f3c0-442d-9196-7873086f9891": {"doc_hash": "a81f76ee5f4a8ec8a3b395094c326d8802888452d16146373cd8695b2edcded7"}, "9bce3f71-8e78-4ea0-9987-d11591df41d6": {"doc_hash": "a76d9a8053abc399ffff4fd1f1fb1b756f798355021407f02e3ad162a40bb3b3"}, "6974f616-6ba9-4713-83a9-61de55a33db3": {"doc_hash": "fa2e0e67a7490b024ebbcc179db62804cc93989414a87e49303915c90c0d28f0"}, "8577e23f-d9ed-4583-9a54-c9f7b40c3a47": {"doc_hash": "f443991b5b2acac2a5ab461f93007b7b41f1fb4b851ef57a9e0d959567d4ce8a"}, "86212aa9-29a4-4b71-bc92-eb2ed1bdc8a8": {"doc_hash": "494c80c57daca30e35e5206fe5bd5dbe637c092fa8d93ab202efb12ff26e706c"}, "92a47846-6bbd-4d19-8773-c8a116cb3d56": {"doc_hash": "b9e34c8271b6e949a035864c4d8a571ae8749fe843510ebd64367a3555b85296"}, "d755679b-ab89-46de-be5c-71aa2e2c3e4f": {"doc_hash": "db9dd1f52b48fe262e965398c63cf5d7300aff8cbbd00603a302bb1af878482b"}, "1aa3d136-7155-431f-90aa-e7ffdf21d1f4": {"doc_hash": "d55c3b816948cabf8d42f2e2d8b7abbf3860f391ee0aa913360a131580f9eb81"}, "0310baf0-f6d8-4e66-95b7-297954d005a4": {"doc_hash": "d994fba999544b433532f5fb4f2cab3cb273b3b9be06cc1cf47c20624582bdf7"}, "1e0ec999-3f26-493b-957a-0dbf972623fc": {"doc_hash": "66e3cfff96c1c21aaf43f7c037024e997ea68061db0a295dfac9a3ee3c789bda"}, "752a13d0-504e-4de5-8327-e39b6b75257f": {"doc_hash": "58553f7251e7f4ae427cb9a07ead93e989ae73c26d3babfc2f271039b11d988b"}, "cd50fa0a-ada7-40a1-9d52-f08f68f4bc97": {"doc_hash": "4f6fdc8505c6a3378468cfd321e3447b4474a67df249bcb7f9e27386b4679e5a"}, "cd361d49-51da-437e-96e2-1107dd202274": {"doc_hash": "dc129a7ca20ba743f86595ee32238e04310cfdda9b2f1c011562b90f75f00ccd"}, "86b47fb1-3e79-4c92-89c7-d6b414b67ef2": {"doc_hash": "7f7cf1b681c58adc8b6660dbd71d48a2b00d188359a89291e341f9a811392801"}, "db542854-ad29-4776-9aa8-f05555a3d90a": {"doc_hash": "ef5369b5a2437750dd105254c4c74b4bf214c2b54d8515fc2f6f9a03983a0626"}, "04d05ca5-142f-4ffc-a750-2602e7894aaf": {"doc_hash": "e82a819708bc7eb3e6e98cb016906cc141d52e664bdb2c0763bf0f87f9282154"}, "eb2287dc-3a46-4c02-be47-6e35ab3c5570": {"doc_hash": "e732b69fd957c308db20628ad233d88bd1dda52b52bf8ce217a1d0b1670888d2"}, "373f9b7b-23bd-4ae7-aa41-ebc411ee2540": {"doc_hash": "4ad0d8fbdbe8c142e4e83070455d1b1221e90c4a1535f5b12b833d3e4e81a284"}, "4791cf46-55a8-48fb-be44-31122bd41c56": {"doc_hash": "f21ad42b3c02cd0c262ed144db339f4ee221f7a934df2c740ebd061b747518c2"}, "3a5d7777-e453-4cef-a0e1-19287328ca67": {"doc_hash": "caf7f6e40270ff997494cec0eb0a26d991c39871f027535061ce2eb5e0eabd6a"}, "afb942ba-57f1-43cb-b23e-9d05f10430d5": {"doc_hash": "3bad38f28694bd937dcb173aaad9ec31863c9c69ff1ac5389d599b3b65318000"}, "3716d104-98cc-49df-910f-759868f60f9b": {"doc_hash": "b8a7afa313ec7debb4fb0e20127663966261ea39d5590049c5ee92ebbf82a3e6"}, "fdf48f86-0981-460d-931e-96486505d1ad": {"doc_hash": "fa62029bd19562cd19c551952c031c80c31e973471948aa1baa521c4fe2f48d7"}, "1a507592-d012-459e-a9d2-deb1a9ff880d": {"doc_hash": "58709d776758fa23c442ed49da2a2c259100bb032d5522bef2e8e12982cc01da"}, "28434f20-7575-4608-9a9c-ce9ffb12610a": {"doc_hash": "4e09bfd73f69c0ed2807c475e0df5a603f632c4238a5277145546b6f4a8df2ed"}, "64ddac62-086f-4d12-a8bf-ab6b9b0407a4": {"doc_hash": "6b0f4105d0c32fb5f7f8e69d3cc89137f87cedf266a357abc801b432117e827e"}, "25deed2f-309b-4914-aec6-afd021a1cf23": {"doc_hash": "1f72e6b0cebe74dd68368778c846a834bbbb934c4bd4c45360f008e283818c50"}, "384cf783-64e1-44e3-9508-264637f0671f": {"doc_hash": "f4db45780981fca85f63740f9b54d3c73aa747437896ceec62ce15f4119e2e6b"}, "2e9678d6-3640-4bd5-b0c7-4719813050a3": {"doc_hash": "1343ca2a48f05ad3f62dd7814499091cf85d3d83bfeb17f347bf9777dac9b7b1"}, "f2f81fcc-4bec-4b4a-af07-9edbc5162e54": {"doc_hash": "ce463bfa6bc00c2730eca77fdaa7c094fcadb1fd12168f6c66f5e9416a1b4def"}, "686dd075-9356-46dc-a340-904e3e4daaa5": {"doc_hash": "912e6db85f91248245b34fafdb9c622695d7bbdf55e279fcccedc3bc65677bc2"}, "04ceec14-2962-44d4-a9fa-993a08af7e46": {"doc_hash": "2e47cc83612d46b2223e9926c46ff06842d13fa2d1bf23a54b0b722a2b4ca14d"}, "e14179fa-d9cc-4ecd-b7bd-c1637c4cedd4": {"doc_hash": "f5b6d3ed66bc9818a051e8536b9f6b9e96be24c830021262d4ced10cc569d2ed"}, "2759085b-82ee-4370-a98b-c94f627a1f23": {"doc_hash": "c5cf115557dba25b2e5ab83d41d8f414ef4ed654935a85f548b2ecc162f97d88"}, "50953c0d-08f4-4bf7-887d-0824a3ae4b21": {"doc_hash": "2bd1c2d32d142fcbc83fa606069c50094828b4ec4f7beddede9cb63b0839f58a"}, "d9ef1ab7-8aa7-4410-bcf4-999baf51a6ad": {"doc_hash": "883b6cb5332733cd17fddffe6471f58da50a1e477a8caeec2537f25034dcc21c"}, "da421034-9367-42e2-9aa9-a3b7fe176bf4": {"doc_hash": "8bdded2659d503c496d79fb66ca913f083986928c818ed64746b96847ddb4fe2"}, "29d4db7e-f76d-4d34-bcf8-79dc57b56f72": {"doc_hash": "3b94b48bcbdcd6f1d4f3305911d7925ee57e1161a644df367b40add6dba5c2c9"}, "02328d9d-b01c-430a-a14c-a556c4f6c998": {"doc_hash": "368c45d09fc73dde0626de33e809405585da3902a0ab60d28a08a57312266a5b"}, "5be59c0e-a045-4202-923b-d29c551deffb": {"doc_hash": "a4453c05e1be7c51d27aef76b8dc0f4b906ae5a79c1aa29a359db19e10457a33"}, "989b1dc6-8f5e-429b-a0bd-06f5731c5ef9": {"doc_hash": "8dc4aed30f57927d516f0d57cfb4a0eeab379659be38cd1a9c02205f1c22660f"}, "a1cb9fc5-6517-498e-bdaf-54bca61fd0ac": {"doc_hash": "8a529279ae864aa63e64de770b5fe04b49f7d10cfb75b455fce9c007e6570c37"}, "63ae930a-ec97-4d35-b853-5b2a4f506e79": {"doc_hash": "403633abc7dac685e62a23f36c85609f5139378638f7f4de0d470ecdc22ebcd5"}, "072ebbf6-076b-4e7a-abcd-5e2dfdc30b6a": {"doc_hash": "bbb5b9c94049fabb231ec0eef5a14bf21fab32c44019b76ad53a28d49dc5ddf2"}, "76915387-56f1-424d-9af2-c3b315c210e6": {"doc_hash": "77683b4dc2be06e46266f4bafa3d83f12c7d55e282a9d1f16837f3af5b1e42bd"}, "38a7f4df-1a1b-4561-92e8-8fed6878b77e": {"doc_hash": "1c59112e3457839c1702682669b00ec1639713ced044a48fd0d2427c87958d36"}, "da994521-422f-4de5-bb21-dd8076b2a926": {"doc_hash": "a6312064649aa655eb0b123fc64e463c32a8cd9fbea3503362e223e8087480d3"}, "babeccba-d642-4bbb-91ae-5f7eaebef792": {"doc_hash": "2aea31df0b255599b07d56b6c41fe23c7f97882355f039b04a4ad85efd8e5cbd"}, "2df34c46-311c-4d79-bd9d-50fe414bc64f": {"doc_hash": "0aa7f97bcd34418f59ecb614a371552c8a70a18c97ac78cde5f80d70967ffe90"}, "4bc23479-8a0d-4b3f-b6d1-2fd2091b1bd3": {"doc_hash": "541a994a3959c18afd1042778f4eeb2cc0c39bab47d4e0a1cd7de6633e720985"}, "4c561fd9-f0a3-4e37-afea-719c3b5e7b2d": {"doc_hash": "8ac7bdef01c7cd5bf57cd54185a8d99db333672d1538b25354a45694a19d4ae4"}, "d188a54c-d247-4a5e-9642-c09ddce037e4": {"doc_hash": "151cb7dc1e88499d8747597a784dda2e8a0ddfeffa227421506f04e3fd59294b"}, "e7f5bc5a-7aa4-4702-9985-21e79617638b": {"doc_hash": "11c511192a87727e75329d6b45571d391bac577579fc3442676dbf9d8f9a1a00"}, "c667707d-8b8c-4ee2-a8fb-af8f5c19803e": {"doc_hash": "6f84de2ac337257064b45aaa614064d2d715829e89bc093d7f9bcdeb578833fe"}, "61c6cd23-2f93-4bce-8fa8-96c0518ef2b7": {"doc_hash": "f6c3b0ff61c2a61129c11af93862e19c15ed7ca80fb1319da4d865f98491cd47"}, "3637ca77-dc23-4563-bd3b-b222a3925dad": {"doc_hash": "7cd5a33de90b9704595d38a95af709108e39f45063802f9a3dcefc0c47a448cf"}, "cab22d28-0866-4950-b4c1-799574fb6dd8": {"doc_hash": "c7f38699ad705bd49beb127bd19c6e08e38abb8d8765a830b457dedc6fe427e0"}, "b86814a7-bf2a-456c-bea5-d546c735ca86": {"doc_hash": "590701fbd9a807ecbf3a78f6ab2808ab5d58058d0ef7cb8bd8e8212b41f9cccb"}, "4fd94a1b-6627-4722-b12c-7a5f3dad0cba": {"doc_hash": "08e956d971e74f663b518ebe5dc1b633cd07e9995def6d1d0079882d29b53b9b"}, "af69ac24-a31c-46a7-bf82-1a0fc27514ff": {"doc_hash": "356622fedb15d00459ffead77746804d27e196e13fc1077c59eb9dd36763c06a"}, "802a1399-cd48-4005-9b58-3cd02a5e80bd": {"doc_hash": "07a6998f406276f9aeb317bb787f6fff6ff5d351916f7d6e4ba9ce6482bd9ad1"}, "19c01d89-797d-4e6b-84a8-b2884c4c8b26": {"doc_hash": "2bdfa05f10ba46911829b6c87830de4b1e18a4777932f8b5858a625ee265a31f"}, "f43f3a7f-984c-41d4-a7b5-a573bb37c17e": {"doc_hash": "6efb20cbb19022f6486bf92e69c85a9192483d3b1938f339ac89229e0ec5792d"}, "a476059f-5279-4371-83e9-c070d9b608d0": {"doc_hash": "639d1afe347e4dc137c131a00fa0d182b000f9ef85a482a350f335300dc226ff"}, "db6ee8a3-1422-4c72-ae2f-f18f62ebe942": {"doc_hash": "6251458f3783731536c3e142a371da715ef0bf58b1fe753bd5786df5364dcac5"}, "675e3e96-5a37-4131-8e3d-33c9a101da9f": {"doc_hash": "46bf00c8289c2851a1d9562e42e3f00214c3ed20e0d106637ab279da0d3a6f48"}, "e4931d35-509e-4fbb-b2df-a704477dcf21": {"doc_hash": "c6ea7d404e7fe146a797789e4ad723d3a05ca4bafd85eaf5969d2ae61eaaf5db"}, "1343a991-eddb-4146-b2b5-ab0b94d0d499": {"doc_hash": "48636ac96753209345a97ac32b5c67ac76c6a78987771a67cee677c3e2279e59"}, "639d1a44-2690-4d4d-b762-70284f21d9e1": {"doc_hash": "e7841b4cce7e1254c749ded1494f3e866940c59aaddd26d4e9abcc1f07e36e3f"}, "3d8dc40d-1eaf-426e-95aa-f2db4faae8d9": {"doc_hash": "91e13edeaf9573f753dbad53bb077bc0766f6f0603ba0a36fb53c97f3736a308"}, "85f57f83-c71a-4d96-ac88-454c34ef7dc6": {"doc_hash": "4a72b5d4c1f5e9095a4b4ab3366819c3455288473190d3b434e3ea537befb609"}, "89283646-6de8-4f96-a82f-2955ca8e2097": {"doc_hash": "1f6876a1f47618ab84ab56b52a7f52447a1d56feded84cd6d33d7f851d1cccb7"}, "84ad1be7-7555-4020-9b45-06060b679153": {"doc_hash": "d6ff00cb8e5b389996cc90513a0ab35edd4c95c457af1f152f82c1a8c95438cc"}, "c4474d54-061a-4010-953d-9f752cc5e6fe": {"doc_hash": "e2fb89d4f6f297a126464599f2eeb02048ed6924119a9713ec9e94660144d911"}, "cd60365b-d6fd-43f4-bc38-a152a80b577d": {"doc_hash": "3ddd29c7656029ef1cd1c04878c5177dd63530e0b2f9eda40d9d38188db21986"}, "aa194ec6-8164-49de-86de-2483a683687b": {"doc_hash": "055c91cefd739aa645261b5f4a44d5b31dde2caeb562d1ec98d0c8c55489e81a"}, "8d84730f-7130-4a46-b8f0-a6aed480602e": {"doc_hash": "35a02810e124d8c76559296aa44f4cc0f4bd1c94619c214bdf224e03a90693d4"}, "55b47aad-1197-4205-a5d5-ea41ba7560c4": {"doc_hash": "e2f8059f9d790690b3a0d9506fb0d722b9ed7867a8914621b139d512348a36a9"}, "d9bfe696-740a-4b1f-b6d6-27a39a8d7e19": {"doc_hash": "51fd092a6e29a1d4c4249be4f8c56216099ddbe9ff8a8e1d8b66195633aae12e"}, "39956655-c7e8-41fd-9caa-cd377905b6dd": {"doc_hash": "be0145464443482f5baf980fee811f24b0bc9fd04d12192ffc75eca2246be6ff"}, "af6898a8-d8f4-4a46-9de8-832ec55666af": {"doc_hash": "85d43660c880cdfe4e113206f1a0a507995664fd8e769e1d82cf44e90c7448d7"}, "41a080ef-1ca3-4ce0-be14-9827cc75f014": {"doc_hash": "40e7874832f0d1f33f4a04415887459c2e2be9e3fd5fe2d9d3fc8438ee8efff3"}, "17d68168-b6fb-4843-a9ba-2108f87e496b": {"doc_hash": "c275223df65f380754f228f57fe7bfcfe081cc07b19fe3f31962d748cb110615"}, "52aa6f40-53a3-47bb-9fd2-c5fcb7dd9701": {"doc_hash": "3a75c0222f040458cf3c0485b5a95968f48769ae06f5c7144d9f791da9bae67b"}, "95f6045e-f617-400f-a949-40338db36318": {"doc_hash": "56102fd47f8237be5ff6c14a50b8247490fd8843162e867cf210e44ef370b5f3"}, "a8142cf5-001f-43b4-bbf0-617b9d5aff65": {"doc_hash": "f7edb4b80162c9479b218868b602416971abe785eabdc3e96cc32ebd08053886"}, "fcef26a0-5b5e-447b-8807-880c00097617": {"doc_hash": "de918b10c533de5fddf5b6de3b704261c2fa62b0821061a5c4d9c992ec524e74"}, "beb5b6d8-5d74-4442-97bd-533acb233990": {"doc_hash": "d5e3c59a70e817aebd596f10b33c802b6e5c0bb8e574d237304ad29aeb091cf8"}, "c791b4b3-aa16-4823-b2fe-b8a57cdf444f": {"doc_hash": "685b69a2d15bbb6ae4e5af4a7e1c3d810591f3c6f54803f3883532e3f1e4def8"}, "e88032a0-8bc3-47c8-8466-b5774b2fc86c": {"doc_hash": "4b694645b390acf19046b1dd8c76f9cd65de1c770ce3417b22e1d8fa5f4d8335"}, "586b4c0d-f5ba-405f-92e0-29786513c403": {"doc_hash": "a8dcb7981c54afa1f6b92bc857ca47e4eccba91945eb88ce0a8fa4cb5d41a158"}, "87aa933d-8ff6-4bf8-babe-379405698d4f": {"doc_hash": "3ac6b4bccc13a4aa09490961d66411f17ec7b3143687a018ce722cdedae29f58"}, "3d42fa29-d9d2-47f4-95b8-a42534dcebc9": {"doc_hash": "8adcc32638589e9f63498ec75bccd12ea3ea1b1ac5c6e1b25ebe1f1376d816d0"}, "e399a74c-6cb4-4eb9-9014-f338f5759929": {"doc_hash": "03e64eaab0e9cf3227804b70d33959d9a50cdd87fe41b2682f1642c781e45fb8"}, "1a28cecb-44fb-4d05-b4b7-44a6fb870dc4": {"doc_hash": "59f28c5c538ed5bcd4d20ae1a86203bbc518b5337d38feed64279058af993673"}, "f82a5355-9a87-475a-a4dd-528ea34012f4": {"doc_hash": "1c275fdab0c453000649a09de91f6ea669984165de3d530e9b035f3c3b4b3f37"}, "c71b3ecc-089c-4cb8-9a9e-f11f92f883d8": {"doc_hash": "be2cee692fdca130f6fc1fa640c5a1d890d10cf318efea151b85825dd0c37127"}, "ebbfd42d-8032-4d4b-912d-9f7ad3dccc24": {"doc_hash": "4e561a7f71c0f9599d639d38294af41b6fc14f28ad1284e809ea9e44a09ba63c"}, "59b1ebb1-e1c4-4a6c-b4a9-041324885ae0": {"doc_hash": "e4f33de69539c3184eef9f445c7b60771b546eec33677c9d0b0fa2b37faddbe2"}, "f2520b4b-3672-49cb-81ef-ead640a42839": {"doc_hash": "96b18426c0de4e6f6e73823945f860f0ee045a5dbf4297522e32f6deaac2c406"}, "c9ea8c7c-ac24-48bf-bb85-37feeae615e2": {"doc_hash": "1c4a642566eac8f98d42961acf01336b9259d371da93a9c4b5cc7fd7b032bb0e"}, "1c36efcc-53ab-4827-a815-5e4f90233a42": {"doc_hash": "30113c168e07bbdd2fb02f4cf330e957d6ab3654dec52aae69521a50a01852a9"}, "ed22652f-af06-4c76-81c4-8e203797960a": {"doc_hash": "0b892aeec45478c847cd36365a817d402e6e9d14a15423c498abf048e3524328"}, "df6d6bf7-c8c7-4291-a22a-c5fd1fbacad5": {"doc_hash": "00f47e28529f2737c71c1178f57c28ff2d6265aa60d39025d8df8ad438d1c5ca"}, "dc60c2b4-9502-4114-8203-664b8f954cad": {"doc_hash": "d0efa751e4607152279be93e11a22471a30fde77900cd0543572bb51a4bbb825"}, "ecc0602d-4f13-4157-953e-29638fcdf5c0": {"doc_hash": "8399191ec109d90bd0c0e9f57b22a2f88de3934d0f6f7a8066ddc4052cfa8254"}, "5f50b1e0-1079-4a25-97f8-c3ec0f63cd2f": {"doc_hash": "ea8e73c07f8ad9b5925da6aebc5069273c2c05b25719f9ebb7ec4bfc7848cbc2"}, "cddafd7d-c218-402f-932b-77461f1adfd7": {"doc_hash": "dd45136b36c613df05e8f6436de88c4a499a6dfcaf14b395f575b55225ec5b1f"}, "c637192e-847f-4744-a1f8-50229f9f4c35": {"doc_hash": "78150025101f4a1fd4d2bf069c95ed3781ce1a4dafd0374c5ace86cac4d28824"}, "8603616e-e9bc-419b-b465-83c7880a3bc5": {"doc_hash": "1c7ad5ad966a3aab18f7bbaf0c5d90d07e90897a15513db83ae1f6e7b21142ad"}, "ccf2b847-0472-4f68-bd38-87e37bb553ca": {"doc_hash": "c715e43f24d3e478683e6df03bf586053e26a5de98dc9059c251a250b8acf001"}, "7e70ea54-6599-44e2-a8c0-0f6a81ab58a2": {"doc_hash": "9fc12a6073c239a3ecd7482df02ea7f91ab6f9d6bc368f6dc9ad55fbafed0507"}, "9f89d8eb-e6a1-41db-98d6-b4b791df5ae3": {"doc_hash": "0b876542f19be87c5a7357c03c62d8c9d1c0a2e37b0a47d644440daea06bd288"}, "41564be1-8004-4734-b1f0-0aa675d10a0b": {"doc_hash": "8ee1a935845c381f541a87641a2f5c705fdba384fcc87550f39ef62309706e22"}, "e23425a8-660e-4aaa-bacc-03b1dafd4ea0": {"doc_hash": "35d7286fb4b6b6fdf780b50cb400f4143aea763f2a9653b5fe86d4a9998cf18b"}, "f9186b92-4272-4bc5-bf77-1983b4d56c11": {"doc_hash": "e9c33af2879153b82c9baa9c722c3d2dd77bf48b212c7ca968c97ad7afd8984e"}, "97d5c761-60f5-42a4-b1d6-dff94b101419": {"doc_hash": "cb3d15440da4b873b2bc653dc6c375574f2a917a0780a97ce863e3d277023980"}, "3bc32eae-98b8-4f8b-9fe4-f24ab577038a": {"doc_hash": "3412c4e5960d3abc54ddf135ee99615c8b294c4c744a4d1a81e86e91972319d0"}, "6f7e7977-af31-4a9d-826b-bedb8172a0a2": {"doc_hash": "5efb179e30e37f71b97936c25728f4fc66c4177bfaa176fad15c272e6e6c9543"}, "11f821e2-3748-43a5-8da4-e97ee68b11d5": {"doc_hash": "888f5455852325f9999e8f62ae527de06d41c1db665869061d086dbab8da3dbb"}, "39d13d07-7776-49da-a9d1-18b74867d0cb": {"doc_hash": "672b5db48fd41a7be574ad60e43434fee885e9f13ee8bb20343c4cf7ea18eabd"}, "5c356724-655c-4d09-acd2-64dfd70de266": {"doc_hash": "778c81615ee8aaff3d8fd1639be9f92c1f29a50864abe422188f7b8a90b4ef94"}, "a9ccfe4c-1a4a-41d5-8903-6ae3c5a8cf92": {"doc_hash": "92f63d0bb9bdb77103bf2dd5e0972ff23acfe89b2ca6eb54bf67de23522d7ce9"}, "cdf9e6bc-7d22-4c0e-aa47-5aba8b03c222": {"doc_hash": "92888ddd7f437b483f0fa6e37b00a710315662e3fc44b430f8d4c37e28f39ae9"}, "aaca0783-7913-4271-b9dc-601bf8eaeb36": {"doc_hash": "de13869b3675b17845f035475a0966a50dbf0ea4f0c47442da56188c415291b9"}, "08548898-0030-4dfc-a25e-bff91a71ef29": {"doc_hash": "ec44bfbd8fb63fbc054bd7a5fd40b57363e9e72b1966b399d218a3e4bc5a717e"}, "ce6e6fcf-7b5e-4b9f-8c60-51f49eedc9dd": {"doc_hash": "29f0c25c1a5f0064440d28ab2ffb545a62a4c29d0582fa964973d4644e2000c3"}, "307b1555-9c77-44e5-bdae-24b4a2087bf0": {"doc_hash": "658e575f9b0c9cbc727d7baa8c2b0bfaa2b4730e5f3f2068d914f5a80dab1a62"}, "ef1ddcb1-8ea7-445f-9dfe-f42065d7a33d": {"doc_hash": "cdf1d8ca1f8f3162f3a8b860d5ee61d4f728cd01456561efab12b594fb6e149b"}, "e7a604dc-93e0-476e-9b20-7313c591698e": {"doc_hash": "bd6ca3c5f0b2220a8e501391fbecd85541bdf4aa2186f48f18b59c2fa93e37e5"}, "0b9ceb46-4872-4c4c-81c4-c13ce7ab0a61": {"doc_hash": "570e76c7efcf4f013919d94dd21efc014d847e6fbd545f52d9317a59a1c64e52"}, "c5317b1d-2c0e-44cd-9e62-450728d2ae7d": {"doc_hash": "6c1a921eed601184a760be53bd7189beda4c1a350e6db9cd83a15b7d69448d99"}, "b67fb03d-660d-4496-8bb9-ba54c0e8f9da": {"doc_hash": "621e0fd682edd3cf839908d04ad4e582a7abfcaa0b48bf29eae77b4ba60092ec"}, "312330bb-3ce4-4c24-a9f3-145e68e04a71": {"doc_hash": "f1b4942d30d999b0221c93e94253d32c746e264770bd6df08ff81191a9aac977"}, "2436adbd-7070-42f0-94d4-1c291b49af41": {"doc_hash": "6335d24a99ddbc5af8a019592020244ecf8f10ec4610772b46e4b207c130c6ed"}, "5602e206-493d-4a22-b85c-9435ead7d23f": {"doc_hash": "f255ab2dde46c8a26b67d9c9b4dd8ef8c52aeb47956e48c39db4ea43823ab4c0"}, "f9fcb0bf-6866-4101-916f-a25141e9efb6": {"doc_hash": "b1e52eb67265d9139ee384ea05d87458796d364df107ea0f655289cb2ab4e4e8"}, "3ce6a96a-653c-4c7a-bb28-f75bdb679768": {"doc_hash": "6176af443491969977f13aa6f61cc54d87ac1d39db65172949f6731f008c92e3"}, "e2331142-dead-4f7e-a970-108573a30a6c": {"doc_hash": "f042c2fb361d50b38b621e1c004116e4b667cdb1352b612d7fe900724d7d686a"}, "6c4a60fe-e288-428c-935e-0595598827b7": {"doc_hash": "8e3b910a36426d1e39f1fb1b939ba649f69dc3d99aad48c48a18c203b70dbfe4"}, "bb876959-35c3-420d-9c93-0d9acd80e6b4": {"doc_hash": "c38826d787914db26c9ab787ebfdb1e1fd88ac079cd174488fb32b60ea487628"}, "0e2dbac8-e629-4ce4-b483-8c6e0970ee85": {"doc_hash": "2fdc0eba4be36da4fc33fbcbab6702820e1b28d9a458c423b387973f36d7cdd4"}, "8f333039-6eaa-4252-a324-330b0717b3a9": {"doc_hash": "a30e764ef194b0d5436468517f8a1ad78fbf3b82d29d74ee6055f323cd1a057f"}, "6682a1c8-3c9e-48b3-8cfb-6bcc61fbcbcf": {"doc_hash": "72934fd3449b2851e457b6ecd6f53b35147503f7ea40f08c58995423e01c7cb1"}, "844f2dc9-fefb-444c-9ee5-cb650ce5315b": {"doc_hash": "21d93a23dd2c4b71c86c6b6a4e5718e07ac30ba6933af0d00d36a016936b302a"}, "6c9ed32b-39d2-4a8a-9199-ded4563bc27a": {"doc_hash": "7532243347fab1a7ff5642b77e737583a7436dac50e7e3cece859707360578a4"}, "fd3a9104-2731-4f6a-bd0d-50df9970d93f": {"doc_hash": "8f0e6232e8e0e0f866b1b888d48284ae4772d5a212bfea4ad23393865c89aae1"}, "d85c0f50-ccf3-4d79-b31a-457a1d25d633": {"doc_hash": "253c2172d5a8ff74ff052f9ad7c64c5f3136e54b30259c807ab2caa22803c992"}, "7ac0ec1a-4044-4955-b886-6a31b6a25580": {"doc_hash": "1b2142a31a2556bd4b906b31f5aafc58ec2ff08cead680ac1d174f6f1fe140df"}, "66a65e5c-bdc6-4202-9d42-6429e98793ba": {"doc_hash": "371630398705099f9c0003ee312d81bd9ee6affb5448ea13a990a2973c0b4719"}, "4f1889d1-3c32-4a4a-90df-fd457d0d4822": {"doc_hash": "6ff624084d53ccfb6c838d8b1f45d85f17aa35114bceee9c51ecefc40c99b989"}, "c4b398e5-aaa8-4715-b7f8-1463c731c663": {"doc_hash": "cf12fb295d03771c8797fdc8e37c9bdff62d85bd669a0071461ad68a14326f35"}, "7d533572-3ea3-4c35-989b-dcd05f5acd83": {"doc_hash": "3a92a3dd4b882355117d1507b6532d8d8613fcdae3ce631ecd2476e8371ffe89"}, "fa679080-6ef8-4f00-abf5-f8dcb6fdf16c": {"doc_hash": "4b9d40d5b77c2f12324028baa9c69898c26d3f8ad300d2a0925ce6e7ce8845a8"}, "06c3f6cc-09af-4b37-a9b9-11962aaa5e6f": {"doc_hash": "5bb386b95213c81b5d7bf67acdf335302c0865fb2718940d683fcb3cac94faf5"}, "95fafc13-3251-4d38-9115-7a6de292e48d": {"doc_hash": "e6d8b2b52fced5952936e3b17268e030b659cfcd0988d290f81119afe0b1baf8"}, "cd537f4b-8bf7-43e9-838e-f5ba685f5c48": {"doc_hash": "4ca103f6075f80e834aabcf4af4a5c04daf30457b2a370effe6acb7ce9fce892"}, "80d90c91-bd33-4106-80bd-fa286982cf35": {"doc_hash": "cc6fccda0ef296c90bb4064ba82cd496e093d3c72a5138e6b4dfd7f39ee178a0"}, "eb2cf21f-d1cd-4d9d-9bfc-cfa5e8c8efb4": {"doc_hash": "fca37c056ecc07eef4d36313edffd5b34675c1b1b1a7c6b3762128c9cc9dba44"}, "caee149f-edbb-490f-a1e7-dd00cedf2859": {"doc_hash": "44cbd8a97ec34cb93cb7e04f38f20aa3e391ac5615629e62feb2f1eba7ace7cd"}, "4c62c5aa-4251-4316-8572-5bfd8b29bddd": {"doc_hash": "d7e61abfe1feef1593725b0bdaba1daca57ae85147c8c42e3cd1059158cc86eb"}, "2dad4f1c-e74c-49f2-b585-694ef9c2c853": {"doc_hash": "c590805fa25282db5db6d9e696fabdd579b2569ee8bf45b7db71180e8c8cc8bb"}, "76777df7-7484-405b-a7d6-55209ee6166f": {"doc_hash": "c950d9e016e8451477cc9f12693ed25e217bf8cacd14fe009982541dbefe9a59"}, "bc186a09-295c-4b17-b298-1f9c68a1b6d8": {"doc_hash": "dc71f397ee93f136c2244f094e660f984280cd1e91f7db612240da6c2fae8ec6"}, "7a796103-62a4-4ae5-8517-51440d933cc4": {"doc_hash": "94ec4e26018a7bca78433fca7425d719513ae0865e7a04f49d6ea9b68c9ebf32"}, "9fc8cfae-29bc-4aa5-bb37-425035fe94fa": {"doc_hash": "e37d842359ef17a324ee2ceb15a646c3a2968f77a117d95a6342b0d2d8844506"}, "52ea81f5-127b-41b5-9809-3e43ca114608": {"doc_hash": "d10814559f2bc910dcf244a4ecd522666e8f63c6033ea0b5ded7882f65f62a80"}, "0dde45f6-50f4-4928-b28a-bf6b5a0620a3": {"doc_hash": "e2a6db66c3a2f3fba6c4b0309e2555bc68afa7e6e2be782cd7a9c2f7257a36e0"}, "e4403aa7-e587-4f09-8bbc-41952560c7a6": {"doc_hash": "af04a283e2d164c51cf0b8151ad5b8303e94ccfab9e8d8ca1c63fee110870e05"}, "c4e0b9b3-7588-48fc-ab1a-50b4e0c77254": {"doc_hash": "57b60ee7e056625b4d5450115850db42f45591ab5855001a0328d38efcf66c08"}, "b5f5ee5c-1c8e-40e4-8b98-3359ca415f06": {"doc_hash": "a04b4d982a7311152f743e0c039ee8a0df0824021a7cbbca50b0affc29b17669"}, "94bdb416-b2b7-47e2-887c-fbf36feece4d": {"doc_hash": "ba85eb78f3808de6fd83a2591e6d122ebe992656e5f03555cd8c294cc68fde96"}, "8e2f0f90-fad6-4c44-b776-41f9a57f514b": {"doc_hash": "b7316dbf2addf3211746bf893777fd125ad23672dd95a9d49729d926afd37cb3"}, "f25fcd3d-217b-4431-bce6-45723e3d5d1f": {"doc_hash": "a67a09d17287e3adfd00b6ec103576eb02a2347b00091cf26a3e26a08213b957"}, "43311f4a-eea7-4b94-a80a-db9a50bc9e3d": {"doc_hash": "53bbca9b760fb591eb64f59e9087395cc8b6b81983f59ea20a217ef4ed2c763e"}, "a6806f8d-f624-4926-93fe-64852cb6bff3": {"doc_hash": "a620f444f71e5a0dadf38bd0b129d3a5ca2d40c74f0e9fd0e80fbaa3949a786a"}, "3bf38434-a155-43f7-a3ce-aefa50662019": {"doc_hash": "85321cd8ab4fa1df9b660a68bbe2f33d804cd4978fa8ebdb532650e3fcd19b67"}, "ec77809c-d569-4960-b589-a95ccfffc6d7": {"doc_hash": "2b762b0ced9aafdacb84046fa5c6d7821ff95de771746af9dc8b027e96d4ad72"}, "e2f831ff-4192-4971-9bb0-07ae6ce392d9": {"doc_hash": "6b4d4f9e808e725b410483bb5911b67d2022fc021e31dbdc5a37be1d4ebbe293"}, "bcd38b3c-d075-4ab7-9501-832b0fd1dcc5": {"doc_hash": "db3fee3a080fdf2644be19ae7a36e138f7b155483b23fac7d1055468c3dfb722"}, "93316b76-8d96-4091-b81e-c9d5a7a8c459": {"doc_hash": "0330ff712ee01954380baf648d2e78219be100507e73208375480a855241b6f4"}, "b078bdfc-a9d6-41a8-9dcc-909e234fe759": {"doc_hash": "2725d0d113677527eafa970af3a4c34009cb1aabcab2d220ca7cd6d2819859c4"}, "b64b63b3-00d7-49b4-8995-67ef16daa102": {"doc_hash": "8fbb4cf38102c0f7ebaa3d5030e278dd5b6e917254bfd18dbba870ec6e13d043"}, "bfa93f03-3be6-4d7f-bdb1-88c684be3109": {"doc_hash": "e2a4c201c961876b8bedf559781070273e9d2cc896307c33a5eca7c53756cbc1"}, "2c033b89-6c9c-4d16-b2a0-996ba40c57d5": {"doc_hash": "06019dbebfea6f1c9bda5c2ed585b29abf98a7d87eb1dcbb5b6068b343b32b8b"}, "066ce23c-1d25-4912-ae85-80bc5c1a5d75": {"doc_hash": "a5738a080aebea18c6654c802f3e960cda679991e3154b7af05e8f30f5b16a64"}, "351faaff-5dbe-4c95-9ef4-40e1e3bc6934": {"doc_hash": "e40b3d670e05fc7a96c04b424daf438f352a1512191851d995b3734c64407720"}, "fda9f331-5ce3-40b5-a887-2a5ee1edd2bb": {"doc_hash": "18ec4332ea28696d22ddd6a06c2368983ecdb777146a1deeffd825130a975559"}, "e4aedae2-1b2c-4dea-8e99-bb785e5d66d8": {"doc_hash": "12aca3665babd7ca6aa5e0a589cea0388198363d5506e4fdf34e793fb879fdd4"}, "07de06e7-c88a-44a2-9016-5c300179423a": {"doc_hash": "e6cae8a591aeb134042bc1ca3290d541e01d2a47d7da0ffc639c72f5ae9e4395"}, "b5f8b6eb-52b9-4ba3-a118-5f1c826113d1": {"doc_hash": "9180fb8ae66cf15f6f571d5d0ad01cc70b31da1abaeaba28fb415e4dca7907b6"}, "c0bcac2e-194d-4385-bc43-a12adffd75eb": {"doc_hash": "f913144dcc2aea3723e19844a60fe0890710a8da709ec453a629794d43f918e8"}, "906dc49d-2393-4999-a9fe-e608ca092a3a": {"doc_hash": "b7a6d56495b4e0f2c3bd78c90731eb767260afea8c5f1d25837da5384c933d33"}, "28f3f80b-2436-43bc-9ae2-cace8f4b9801": {"doc_hash": "e3ada152bd1c0adfdeb43a3ee45e705e094915f6ec21958a83c44ea03e52a466"}, "8e7d5f38-d82e-4fe6-828b-68ed239ee029": {"doc_hash": "9db2cfd1576574266d6f1907a5567ce95d6d191a0807f291b35a88d462fbbbaf"}, "c7417ade-eac5-4a1e-8488-cbcc617384ce": {"doc_hash": "6311929d7590827dc33f37fff1f45267a3026f58859d41ac46985293ccc923b9"}, "b085394e-89e2-48f1-a531-4c9353b8ac86": {"doc_hash": "7cedbbe653ab9861b27f0b3fb3c48d04198c14aef5917b1b6382f8155067023c"}, "51bfbdb2-df5d-43af-8088-2434ef93e3da": {"doc_hash": "ccc9d20fe222de0e992635cca24468e003195993eb8a7bd87febd80d8b712762"}, "fbc61e67-5953-47a1-ab17-ab353a6a4927": {"doc_hash": "123a5e17b28c8625e1c040f4a7a316302e21e8a39042e57f3e66cff7c4fa852f"}, "668a5f84-1c36-48c0-aed7-90fa73985937": {"doc_hash": "9a5623fc42ac5893ab18d08ee4cabc0d3203d3af582dc7538e0db18332a62554"}, "9144241d-aa68-4c37-b73c-e5cb255cdcce": {"doc_hash": "b3a16cbdc68c85d7311bc1e81bb85d5fad46e8df7ffc55be3a914fb8e22bb1e5"}, "7562786a-7465-4872-8d5e-0d40e9e70269": {"doc_hash": "a80f103a6606ff65cd03508490fc0fe6540545339ff4bff7231762318856247d"}, "1f797806-623c-4ac4-8474-4331425ebb40": {"doc_hash": "3ecb9587b5dfd87e88de9b71bf53d5674e4abae43fdbd23c5ede5593195cce79"}, "1c017de8-576e-4e98-80b1-ffff03c0e2cf": {"doc_hash": "57cd76bad817d738e3af27ae73ab558bec694056adecd0b21b2460dd81cd3197"}, "69a211bb-eaaa-4a08-9e07-5ae3ad3eec2e": {"doc_hash": "bc58e31aec0939bc97ccc8418cfaa3624f42460b21523e3ed61c44ffd5a2e0c7"}, "9ed73a6f-597b-413a-bea0-70414352ce49": {"doc_hash": "e6d663e151d95de6f3180efe50a6a0108428f44d64d3fd69dc5157e6f500318c"}, "de43bddd-49a2-4336-a75e-5ba57a025b8a": {"doc_hash": "4e1bc8794e6e39eef6cb5a7987e78f360dbab12753253461660aa93cbe85bbd3"}, "5e932a48-eeaf-4a0b-921b-36c271973d66": {"doc_hash": "831f725c9193af19464bec84f3bc93a44e10cda6fb7f3ce9f59947f9ac1f0f3f"}, "b1acb276-a969-4dd6-a81c-4ca4acd26c80": {"doc_hash": "5342fb3b758e3520c4d2741bcdc9d376656605f854d4a16947b973d52869e4f2"}, "518870c1-315e-40eb-8fc1-a8c2d8ec700a": {"doc_hash": "25406bb5487899c3468eeb3bccf41900a52574adba37eab8f277eb35675008e7"}, "775268c9-4b5f-497a-988c-ce3f29f1f8af": {"doc_hash": "8c72f4abe6661d797fe99596e1c787fd84c2f691e81ea97ed2d43b52b38bfe06"}, "2ee9250b-2dd1-4822-8045-73dc006775ef": {"doc_hash": "c49c7e78389c3a7a5b7ac8183b1e0599a8a58713983ff287517ccd1d400473cc"}, "dbd2fdcf-60a7-40fc-a62c-c862ab892f5a": {"doc_hash": "fae0167689d9e4ef3283630ff8760ddeb11b40e44334d53b83804927d1d76acd"}, "78c41961-0bd1-4692-b5f2-5b893918569e": {"doc_hash": "19ab81825667513d45af03b483e4f632a303e002be29e44c166858529cd2a0b9"}, "d07687c7-f6f2-41ff-b629-2b405e711c7c": {"doc_hash": "66eeb709abb7a1533285af65ca5435fa58f59fdf218ae762a96e2bf1f76e1920"}, "cd060354-6956-47b5-8967-1dabd08afa49": {"doc_hash": "1a639daf48700b6d51636dad4c73119b00ec0062d67e5dd92fa0ccbcb597e424"}, "af4ee6bc-9383-4e3f-92e3-c49c93f900af": {"doc_hash": "1cafb40d451ab12713b0caec3236b1e8be8ed7a68ee6d84e8aee625292911840"}, "702a2475-6fdf-4385-88db-6ddb7670a702": {"doc_hash": "62dbb2a6132cbd7c4894aa7031a87acea0fbbfbed74dd93ee3dafb7457246271"}, "b3fbaaab-fe1a-47d2-91b4-2740a1348f21": {"doc_hash": "085e29b237b84252f91a6e8d034ba76bc1379124151c6715a74c7af352b3cfc2"}, "54de447a-a2a3-4208-b1b7-2cf33945b289": {"doc_hash": "09570eae384cd57cf51d2fd66e374e02902d84512a06c566c8182be4aac3189e"}, "8f713de5-35fc-41a7-a218-5bb28385878c": {"doc_hash": "7600f755d7b909a0c513c0d0d8ef49373161973f415bd43e033e5533fafead2a"}, "1c5b3905-6335-4a89-90aa-e21b184aead2": {"doc_hash": "6d4be01fae9c462bdb19396ec742645790580ac271a7f144776edcc359d87a36"}, "591cd493-095b-49e5-9315-3b2f25aad2c4": {"doc_hash": "38c0c7adea42406197113a324c882fee686a6ebafedca9029f85923ccf753450"}, "e207d619-4c96-46d1-b80a-fd95ef2fbefd": {"doc_hash": "052c9ddee49bb23f56d3bb5e0ab8b774d8e3de31c35033c1450a74cd2c5c5a41"}, "b7fbd565-1de4-4750-ab5b-f1860491695c": {"doc_hash": "9aa4f9f8adb34fa454b55912d1c8504a7cb61d3bd05ef6151de6713994f6d772"}, "34b5e631-8caf-465d-b262-742e0c7920ae": {"doc_hash": "933586953cd8ebaf9446c208e4ffb879255ae59fdeb48f4e9f1643f4645f8560"}, "273f63c7-fc72-4643-8389-e885edb86da4": {"doc_hash": "c507562690bc9e77ceca904a9f350c94894d0b61be2d12975cf7abddffc91197"}, "99d5f27e-6fb5-431c-909f-f45c6c3fb63f": {"doc_hash": "f18121dc05f078f782e3a871f4bf2d5dfd988344f6328884e617a8693fc9001b"}, "bcd8c3e8-bd49-4cfb-8f95-e72610e8353d": {"doc_hash": "6d789fd619603e9a13a267c0e556f2c8442aa451c2e4f164e39f9d00b2609daa"}, "0f91424e-c1ed-4e93-a54d-72276d74f039": {"doc_hash": "31b1e6a8a6d46abb52a9346cd0ba44d69db35ccd20a8535f76326a3409e13f65"}, "4784e008-7fbf-47a7-93e2-744ab9efc4b3": {"doc_hash": "e2f6cf0aeb906e2ce7ebcb468b3f0f0cdb520bed2093168995c06bdbc2f0e8ff"}, "33de3b59-f854-49b4-a36a-c6668bb06f86": {"doc_hash": "dcda7eb02b7b05e9856c59831c68a440c579e4fd1d5cfc5501f917797004ec41"}, "fcbc5356-cedd-4f7a-8e19-d5a11a140591": {"doc_hash": "b1f1f92ebd8fc004fed267dc07c8187b32c87c4e9ad8b7b6b0dc52c1f6578451"}, "75ee28e2-0ada-46c8-89ce-ff9e09e96050": {"doc_hash": "ce36da1d7cf4c0f7e99845414230e30f14945fab3f2c03ca5f7698499b0f6805"}, "6318d7ff-63bb-4d47-a7af-7e4b30868208": {"doc_hash": "c21f75e795ca50a5bb4c883de88fff7d35c128eb019e084d631cc6f53755c9f3"}, "cfc8aa9e-9954-4082-a987-04dd29019642": {"doc_hash": "0f5be86bcf493428c49872c754faa2e86322502cb097bc7e77e3f65d48bce7c7"}, "5599510b-ecf3-4968-91c7-00c96970b4a9": {"doc_hash": "f780eab72e1f714d5aac06171b5b34649e34a3450e2d8749dbe44005a58e8a2f"}, "ef80d1fa-1617-4653-abb6-2f0460659a3f": {"doc_hash": "1ed2c5e21b23d15ec90e52edce234bffde603694d4d4a735336a48be5185e5a8"}, "92561c60-dede-4b85-b104-0bce545ad5d3": {"doc_hash": "dbe74ca76f4a5b80c921d546e752051955b406dce56c1224d2c58125390d0e31"}, "6bfcad58-ea02-44cb-ba90-9f9c8c7bd53b": {"doc_hash": "c25ab52ef745b006cb0337e0d41e9cf7a7828f738c5216ee75b8f2185e040ea4"}, "d26a13ce-9a22-4bb0-ab47-c65a3d7d5416": {"doc_hash": "7e9c58b9512311ce518698878ef7232d80436ce81004357251a87537c3bb5b4f"}, "4e79b465-7069-4996-bc22-5649ad17a28a": {"doc_hash": "698e619b49587529f4308b35e60b6a94fb1a66ca44c431be84ba64ec92f846bd"}, "1c88a50e-629c-4f7d-9e96-4bfe8fec29da": {"doc_hash": "9f5f1f3afc9c14503eeba08e883450b2ab29db958c0e70a946fb035d3215b8df"}, "647080c8-8242-4380-b00a-cc2361f34978": {"doc_hash": "2e92c69e553a2403c67fa15d065bec1ae894864f6e436e900e457e9f350bacfe"}, "c981bd17-97d3-4a20-be6e-33f75f9864d6": {"doc_hash": "23e172fc74f14970735d9fffaf2c0a064dd45855a7bb598c42617e226571bd88"}, "4e49789b-6176-4c0b-b07b-abba86b7ddb6": {"doc_hash": "19af5726561d15107dd68bc739e3cfccac04e8113bf860aeb7af075fb7ddd87e"}, "33d17f0a-c77c-45d4-8481-5a5b63aee2f5": {"doc_hash": "1885513b842d609b36251c55e1e0adcb468b40f816f704c9abbea11feeca963a"}, "c445e2bf-4d13-4688-bd12-1696cb9b29e4": {"doc_hash": "2a0188865f5534b799d10ee9717905c0af84c3286d4382f749ddcd3204b5366e"}, "acf25c4b-63ab-4b67-b4f6-9bb181f3d160": {"doc_hash": "4fbabb32429ad17d356fe38db6ab2b051b4efe5f03f1540b2c13995c4b1dec6a"}, "5b3bddbf-0343-45c6-ba47-3bbb3a1b4bba": {"doc_hash": "b376e16240bc5db20f5038527cf374ea55f96c87d7219961251bc6cb24d14a8c"}, "3de21842-4b77-42a2-8cff-b131b5791348": {"doc_hash": "8b5d8db7f8bf2d4d1aedffa308ca3b1dcc868d0a8ecce6b22066987372612258"}, "af33cb68-371d-4084-b51d-69600fcce609": {"doc_hash": "7d38b19c9402dd2ecdfa536dbf161dd4183cf4f6db48959a94b7c2b0c55655aa"}, "093b8c7e-edcd-4fe1-be4e-c3194f96af17": {"doc_hash": "5de9579f1e5d3184c2e5915950d542519368e77e9e57081fce9b1235c58704eb"}, "bc92271a-2555-4e70-9456-6c944e1d302f": {"doc_hash": "74a9e5aa67424804590142b41d77907ddd7d0c310630f75f0d5f10bc9ea0bc49"}, "a62366b0-6219-4084-b805-8b7d7fadd131": {"doc_hash": "b69a656fcfd45b4aa15db3919f2451dac9de54344e6ee0df7357f46b0bd3902d"}, "85f08d77-3da8-4895-b05a-f0e9c416f2e3": {"doc_hash": "7b755c66bed12f2c84e13e8b4e0eb4a66d6d46fef376c5cc222bc556c100b982"}, "b3067a66-7d00-43d9-b68d-5592b5773fed": {"doc_hash": "64b3d3e7b6cf594dda619ddcbcd17f3db71b38f500bee471424b1a7e3f65d327"}, "0d550a27-d889-467a-bf1e-37af15ddcfe6": {"doc_hash": "b68db1fc69964021772b148c40eb407e0214fad2e0330e20abb23c22d09c887a"}, "495e66c2-db43-4596-84b4-6bc1636dd68e": {"doc_hash": "da1d3a5f406adff9847ddd6d7404e230029c9578d4b55d6e1f489e7e61cdb5a4"}, "03b53952-123c-4e85-8c33-cdc4819d8840": {"doc_hash": "9c2f4d1152b69144f629eaefbe83b837854b700837edd66b981a24ca187cd681"}, "057aea98-61e3-480b-848f-2fb642f9fa03": {"doc_hash": "330942b1776f1d98d31b31583e064d7bddc8d9d035a27ffc9770babab55cc8a3"}, "22afdcd1-377a-4f8d-97df-1a70dc9602d2": {"doc_hash": "268acb7b2d20d707569f266096cf3ec202c8d5f1ae5c052aa1dd49a1f50bd0f2"}, "3bafc29a-1bb8-455d-8d26-3ba4acf2d04e": {"doc_hash": "b0fd864656ef0d4592b369f910fb23e54a87e968cfa2c17b1d38b9931339f52d"}, "0a9b44a1-a61f-4652-a24a-2a0557edf47d": {"doc_hash": "797fc825602db6bb0cf3f1ad33bbed4e1ba595b62000c7b95ff4c80739fa458d"}, "0351e24b-0280-408e-a308-04f61dec4085": {"doc_hash": "9d9c5d6504f3a35f8245e11b82fd3b501bb20a5f309855de077a219c1cc50ced"}, "ca2084cd-b41f-4bd8-a9eb-390150ac1190": {"doc_hash": "eaa242c0155b07b388c2069017a75c41294ced6e001c66ac688d1c2d703d9286"}, "b30da191-a3fc-4651-a44d-a9c667c22198": {"doc_hash": "3ebd7952092d39083c5fd0e1f538bff922a7d473f4ab6c296d5a4a3879b172fb"}, "49cd0ed3-2ef3-44c9-b4a5-d4f9fe5aeb99": {"doc_hash": "5643d99d54e31617060b54210d8467bc94272ced4a5aa1a223ce0fca73729f43"}, "4c9b256b-9923-4f2b-b0b7-126ce880a277": {"doc_hash": "66a4ac93f50106f446a8458eb8f3dd51e60b838cc28bf88e3c13a7830325df02"}, "fe65919f-b312-4472-b9a7-5a9974f9b36b": {"doc_hash": "046780d6fd6d4c036656851d5fbca90f6ea60fe6ccd4e3c6f46eec98b8740013"}, "464102fa-b32d-48e2-83cd-76105518917b": {"doc_hash": "dc14e786a102b21a4a4511665c90f15d7e87778e03122f0e318dc838bc9aeafa"}, "53f014fc-b6a9-4ed3-8cea-88e007ac41fb": {"doc_hash": "8c516bc48722eac946aa8be6a5ebd88464f8f3a995bd7cb0ae02c6ae6c7ac21f"}, "b0497449-4d2a-4a12-ad8f-2789d83ae807": {"doc_hash": "4ba7c9caac68f56b6683629fc7da667514360f4b8bf78d83d28695c371a72eef"}, "f18c9fe1-2b1e-4525-b081-56d3556a0a98": {"doc_hash": "27c92780c715cc56e6e26d0829be3234df3f9f06d05ede2625e1298c93ec686b"}, "07a4f12f-6339-4035-959d-3b789afc5323": {"doc_hash": "1dd70897843d6b98e61140cb468a3bcc9a15e9489517a38e07d726bc2a977ff5"}, "b69d30ac-6905-437e-bd96-1334008a552a": {"doc_hash": "aba16b18984cda6b9098ac3b76af42d91f13d43d535c177134b4fa0595ddfd42"}, "a98c75ab-6011-48f5-918b-eee7d89a6e5c": {"doc_hash": "1714599ff65f52653dbe43186004d7ed08a3fd0ae01f35e7f2d795484a9d3ead"}, "01503144-2d64-4a36-be4a-f42903d97768": {"doc_hash": "939adf419b7bdb3c25e517f5f972eff2d540fdab917b2b63f5d1e715dd0ce0a1"}, "7724cdbc-3541-475d-8b52-1ba5e315e1c7": {"doc_hash": "06d429bb446a226750ee05edf0e1f37d6824b718947073498714f14bc019209c"}, "d40e2c66-427f-43dc-99ee-2be2b750e8bf": {"doc_hash": "d8880b430e4e1c6ee5e10a502e0df0002b563dff0d2757e0ab477d1ff73612d8"}, "a111898d-d2f6-419c-975d-72f932b98dd2": {"doc_hash": "04e93086a4b971b067f680238c07b7b325014e9d638f48dfc93939cc85c47aca"}, "b95592ed-5b7d-46f8-a759-e9bc99fd021c": {"doc_hash": "cf86633c29df29fee55d2cf1a4806b2324da29738368e53f9cb1d8826d59774c"}, "b16f29d0-c044-46ff-8091-bddb56717138": {"doc_hash": "9984d78df03f06a5e4f12875e7a6ff8b449b241de297453c1981b757acc837de"}, "36ab6a7b-1a2d-4c73-9688-95bca8520ceb": {"doc_hash": "fd2de4c6abf4d3c2c02e1125008d837c9e21c2bd87f41eff24e1dba7d5c004bb"}, "3cba695a-8130-4789-b5e4-8335550ed03f": {"doc_hash": "02c160dc79bccb14f6ede6fd733fca5a99e141714d000b1e15a8e81d93afe9cb"}, "765a1989-ad89-4cca-9793-29b0db748bb4": {"doc_hash": "838ba6909aede41919e68514a23a51e20cefc725d80bc8f599f16db92611f9b6"}, "97a0aefc-2512-4357-9725-fb60b4b24915": {"doc_hash": "bf5f802189b9068f2ec236cc1162687a02de1aeda47d7aca22102f4578c998fc"}, "13f84fa2-9828-49bc-9727-b7f76491459d": {"doc_hash": "5e3190cc90934ffaa985b94943f21ed64cb0d180cdafbbc51fb81cce7bc81e47"}, "a23708d3-01b1-4792-98a6-accdf6230a0d": {"doc_hash": "539a8fc145d95968fae03a18aa1305723c8e4263051c84c9897c4716d3b8a3c2"}, "8467ef40-f77b-4c4b-9f98-e597ce30cb09": {"doc_hash": "0b0fcae744f51f60db5c16e55a8941f4d05c86add69d94131ced5b53d4515c71"}, "02683187-7f56-4101-a593-d2e4aacb306c": {"doc_hash": "a73a2703f74d92ec95f841693c6e2a180b41d3bd2e51deacc79ec885a2a123ed"}, "ac18d023-1990-4fc0-b763-cea32dfb2456": {"doc_hash": "3c3e40b5911cd54456b95c46b1148193d5138037e04b9639b90545221ceaa84c"}, "787db609-32c1-4a63-bfb1-66ec9f4bb595": {"doc_hash": "a916f43a5de61f030c1cabeef6577968cbb74a5922aab0b7355e3fa72754dabf"}, "0e736a53-f85f-4753-8f6a-94c0eea906a4": {"doc_hash": "6e6ea0eba51e7e0b807e7a96af703cd8e95cc2bdb02b1d834aef654e5724f0d8"}, "f3df090c-21d5-4383-b5fb-3a4eafa40f34": {"doc_hash": "935c18b9bc1447908e18562604467d0057581cda8cdb873aa1d7bcc7e5eecbdc"}, "3d1bf235-ae8a-45ea-9892-aa3993159787": {"doc_hash": "42da7b38a643a1ed5ec646cef830a2b6e1c887e44a574cf0321e64f9ae79e321"}, "683fa2dd-5162-49e8-bc10-b8bdcadd1559": {"doc_hash": "df4e81dad9c15b89f74b2f780d8421c053d54d7da34ce4f4e5ea966ed1400471"}, "16a92def-febc-4d7b-a3f7-f85f064ae547": {"doc_hash": "0d3ef1071ed224beeee152ee1a959d96185f20812202ecf1aab62d7f77fdb00f"}, "35cecdb6-973b-4c34-9478-584d81a4429e": {"doc_hash": "ff24ecc0295c6583f1dc01cb95c3a3f727ea07d219621755f82d608ca5a1f8bc"}, "d901f3e5-a9bf-40ea-a47e-a1b8c840456d": {"doc_hash": "9a0bde72792533d24d0b5115f55277f1e514e1a9c695716cc032d4c35842bb03"}, "ce853ad3-dab2-410e-8b66-c8f5e1c46aa5": {"doc_hash": "e538e74c32045e32685a7925c5fb2fbdb8f4c900073d81a0960db6f81f50f6f4"}, "4f356d76-ee31-47cb-ab29-0a945d391e1b": {"doc_hash": "22c710b2feb27734b650e13ea012b942dc455db7da884ee6f9af976648366445"}, "cbf9004a-14c0-41a9-93c6-7759ad546682": {"doc_hash": "e0d61dcc9931671205d868a88a24348b7fd397de65323dcd9889acfba7c46784"}, "2666cbef-f4d4-47ab-a99a-45ef66f264d4": {"doc_hash": "054e4be6e63a8483743adb7ca2d9e1ea767a340df5eaa30c68d59afe92f5cd19"}, "c0774129-fe44-459c-a854-cf4c1a59508d": {"doc_hash": "27f0d6cf2234b60fb3984e2e6ce22df891286e3e1d39a836f10348b41ec21c60"}, "f4e67bde-8edb-4475-a415-667ad3f7c1c4": {"doc_hash": "ac873ad8af9f79f8ba3a57febfdf006d57fad50cdd0915e63f33d8ed233f6bc1"}, "9cef4ce6-733b-415c-9275-55bd772a77f1": {"doc_hash": "d6d2fc4dae8249c11ae06de8eeafd600dab2bd5e1957f06d33de8f0d15693d47"}, "24ace6ff-47f7-4945-95c5-faf26b5b5bed": {"doc_hash": "b2bbafec616e8a009ea30d30884077ec177711608e1c832401ee7ba3343da1b4"}, "d39fa593-f982-4591-8e49-6930edaa0060": {"doc_hash": "3a5664a5f40ef840bdc93d425c34a755adcc3e3d19e481316d85806827bab70f"}, "83498cb2-5826-4328-966c-9f1ce4072c6e": {"doc_hash": "1db2f801e440850c13c22cbe3cf1c84714698bb46f3406bf43fe367044c87515"}, "155c6d67-9e66-4ae3-babf-b39ac8bc8310": {"doc_hash": "8a6129bba80ff651f57ff8650c5364f9286e1681f82bf88fc55bf81648b2e3c2"}, "214ccb85-281b-4d5a-90d3-73f4338803be": {"doc_hash": "f766b50c009df1875dd392611bf4dee00193e6e31875cd2e00a4cd7fec8ef7a3"}, "348b29e3-5db0-40e8-b0c9-2aefe7395e9b": {"doc_hash": "0e47f20d9fefe4f0a1cf1123dd4ced95b76757243313060345a5fa5a77785179"}, "070ce05d-7674-4a6f-98cc-189edd9189e2": {"doc_hash": "5e8020108578c006352ba1eb4e17d3d90bfc6a597c919d10845083778a821bb3"}, "2603c433-85cd-4c63-8c17-6ca379b0fb24": {"doc_hash": "9cf3945bce18eee34aa8c1362603771639e8555a99a2fd985ea6ba88d9bc4e3b"}, "cb6c30ad-828a-4186-b725-e886ab526091": {"doc_hash": "e69df4bca2638f4f9ac2a9f6c753795fbb41a5f9889347444586cfde1fdb29fa"}, "9df13367-0980-4d04-abcd-5ec86152cc57": {"doc_hash": "88fdc7844e330ecb5809ad577e5ab8d582ac752d63d9c51069eb5f49b58d7e80"}, "c89970ad-e67f-4ca6-866c-16342819768f": {"doc_hash": "cbc16598cf7d19d69351b5f2a5cf564e55f6dcc09f6e8f6d37c5729ddbab1aed"}, "7f277e7f-1f0e-4f2a-affa-67b375fdfa17": {"doc_hash": "0950c0a81d2589c55d519cdfcc7d87228b19638d544ab33cb2ac573d3429c901"}, "fec8cc29-296b-446b-832e-c8cee8e2b9f9": {"doc_hash": "5f187d2ed456a9fc87a7206dca2d9ee87e65ba652cf05cfc2229241e23c01ec5"}, "e1ae9a87-9a13-4d14-8875-4cc3762b49b2": {"doc_hash": "e216e24f681b3cf6d196784889cbf254ef899d321cf2d6d53effa62ae4dd66d1"}, "9ba9ec66-2d83-4209-ad0b-492d9ecdab2d": {"doc_hash": "f51c09512831d3193595a1527f7b95bffdccb7122950ea96bc33d07b4cdc2ddc"}, "c2cfcdc5-0bbb-469c-936f-4540ba98589e": {"doc_hash": "f196c36a0c909f3764061d6ff87f30eb269280247a42eb9f5737b2b0c2193b5b"}, "3825d84b-088d-43f5-a57d-a81aba68641b": {"doc_hash": "d4d54eb1a4eb514d32ed6f121f0dc7add8e5e7ad5da2e4ccb033827ab3d6369a"}, "20d1c5b7-78f2-467f-8a53-3acb9a924dd5": {"doc_hash": "5e188aa7ed137e62d19f0d3a489150078b494efe2f9ce777b0052efdbbf9b595"}, "5d803afc-6f92-466e-bb2d-519e9d48b617": {"doc_hash": "a1c241fe91e84465e43ac221f48b173c839415f4b687a31c11cad2a440c33bf4"}, "07e3f314-e2f7-4087-83da-d54b0e231a09": {"doc_hash": "e6222919d47f00aece8832bc853b2fe00e375c5d6271771ae4d3e1ea6a88ef69"}, "a5db1f66-0ef8-4b1d-80c8-b3cc79cd40c8": {"doc_hash": "c7407f8a13169ccc1a9858727547930c9bbbaf2456d4441c48f08d08fc0bac72"}, "f1b1b124-aa3b-4e2f-a287-8a5e37831301": {"doc_hash": "e30a997f9464605cd2064b51cbd79db33c3da700c48a497caa9082af3494b695"}, "1874ec88-199b-4aa6-ae4a-dbc1f356f779": {"doc_hash": "3b2ffdadbaa83f681fd06579473dc16322642de7bbeda25c5eaa039e9cf0ecd1"}, "b84f75f2-6cf9-4721-ad2e-8c1997b829da": {"doc_hash": "939003504a091464d5a7b05b4a20ec9358976a06999caa6c6d8539f454b4baea"}, "15d7fe38-76ac-4c70-a201-c65ba108a9d2": {"doc_hash": "efbe7510be2f9b1a709411f5dadef10a83c2f4f9963e5b90ced58c2600248c6a"}, "1b6bd240-efc2-4fec-b15b-b420e729fc28": {"doc_hash": "c5517a7a08a696ce7c66036f64b2c879cfc4a262d6ebd61562bb3cf30f4a38a0"}, "ef0f4242-b98e-48ec-9abb-305d3b6fb5ee": {"doc_hash": "b91fbba5dd23bbb6d613f52adb8db0617837c457a7c1cd7edf26bd2c1a5d5d58"}, "084f9735-27c2-4628-b209-af3c7c84bd63": {"doc_hash": "69bbac75cc1d1026635f4129cdbefb3ba316422228464b4e7313fdd076f52132"}, "47f3a565-d5c3-441b-be03-8b7a4101257d": {"doc_hash": "298e059f7c06fe6a041a7c1150bcd94523d8298f3d3cbfaa363184b1d6051454"}, "9b75efe4-9be9-4d3c-a572-24e18921dd9b": {"doc_hash": "912a8fd2d8dbbca5ecb759e7f1be5a6181e760120c2141e183f0af9a996c15b0"}, "dd48145d-ea98-46fa-b7cf-ceb6102456e6": {"doc_hash": "e4e5d6b9dd2bce3b7558a1d3296c5b8ce42b17c2417b9ae0ea67e48cf7a661fd"}, "a8508070-349b-4b0e-86c4-c770ec383d8c": {"doc_hash": "50568c575f84ec8449d025a888b4c757040ace913432efcc61e1bfe5f09edaa9"}, "2d0e9e43-6a01-4280-a108-4f9319e13558": {"doc_hash": "d12323f665db70e1cbeb24cb6dec5f21a5106f0695093092d4ce99e4129d635b"}, "3f8f418b-5fec-4f88-b3b9-acc21e47aab9": {"doc_hash": "c10aa2bcabe134d67ef479c9f2bdd0751a4be6828e7c6fa6e793d645aaf62a9c"}, "3a69db45-70de-413f-8219-02bda0026c91": {"doc_hash": "19ba1f558d4db6653c422c14caefe011345b773c1afe337e8c3f6f62cd0f9519"}, "fced9a6a-753f-47fb-a9d3-da774afd7dcf": {"doc_hash": "decc3f822872f79249322e5def4f9c9cb3cdb14979f02fb66ea9da620a64cc61"}, "0977efdf-4bef-4cc1-8610-b5736a69be2d": {"doc_hash": "1fdf7907fc5772b1ae9c27939599d39633624a11c698a5664bc202c7850c108c"}, "a92f43ea-c6b6-452c-bff7-faad8cf52baa": {"doc_hash": "5f5430d3f8a33b4d8fe81d184d3b0022d5b6f16c72a6b2210d30ed1588b90faa"}, "fc8a93b6-fa6b-4364-8c64-723c6eff37bf": {"doc_hash": "c89e27b8abbf8b67307b6d38a53a6ea372ca32b9dd655e1975e52dfad548e28b"}, "a9d9e244-8a6b-4829-96e0-bd2a0a09b8be": {"doc_hash": "e1cb75006a180fc4e6fec232533e074fa5521bb846bc3d41a6d6e4cda001579b"}, "05289fa9-f960-49e3-815f-a3e54ebc9db4": {"doc_hash": "f255962bd5f6fa4150ca774fe2376d8d42efa08c4a34862509d508068f9ae95f"}, "c9f7e3c5-87a6-4c33-a48e-ae769ce674ca": {"doc_hash": "9759e795af4e87549ff6d75656207190241776f5f2f99632688cbcea360dc5cf"}, "6514edfc-c200-4b1d-ac1e-d30cc73173a7": {"doc_hash": "e1aa02a8457cc1b417db1ccb2440fc2835ac51dd801559068855a66bf04a29ae"}, "fec1c7f1-7195-4ef1-87bb-e6b8d8172aed": {"doc_hash": "ff0792c6ae24a26f688666c95761ac43242280c662604635da097da93e598525"}, "fd2ec5b2-4cc4-470c-85a2-920435b04fdd": {"doc_hash": "5d150e8ed55163f6a2f61c2230f71681b753cddef1891dda7163ccb6c62b4023"}, "08b10f10-0620-4815-971f-50a1953fa89e": {"doc_hash": "c32e99be6525cc925d361724999289345bc2ace71257d7fd52c169f6c150d370"}, "d575e6af-3f05-4d85-ab94-3a5f45121fb5": {"doc_hash": "e8c8b774cb2bbab0920af4e38b37efeb2c5195046332b0529ab139075837a485"}, "f22f6c27-5c78-4b72-b53d-7d724480c47f": {"doc_hash": "3fb722ebd293b9bab6893c68804ff867159528bcb3799b355c1f064f9b6322fb"}, "633704d6-437e-45f3-b5f9-e09cb1ba9e84": {"doc_hash": "07b60a7cbcdf229d00fb1f4c04c340a32cc264796cbb62a950a579f0b325b7ca"}, "a6880fb2-e1d5-4e68-87b9-5706a74c3176": {"doc_hash": "ff033201e56ab3d57c0a8f04c64c25af15e7c1d1ef0443097994ccfca2b34387"}, "e076ad79-38b2-4ef3-a856-18ca0b811df3": {"doc_hash": "36e44ceaf8b56e461940557fd9a9d258b80cc10a68d4d3039cf51236319033f2"}, "0a25170d-ef62-45aa-aea2-e257ce4ce09a": {"doc_hash": "fffa768579efe5254c8a5be142be89afaa759c434639a90046a7ad830243f2b1"}, "ae197d99-74b8-413a-8959-18bd9b3213c0": {"doc_hash": "c931bcb9de46b919083a144a3329e94b783065bfb8ef335e4301c2e89a00eb94"}, "811e9aa9-9c15-445e-a2f3-4657cbdd0250": {"doc_hash": "c7657c44c54b1d2943ee1e9ff07aa228e39937e76f39636100c7f132fe4d5a8f"}, "c0e6b78e-6b34-4d09-b100-76909b2c81b6": {"doc_hash": "9c94364f47d5788b6b92e494ac0458bf5a2d92b41297ddd63dc8b125a8c6fbb5"}, "dc150f0a-a89b-4a87-a75f-4f0800af7da3": {"doc_hash": "c645278d681eb8dc3630c2060b33a0c3c1170a45e5955d6da323e44e4bcdb04d"}, "7d2f0545-18e0-48b6-8df7-77113a884184": {"doc_hash": "37cde77623fa486919e22780c96c54d3893009244cd2df7ac3ac87ce8da1b58e"}, "9ef82538-c30b-4d39-9091-97d3e98465f4": {"doc_hash": "c46d77f9347f95d0e293b4d3df3565dad565d29101ce28359eabf2e12cde8b78"}, "ace16d59-b77a-4ca5-b5c0-0c85567c4e1b": {"doc_hash": "d63dd20f6c26f040b104db07f23b33473d835baa1ed6c8da08a760bdd8b995f2"}, "a9667236-7e54-48cc-b16f-3261498f2313": {"doc_hash": "c468d7c18e3dbd22e1f9a161995ad838e9f2a9cee2373d772a90a507c0d3485f"}, "850f5125-5527-4c5d-ad08-897d86eb4414": {"doc_hash": "c8e048bf767f8c437678534ca66e5ba99ae8829cfad2bed8063d3a96e82c7a19"}, "5b4e2fd1-1c43-48b0-bb1e-c4a6e2f5619e": {"doc_hash": "80c5af4445933b26989c155067a573bff6d74e237f5cbbb6ab0fe6fe2c482e8a"}, "d94aec2d-1086-41e5-b6ad-3b397204e236": {"doc_hash": "a7073bee40289faaf43054b6d95c9fd181edca7421e3351c604317b4fc87bcdb"}, "11dde415-511f-4ba3-94a4-f7e9c51ffb3c": {"doc_hash": "b2120f17d853f1f5c26728f29db09cd06536500b748462e2e9135884ffc71573"}, "79788e03-56d9-480c-8696-e6c3886e235f": {"doc_hash": "e63812f1fd0bbbcb8126cb07193e6092a5d0cfccb443dff2a817e1b9eb97705e"}, "1f533b87-a7b5-4299-b0bf-79f0d252cc3d": {"doc_hash": "9374242caf1a110cc9a3c6cd74879b1ded96adb44812d2c8c96af71ec5b62fc6"}, "30c7b2b8-a6bb-42d6-8fcb-94947d308de2": {"doc_hash": "9bc9f562c725c99ad516323ec76401daec1ba7410be38b2b9d256db63e0d3bb8"}, "395236c4-7b9f-4318-ab2d-7e307f23677b": {"doc_hash": "4c4815796afad523df42b891b979691a88248e4a153a52383e1d36dfe4495b2e"}, "3537e75f-aaaa-4258-b0a8-42fde60f8f37": {"doc_hash": "0ace18ac28f4aa6233f3707c6c17e98b5ae23f8aad695884639badd064e1eeee"}, "a388d8bd-95ea-4dc2-a210-ad79f0894221": {"doc_hash": "86ceb0d68dbbe541aef22e887fdbaf0f7fcd422cce5b8e9c160be9b937fdaa47"}, "cce7fc22-bdab-4322-b73f-d458035d6367": {"doc_hash": "8b04b1cf84134bc3695e43a3347f2bc2e8b1bb8916c6afe84c05a6805fbdf187"}, "ebfcebbf-2884-4e0c-b8a9-ad96361062b1": {"doc_hash": "7a81d45692abb9f81dfa1b641aab24d912c77f24192e5323c7a7e78187429c5d"}, "65462fca-f4d4-4325-bc55-cf80555e8360": {"doc_hash": "ffbccd5adc70e40ab541464400d10ac723ce4bb7e3e3f31f8dd1d07d0c760e1a"}, "a57bc87a-f5b3-45b1-bd91-dc79dafbd6bb": {"doc_hash": "17d1af2f0dc26d2ef9d206a3e78ea71fe0220ebbc08786e5444d1ca484b5d5ff"}, "20c20c23-2d45-4a00-8fe3-eab25e652b14": {"doc_hash": "86e90b7f0ad45d52cb178916ce1b0551cf7aee50e64ed3bb51bb95720dd9e758"}, "f1eaaa90-3e4d-4b55-ac8b-d680014ec418": {"doc_hash": "98809caae3809f36cb3975c854d122d681e1851ca3ec180dc2fe4dcabcb3678b"}, "9ae2ad54-7d2f-40e6-8a36-87915760edc8": {"doc_hash": "ef2f326d5e23754b4841d8613eeab087bdda0b625e5b4f80f022b8f72ef8001e"}, "74fa7e51-f00d-407b-a747-fd00e4275539": {"doc_hash": "d2617fbdf2590e9cf43778072ab666d02a63c1e4f392332785d8ab8574d32951"}, "cf8287da-c587-4944-95e8-521b99170af5": {"doc_hash": "83f14b58360c99b20e1e2e856b4ad76b673b70c1c6abe1e6138169cf5ea12862"}, "41b95b6d-9cf0-46ee-b991-b3410fab7c51": {"doc_hash": "5121c72a8028ff3b6e2c95c5e91684ce6dc831711e2126e1e176aec5b1c43822"}, "3f08adc1-4d10-414e-979e-36d2da82ef72": {"doc_hash": "6c732bdf873c9ef4429cddcf80770b1df8ef3c339def58d80f1f5f752d11c7ef"}, "c3f12a07-6fbf-4472-8988-99fc57900e40": {"doc_hash": "3c4d09678eb93e50abc524fff62ffd37114d841138db90d4e758a5417a8b2fbd"}, "b139b657-d47b-400f-a386-bab5d78c5915": {"doc_hash": "1cc2e7ef1b1caf24dedb99aaa961691f76fb99089c200e329186475aba9733e0"}, "55591b36-c407-4e38-901c-132fe10ee533": {"doc_hash": "4b352caec49a77b1510c7f15fe5a3a600907c96c6706e2b72e38ba3d7c8b1051"}, "f48cdfc2-da08-4fa1-a27e-68e6f8cf7467": {"doc_hash": "55953fe65bb52a2bbe340fff598f57b5b3b9792566cbd4b6c9c216946700734e"}, "c0e920b9-ac05-46de-bc13-50d31e5ec672": {"doc_hash": "e20041396d684fb0329df550244d80998338d99dfb08993ad8df4d7983eb422b"}, "286b4c6f-2595-483a-8b86-08949afd5ea7": {"doc_hash": "06e2af66980d68cc945725d1998a4e5c52da5949c3518042b69007bb0dd8948c"}, "54389c3f-930e-419a-8a77-7ade862972d2": {"doc_hash": "85670ea2b51574ea45054bb497c3d6cdae624af878e41d7da7ea57ebaa2b63b7"}, "0f015b22-f635-4edc-95c4-c795b88d9191": {"doc_hash": "da66b2bcd2699746b29bdf6c4af874c9fc60606ad06a30689536158af9f1a586"}, "ca1d83ce-5662-4569-a8d7-d422951de7c1": {"doc_hash": "d967c42a38f2c901e0310beebe03b1a4e651208bdb3fa894da63df66c26831d0"}, "266b066b-d0c3-4118-a36c-75aac5beeec1": {"doc_hash": "d2a7315fead0826b0a65a6d21409bde446c3d182f432effb248f8c80aeac627d"}, "845db20d-9c24-4acb-91fd-ea0d0a301266": {"doc_hash": "b70240345ef870d7cc09a34c42a0f83466d5a126d6f82d45bafcb30398b13cad"}, "5a3987c2-5d30-40d0-8169-c7a24d729c13": {"doc_hash": "7e8423f50ddf6215ef648dca46d7bf77f4df2334dfabc09e19ad3d43be3c99e9"}, "0048af3b-0665-4f3f-bd98-2eca311c2b3f": {"doc_hash": "a9a1a4410a41841d32527896332ff094134fe15ad597bac04440c5299a41a6f8"}, "43049c07-8d46-4664-bf63-0102c0cb2cd4": {"doc_hash": "de9a44a62c86c03722e3ba6388eff83471de45de273a242b56b6c19f6a4c1a83"}, "84e54cd6-7a51-4181-b63f-f9e3767fc9c2": {"doc_hash": "40b067a7ffa61b97f6cd5115fd876b21008cbe07c14f80f4086b8203507e0d00"}, "cf3bf93f-737f-4c7d-bfb2-a4420474f988": {"doc_hash": "6e8b37ba210c44df38eb6cc3cf82cf10e9cf56bea67eafe5643b28806dc495ec"}, "3fbf10e6-7a4e-4a3a-818b-b7d29166e940": {"doc_hash": "268f99fc5a6a1486582281b969eece8835dcc3ddd57307ed4b76b98d1a088a58"}, "c4f5faab-6c94-41c6-999a-2d6d714e24e1": {"doc_hash": "00ac49370a2f788261fc7c86ff905571ca7f71b786af823f772d366ddd09ab96"}, "18c264fb-ad4d-4968-8ee2-fbc615850f54": {"doc_hash": "f5b50f45cf8c454a51ce770a24e604f80f427a1da9d4bb24a149060f3829093e"}, "0515afcf-1d44-4ba1-8189-83324576b7af": {"doc_hash": "ea56c23638def8780989a704257a8ef6761e6e112d947d299d4c6215ad53322e"}, "753eb80c-222e-4a13-b23d-27a7839615d6": {"doc_hash": "3167bc45c11adb58320d061904c06781cc005fba31af0b0fc324b9e632898d63"}, "fc07e778-ea39-436e-892b-a0b3aac4abf6": {"doc_hash": "719928e42504487a2b5caa62e47ca42a304f7e44ecd818604fa69b86848ce464"}, "c8080ae8-8c43-410f-b530-ad44d4d53848": {"doc_hash": "3711c41bbd9f37325442b3f6a6a5f0549c66af39cfd6d81b8c690a7ffc69b2ac"}, "9c3ba16b-4b60-4ab8-a4a7-c8acf2083f24": {"doc_hash": "8f99656c8d4a6c8da81ac609b219b668dea6f6cbb15c5444c6ecb92eef2076da"}, "685d34a4-d052-4778-bdb6-9f5b3323a863": {"doc_hash": "20f2c26b740cfaaa7f7112e8c40307c213923e72586c73a694f8dd228905eead"}, "a3358de9-b476-4275-8295-4cf7fece261f": {"doc_hash": "05262e00a8c7562edf16885049d7be61c1f354d89cdfaf555e1ad82423f422eb"}, "78fb3ff0-813c-4c35-a685-ce42fbc9ffa4": {"doc_hash": "3a59c78a2374b5b13577003b17d081ce55b173a97b4c03d2580d60f27a2a978f"}, "f4a67ec8-0fd5-4479-ab86-6ea9cd4a8f50": {"doc_hash": "0def5e01ee9752c1cfe68106fc49d1a0f4376d2da553c1d5fb4377e5651231c7"}, "8bb56d8e-8212-4c2f-8854-761f0ecd1a47": {"doc_hash": "d77b1940a2e4df659ceb0c41fe09a42db1c8257f21730a3b783481a5c04662d0"}}}